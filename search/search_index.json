{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"torchmil","text":"<p>torchmil is a PyTorch-based library for deep Multiple Instance Learning (MIL). It provides a simple, flexible, and extensible framework for working with MIL models and data.</p> <p>It includes:</p> <ul> <li>A collection of popular MIL models.</li> <li>Different PyTorch modules frequently used in MIL models.</li> <li>Handy tools to deal with MIL data.</li> <li>A collection of popular MIL datasets.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install torchmil\n</code></pre>"},{"location":"#quick-start","title":"Quick start","text":"<p>You can load a MIL dataset and train a MIL model in just a few lines of code:</p> <pre><code>from torchmil.datasets import Camelyon16MIL\nfrom torchmil.models import ABMIL\nfrom torchmil.utils import Trainer\nfrom torchmil.data import collate_fn\nfrom torch.utils.data import DataLoader\n\n# Load the Camelyon16 dataset\ndataset = Camelyon16MIL(root='data', features='UNI')\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n\n# Instantiate the ABMIL model and optimizer\nmodel = ABMIL(in_shape=(2048,), criterion=torch.nn.BCEWithLogitsLoss()) # each model has its own criterion\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# Instantiate the Trainer\ntrainer = Trainer(model, optimizer, device='cuda')\n\n# Train the model\ntrainer.train(dataloader, epochs=10)\n\n# Save the model\ntorch.save(model.state_dict(), 'model.pth')\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<p>You can take a look at the examples to see how to use torchmil in practice. To see the full list of available models, datasets, and modules, check the API reference.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this library useful, please consider citing it:</p> <pre><code>@misc{torchmil,\n  author = {Castro-Mac{\\'\\i}as, Francisco M and S{\\'a}ez-Maldonado, Francisco Javier and Morales Alvarez, Pablo and Molina, Rafael},\n  title = {torchmil: A PyTorch-based library for deep Multiple Instance Learning},\n  year = {2025},\n  howpublished = {\\url{https://franblueee.github.io/torchmil/}}\n}\n</code></pre>"},{"location":"api/","title":"API reference","text":""},{"location":"api/#data-torchmildata","title":"Data: torchmil.data","text":"<ul> <li>Introduction</li> <li>Collate function</li> <li>Spatial and sequential representation</li> </ul>"},{"location":"api/#datasets-torchmildatasets","title":"Datasets: torchmil.datasets","text":"<ul> <li>Introduction</li> <li>Processed MIL Dataset</li> <li>Toy Dataset</li> <li>CT Scan Dataset</li> <li>WSI Dataset</li> <li>Binary classification Dataset</li> <li>Camelyon16 MIL Dataset</li> <li>PANDA MIL Dataset</li> <li>RSNA MIL Dataset</li> <li>False Frequency Dataset</li> <li>Multi-Concept Standard Dataset</li> <li>Single Concept Standard Dataset</li> </ul>"},{"location":"api/#modules-torchmilnn","title":"Modules: torchmil.nn","text":"<ul> <li>Introduction</li> <li>Attention<ul> <li>Attention Pool</li> <li>Sm Attention Pool</li> <li>Probabilistic Smooth Attention Pool</li> <li>Multihead Self-Attention</li> <li>Multihead Self-Attention with Relative Positional Encoding (iRPE)</li> <li>Nystrom Attention</li> <li>Multihead Cross-Attention</li> </ul> </li> <li>Graph Neural Networks (GNNs)<ul> <li>Deep Graph Convolutional Network (DeepGCN) layer</li> <li>Graph Convolutional Network (GCN) convolution</li> <li>Dense MinCut pooling</li> </ul> </li> <li>Transformers<ul> <li>Transformer base class</li> <li>Conventional Transformer</li> <li>Sm Transformer</li> <li>Nystrom Transformer</li> <li>Transformer with image Relative Positional Encoding (iRPE)</li> <li>Tokens-2-Token</li> </ul> </li> <li>Sm operator</li> <li>Max Pool</li> <li>Mean Pool</li> </ul>"},{"location":"api/#models-torchmilmodels","title":"Models: torchmil.models","text":"<ul> <li>Introduction</li> <li>General MIL model</li> <li>ABMIL</li> <li>CAMIL</li> <li>CLAM</li> <li>DeepGraphSurv</li> <li>DFTDMIL</li> <li>DSMIL</li> <li>GTP</li> <li>IIBMIL</li> <li>PatchGCN</li> <li>ProbSmoothABMIL</li> <li>SmABMIL</li> <li>SmTransformerABMIL</li> <li>TransformerABMIL</li> <li>TransformerProbSmoothABMIL</li> <li>TransMIL</li> <li>SETMIL</li> <li>IIBMIL</li> </ul>"},{"location":"api/#visualize-torchmilvisualize","title":"Visualize: torchmil.visualize","text":"<ul> <li>Introduction</li> <li>Visualizing CT scans</li> <li>Visualizing WSI</li> </ul>"},{"location":"api/#utils-torchmilutils","title":"Utils: torchmil.utils","text":"<ul> <li>Introduction</li> <li>Annealing Scheduler</li> <li>Graph utils</li> <li>Trainer</li> </ul>"},{"location":"api/data/","title":"torchmil.data","text":""},{"location":"api/data/#bags-in-torchmil","title":"Bags in torchmil","text":"<p>Note</p> <p>The data representation in torchmil is inspired by the data representation in PyTorch Geometric.</p> <p>See this notebook for a detailed explanation of the data representation in torchmil.</p> <p>In Multiple Instance Learning (MIL), a bag is a collection of instances. In torchmil, a bag is represented as a TensorDict. In most cases (e.g., the datasets provided in torchmil), a bag  will contain at least two keys:</p> <ul> <li><code>bag['X']</code>: a tensor of shape <code>(bag_size, ...)</code> containing the instances in the bag. Usually, this tensor is called bag feature matrix, since these instances are feature vectors extracted from the raw representation of the instances, and therefore it has shape <code>(bag_size, feature_dim)</code>.</li> <li><code>bag['Y']</code>: a tensor containing the label of the bag. In the simplest case, this tensor is a scalar, but it can be a tensor of any shape (e.g., in multi-class MIL).</li> </ul> <p>Additionally, a bag may contain other keys. The most common ones in torchmil are:</p> <ul> <li><code>bag['y_inst']</code>: a tensor of shape <code>(bag_size, ...)</code> containing the labels of the instances in the bag. In the pure MIL setting, this tensor is only used for evaluation purposes since the label of the instances are not known. However, some methods may require some sort of supervision at the instance level.</li> <li><code>bag['adj']</code>: a tensor of shape <code>(bag_size, bag_size)</code> containing the adjacency matrix of the bag. This matrix is used to represent the relationships between the instances in the bag. The methods implemented in torchmil.models allow this matrix to be a sparse tensor.</li> <li><code>bag['coords']</code>: a tensor of shape <code>(bag_size, coords_dim)</code> containing the coordinates of the instances in the bag. This tensor is used to represent the absolute position of the instances in the bag.</li> </ul> <p>The data representation in torchmil is designed to be flexible and to allow the user to add any additional information to the bags. The user can define new keys in the bags and use them in the models implemented in torchmil.</p>"},{"location":"api/data/#more-information","title":"More information","text":"<ul> <li>Batches in torchmil</li> <li>Spatial and sequential representation</li> </ul>"},{"location":"api/data/collate/","title":"Collate","text":"<p>Note</p> <p>See this notebook for an explanation with examples of how batching is performed in torchmil.</p>"},{"location":"api/data/collate/#batches-in-torchmil","title":"Batches in torchmil","text":"<p>Batching is crucial for training deep learning models. However, in MIL, each bag can be of different size. To solve this, in torchmil, the tensors in the bags are padded to the maximum size of the bags in the batch. A mask tensor is used to indicate which elements of the padded tensors are real instances and which are padding. This mask tensor is used to adjust the behavior of the models to ignore the padding elements (e.g., in the attention mechanism).</p> <p>Why not use <code>torch.nested</code>?</p> <p><code>torch.nested</code> offer a more flexible method for handling bags of varying sizes. However, since the PyTorch API for nested tensors is still in the prototype stage, torchmil currently relies on the padding approach.</p> <p>The function <code>torchmil.data.collate_fn</code> is used to collate a list of bags into a batch. This function can be used as the <code>collate_fn</code> argument of the PyTorch <code>DataLoader</code>. The function <code>torchmil.data.pad_tensors</code> is used to pad the tensors in the bags.</p>"},{"location":"api/data/collate/#torchmil.data.collate_fn","title":"<code>torchmil.data.collate_fn(batch_list, sparse=True)</code>","text":"<p>Collate function for MIL datasets. Given a list of bags (represented as dictionaries) it pads the tensors in the bag to the same shape. Then, it returns a dictionary representing the batch. The keys in the dictionary are the keys in the bag dictionaries. Additionally, the returned dictionary contains a mask for the padded tensors. This mask is 1 where the tensor is not padded and 0 where the tensor is padded.</p> <p>Parameters:</p> <ul> <li> <code>batch_list</code>               (<code>list[dict[str, Tensor]]</code>)           \u2013            <p>List of dictionaries. Each dictionary represents a bag and should contain the same keys. The values can be:</p> <ul> <li>Tensors of shape <code>(bag_size, ...)</code>. In this case, the tensors are padded to the same shape.</li> <li>Sparse tensors in COO format. In this case, the resulting sparse tensor has shape <code>(batch_size, max_bag_size, max_bag_size)</code>, where <code>max_bag_size</code> is the maximum bag size in the batch. If <code>sparse=False</code>, the sparse tensor is converted to a dense tensor.</li> </ul> </li> <li> <code>sparse</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, the sparse tensors are returned as sparse tensors. If False, the sparse tensors are converted to dense tensors.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>batch_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary with the same keys as the bag dictionaries. The values are tensors of shape <code>(batch_size, max_bag_size, ...)</code> or sparse tensors of shape <code>(batch_size, max_bag_size, max_bag_size)</code>. Additionally, the dictionary contains a mask of shape <code>(batch_size, max_bag_size)</code>.</p> </li> </ul>"},{"location":"api/data/collate/#torchmil.data.pad_tensors","title":"<code>torchmil.data.pad_tensors(tensor_list, padding_value=0)</code>","text":"<p>Pads a list of tensors to the same shape and returns a mask.</p> <p>Parameters:</p> <ul> <li> <code>tensor_list</code>               (<code>list[Tensor]</code>)           \u2013            <p>List of tensors, each of shape <code>(bag_size, ...)</code>.</p> </li> <li> <code>padding_value</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Value to pad with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>padded_tensor</code> (              <code>Tensor</code> )          \u2013            <p>Padded tensor of shape <code>(batch_size, max_bag_size, ...)</code>.</p> </li> <li> <code>mask</code> (              <code>Tensor</code> )          \u2013            <p>Mask of shape <code>(batch_size, max_bag_size)</code>.</p> </li> </ul>"},{"location":"api/data/representation/","title":"Representation","text":"<p>Note</p> <p>See this notebook for an explanation with examples of the different types of representations in torchmil.</p>"},{"location":"api/data/representation/#spatial-and-sequential-representation","title":"Spatial and sequential representation","text":"<p>In torchmil, bags can be represented in two ways: sequential and spatial.</p> <p>In the sequential representation <code>bag['X']</code> is a tensor of shape <code>(bag_size, dim)</code>. This representation is the most common in MIL.</p> <p>When the bag has some spatial structure, the sequential representation can be coupled with a graph using an adjacency matrix or with the coordinates of the instances. These are stored as <code>bag['adj']</code> (of shape <code>(bag_size, bag_size)</code>) and <code>bag['coords']</code> (of shape <code>(bag_size, coords_dim)</code>), respectively.</p> <p>Alternatively, the spatial representation can be used. In this case, <code>bag['X']</code> is a tensor of shape <code>(coord1, ..., coordN, dim)</code>, where <code>N=coords_dim</code> is the number of dimensions of the space.</p> <p>In torchmil, you can convert from one representation to the other using the functions <code>torchmil.utils.seq_to_spatial</code> and <code>torchmil.utils.spatial_to_seq</code> from the torchmil.data module. These functions need the coordinates of the instances in the bag, stored as <code>bag['coords']</code>.</p> <p>Example: Whole Slide Images</p> <p>Due to their large resolution, Whole Slide Images (WSIs) are usually represented as bags of patches. Each patch is an image, from which a feature vector of is typically extracted. The spatial representation of a WSI has shape <code>(height, width, feat_dim)</code>, while the sequential representation has shape <code>(bag_size, feat_dim)</code>. The coordinates corresponds to the coordinates of the patches in the WSI.</p> <p>SETMIL is an example of a model that uses the spatial representation of a WSI.</p>"},{"location":"api/data/representation/#torchmil.data.seq_to_spatial","title":"<code>torchmil.data.seq_to_spatial(X, coords)</code>","text":"<p>Computes the spatial representation of a bag given the sequential representation and the coordinates.</p> <p>Given the input tensor <code>X</code> of shape <code>(batch_size, bag_size, dim)</code> and the coordinates <code>coords</code> of shape <code>(batch_size, bag_size, n)</code>, this function returns the spatial representation <code>X_enc</code> of shape <code>(batch_size, coord1, coord2, ..., coordn, dim)</code>.</p> <p>This representation is characterized by the fact that the coordinates are used to index the elements of spatial representation: <code>X_enc[batch, i1, i2, ..., in, :] = X[batch, idx, :]</code> where <code>(i1, i2, ..., in) = coords[batch, idx]</code>.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Sequential representation of shape <code>(batch_size, bag_size, dim)</code>.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>Coordinates of shape <code>(batch_size, bag_size, n)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>X_esp</code> (              <code>Tensor</code> )          \u2013            <p>Spatial representation of shape <code>(batch_size, coord1, coord2, ..., coordn, dim)</code>.</p> </li> </ul>"},{"location":"api/data/representation/#torchmil.data.spatial_to_seq","title":"<code>torchmil.data.spatial_to_seq(X_esp, coords)</code>","text":"<p>Computes the sequential representation of a bag given the spatial representation and the coordinates.</p> <p>Given the spatial tensor <code>X_esp</code> of shape <code>(batch_size, coord1, coord2, ..., coordn, dim)</code> and the coordinates <code>coords</code> of shape <code>(batch_size, bag_size, n)</code>, this function returns the sequential representation <code>X</code> of shape <code>(batch_size, bag_size, dim)</code>.</p> <p>This representation is characterized by the fact that the coordinates are used to index the elements of spatial representation: <code>X_seq[batch, idx, :] = X_esp[batch, i1, i2, ..., in, :]</code> where <code>(i1, i2, ..., in) = coords[batch, idx]</code>.</p> <p>Parameters:</p> <ul> <li> <code>X_esp</code>               (<code>Tensor</code>)           \u2013            <p>Spatial representation of shape <code>(batch_size, coord1, coord2, ..., coordn, dim)</code>.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>Coordinates of shape <code>(batch_size, bag_size, n)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>X_seq</code> (              <code>Tensor</code> )          \u2013            <p>Sequential representation of shape <code>(batch_size, bag_size, dim)</code>.</p> </li> </ul>"},{"location":"api/datasets/","title":"torchmil.datasets","text":"<p>torchmil.datasets provides a collection of datasets for Multiple Instance Learning (MIL) tasks. These datasets are designed to be used with the models and utilities provided in the library. This list is constantly updated with new datasets and examples, feel free to contribute!</p>"},{"location":"api/datasets/#available-datasets","title":"Available datasets","text":"<ul> <li>Processed MIL Dataset</li> <li>Toy Dataset</li> <li>CT Scan Dataset</li> <li>WSI Dataset</li> <li>Binary classification Dataset</li> <li>Camelyon16 MIL Dataset</li> <li>PANDA MIL Dataset</li> <li>RSNA MIL Dataset</li> <li>False Frequency Dataset</li> <li>Multi-Concept Standard Dataset</li> <li>Single Concept Standard Dataset</li> </ul>"},{"location":"api/datasets/binary_classification_dataset/","title":"Binary classification dataset","text":""},{"location":"api/datasets/binary_classification_dataset/#torchmil.datasets.BinaryClassificationDataset","title":"<code>torchmil.datasets.BinaryClassificationDataset</code>","text":"<p>               Bases: <code>ProcessedMILDataset</code></p> <p>Dataset for binary classification MIL problems. See <code>torchmil.datasets.ProcessedMILDataset</code> for more information.</p> <p>For a given bag with bag label \\(Y\\) and instance labels \\(\\left\\{ y_1, \\ldots, y_N \\right \\}\\), this dataset assumes that</p> \\[\\begin{gather}     Y \\in \\left\\{ 0, 1 \\right\\}, \\quad y_n \\in \\left\\{ 0, 1 \\right\\}, \\quad \\forall n \\in \\left\\{ 1, \\ldots, N \\right\\},\\\\     Y = \\max \\left\\{ y_1, \\ldots, y_N \\right\\}. \\end{gather}\\] <p>When the instance labels are not provided, they are set to 0 if the bag label is 0, and to -1 if the bag label is 1. If the instance labels are provided, but they are not consistent with the bag label, a warning is issued and the instance labels are all set to -1.</p>"},{"location":"api/datasets/binary_classification_dataset/#torchmil.datasets.BinaryClassificationDataset.__init__","title":"<code>__init__(features_path, labels_path, inst_labels_path=None, coords_path=None, bag_names=None, bag_keys=['X', 'Y', 'y_inst', 'adj', 'coords'], dist_thr=1.5, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":""},{"location":"api/datasets/binary_classification_dataset/#torchmil.datasets.BinaryClassificationDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the keys defined in <code>bag_keys</code> and their corresponding values.</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/camelyon16mil_dataset/","title":"CAMELYON16 dataset","text":""},{"location":"api/datasets/camelyon16mil_dataset/#torchmil.datasets.CAMELYON16MILDataset","title":"<code>torchmil.datasets.CAMELYON16MILDataset</code>","text":"<p>               Bases: <code>BinaryClassificationDataset</code>, <code>WSIDataset</code></p> <p>CAMELYON16 dataset for Multiple Instance Learning (MIL). Download it from Hugging Face Datasets.</p> <p>About the Original CAMELYON16 Dataset. The original CAMELYON16 dataset contains WSIs of hematoxylin and eosin (H&amp;E) stained lymph node sections. The task is to identify whether each slide contains metastatic tissue and to localize it precisely. The dataset includes high-quality pixel-level annotations marking the metastases.</p> <p>Dataset Description. We have preprocessed the whole-slide images (WSIs) by extracting relevant patches and computing features for each patch using various feature extractors.</p> <ul> <li>A patch is labeled as positive (<code>patch_label=1</code>) if more than 50% of its pixels are annotated as metastatic.</li> <li>A WSI is labeled as positive (<code>label=1</code>) if it contains at least one positive patch.</li> </ul> <p>This means a slide is considered positive if there is any evidence of metastatic tissue.</p> <p>Directory Structure. After extracting the contents of the <code>.tar.gz</code> archives, the following directory structure is expected:</p> <p><pre><code>root\n\u251c\u2500\u2500 patches_{patch_size}\n\u2502 \u251c\u2500\u2500 features\n\u2502 \u2502 \u251c\u2500\u2500 features_{features_name}\n\u2502 \u2502 \u2502 \u251c\u2500\u2500 wsi1.npy\n\u2502 \u2502 \u2502 \u251c\u2500\u2500 wsi2.npy\n\u2502 \u2502 \u2502 \u2514\u2500\u2500 ...\n\u2502 \u251c\u2500\u2500 labels\n\u2502 \u2502 \u251c\u2500\u2500 wsi1.npy\n\u2502 \u2502 \u251c\u2500\u2500 wsi2.npy\n\u2502 \u2502 \u2514\u2500\u2500 ...\n\u2502 \u251c\u2500\u2500 patch_labels\n\u2502 \u2502 \u251c\u2500\u2500 wsi1.npy\n\u2502 \u2502 \u251c\u2500\u2500 wsi2.npy\n\u2502 \u2502 \u2514\u2500\u2500 ...\n\u2502 \u251c\u2500\u2500 coords\n\u2502 \u2502 \u251c\u2500\u2500 wsi1.npy\n\u2502 \u2502 \u251c\u2500\u2500 wsi2.npy\n\u2502 \u2502 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 splits.csv\n</code></pre> Each <code>.npy</code> file corresponds to a single WSI. The <code>splits.csv</code> file defines train/test splits for standardized experimentation.</p>"},{"location":"api/datasets/camelyon16mil_dataset/#torchmil.datasets.CAMELYON16MILDataset.__init__","title":"<code>__init__(root, features='UNI', partition='train', bag_keys=['X', 'Y', 'y_inst', 'adj', 'coords'], patch_size=512, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>root</code>               (<code>str</code>)           \u2013            <p>Path to the root directory of the dataset.</p> </li> <li> <code>features</code>               (<code>str</code>, default:                   <code>'UNI'</code> )           \u2013            <p>Type of features to use. Must be one of ['UNI', 'resnet50_bt'].</p> </li> <li> <code>partition</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Partition of the dataset. Must be one of ['train', 'test'].</p> </li> <li> <code>bag_keys</code>               (<code>list</code>, default:                   <code>['X', 'Y', 'y_inst', 'adj', 'coords']</code> )           \u2013            <p>List of keys to use for the bags. Must be in ['X', 'Y', 'y_inst', 'coords'].</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Size of the patches. Currently, only 512 is supported.</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the patches features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/camelyon16mil_dataset/#torchmil.datasets.CAMELYON16MILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the keys defined in <code>bag_keys</code> and their corresponding values.</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/ctscan_dataset/","title":"CTScan dataset","text":""},{"location":"api/datasets/ctscan_dataset/#torchmil.datasets.CTScanDataset","title":"<code>torchmil.datasets.CTScanDataset</code>","text":"<p>               Bases: <code>ProcessedMILDataset</code></p> <p>This class represents a dataset of Computed Tomography (CT) scans for Multiple Instance Learning (MIL).</p> <p>MIL and CT scans. Computed Tomography (CT) scans are medical imaging techniques that use X-rays to obtain detailed images of the body. Usually, a CT scan is a 3D volume and is composed of a sequence of slices. Each slice is a 2D image that represents a cross-section of the body. In the context of MIL, a CT scan is considered a bag, and the slices are considered instances.</p> <p>Directory structure. It is assumed that the bags have been processed and saved as numpy files. For more information on the processing of the bags, refer to the <code>ProcessedMILDataset</code> class. This dataset expects the following directory structure:</p> <pre><code>features_path\n\u251c\u2500\u2500 ctscan1.npy\n\u251c\u2500\u2500 ctscan2.npy\n\u2514\u2500\u2500 ...\nlabels_path\n\u251c\u2500\u2500 ctscan1.npy\n\u251c\u2500\u2500 ctscan2.npy\n\u2514\u2500\u2500 ...\ninst_labels_path\n\u251c\u2500\u2500 ctscan1.npy\n\u251c\u2500\u2500 ctscan2.npy\n\u2514\u2500\u2500 ...\n</code></pre> <p>Order of the slices and the adjacency matrix. This dataset assumes that the slices of the CT scans are ordered. An adjacency matrix \\(\\mathbf{A} = \\left[ A_{ij} \\right]\\) is built using this information:</p> \\[\\begin{equation} A_{ij} = \\begin{cases} d_{ij}, &amp; \\text{if } \\lvert i - j \\rvert = 1, \\\\ 0, &amp; \\text{otherwise}, \\end{cases} \\quad d_{ij} = \\begin{cases} 1, &amp; \\text{if } \\text{adj_with_dist=False}, \\\\ \\exp\\left( -\\frac{\\left\\| \\mathbf{x}_i - \\mathbf{x}_j \\right\\|}{d} \\right), &amp; \\text{if } \\text{adj_with_dist=True}. \\end{cases} \\end{equation}\\] <p>where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(\\mathbf{x}_j \\in \\mathbb{R}^d\\) are the features of instances \\(i\\) and \\(j\\), respectively.</p>"},{"location":"api/datasets/ctscan_dataset/#torchmil.datasets.CTScanDataset.__init__","title":"<code>__init__(features_path, labels_path, slice_labels_path=None, ctscan_names=None, bag_keys=['X', 'Y', 'y_inst', 'adj', 'coords'], adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>features_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing the matrices of the CT scans</p> </li> <li> <code>labels_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing the labels of the CT scans.</p> </li> <li> <code>slice_labels_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the labels of the slices.</p> </li> <li> <code>ctscan_names</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of the names of the CT scans to load. If None, all CT scans in the <code>features_path</code> directory are loaded.</p> </li> <li> <code>bag_keys</code>               (<code>list</code>, default:                   <code>['X', 'Y', 'y_inst', 'adj', 'coords']</code> )           \u2013            <p>List of keys to use for the bags. Must be in ['X', 'Y', 'y_inst', 'coords'].</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the slices features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/ctscan_dataset/#torchmil.datasets.CTScanDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the keys defined in <code>bag_keys</code> and their corresponding values.</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/false_frequency_dataset/","title":"False Frequency dataset","text":""},{"location":"api/datasets/false_frequency_dataset/#torchmil.datasets.FalseFrequencyMILDataset","title":"<code>torchmil.datasets.FalseFrequencyMILDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>False Frequency MIL Dataset. Implementation from Algorithm 3 in Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests.</p>"},{"location":"api/datasets/false_frequency_dataset/#torchmil.datasets.FalseFrequencyMILDataset.__init__","title":"<code>__init__(D, num_bags, pos_class_prob=0.5, train=True, seed=0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>D</code>               (<code>int</code>)           \u2013            <p>Dimensionality of the data.</p> </li> <li> <code>num_bags</code>               (<code>int</code>)           \u2013            <p>Number of bags in the dataset.</p> </li> <li> <code>pos_class_prob</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Probability of a bag being positive.</p> </li> <li> <code>train</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to create the training or test dataset.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Seed for the random number generator.</p> </li> </ul>"},{"location":"api/datasets/false_frequency_dataset/#torchmil.datasets.FalseFrequencyMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Bag features of shape <code>(bag_size, feat_dim)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag.</li> </ul> </li> </ul>"},{"location":"api/datasets/mc_standard_dataset/","title":"Multi-Concept Standard dataset","text":""},{"location":"api/datasets/mc_standard_dataset/#torchmil.datasets.MCStandardMILDataset","title":"<code>torchmil.datasets.MCStandardMILDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Multi-Concept Standard MIL Dataset. Implementation from Algorithm 2 in Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests.</p>"},{"location":"api/datasets/mc_standard_dataset/#torchmil.datasets.MCStandardMILDataset.__init__","title":"<code>__init__(D, num_bags, pos_class_prob=0.5, train=True, seed=0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>D</code>               (<code>int</code>)           \u2013            <p>Dimensionality of the data.</p> </li> <li> <code>num_bags</code>               (<code>int</code>)           \u2013            <p>Number of bags in the dataset.</p> </li> <li> <code>pos_class_prob</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Probability of a bag being positive.</p> </li> <li> <code>train</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to create the training or test dataset.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Seed for the random number generator.</p> </li> </ul>"},{"location":"api/datasets/mc_standard_dataset/#torchmil.datasets.MCStandardMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Bag features of shape <code>(bag_size, feat_dim)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag.</li> </ul> </li> </ul>"},{"location":"api/datasets/pandamil_dataset/","title":"PANDA dataset","text":""},{"location":"api/datasets/pandamil_dataset/#torchmil.datasets.PANDAMILDataset","title":"<code>torchmil.datasets.PANDAMILDataset</code>","text":"<p>               Bases: <code>BinaryClassificationDataset</code>, <code>WSIDataset</code></p> <p>Prostate cANcer graDe Assessment (PANDA) dataset for Multiple Instance Learning (MIL). Download it from Hugging Face Datasets.</p> <p>About the original PANDA Dataset. The original PANDA dataset contains WSIs of hematoxylin and eosin (H&amp;E) stained prostate biopsy samples. The task is to classify the severity of prostate cancer within each slide, and to localize the cancerous tissue precisely. The dataset includes high-quality pixel-level annotations marking the cancerous tissue.</p> <p>Dataset Description.</p> <p>We have preprocessed the whole-slide images (WSIs) by extracting relevant patches and computing features for each patch using various feature extractors.</p> <ul> <li>A patch is labeled as positive (<code>patch_label=1</code>) if more than 50% of its pixels are annotated as cancerous.</li> <li>A WSI is labeled as positive (<code>label=1</code>) if it contains at least one positive patch.</li> </ul> <p>This means a slide is considered positive if there is any evidence of cancerous tissue.</p> <p>Directory Structure. After extracting the contents of the <code>.tar.gz</code> archives, the following directory structure is expected:</p> <p><pre><code>root\n\u251c\u2500\u2500 patches_{patch_size}\n\u2502 \u251c\u2500\u2500 features\n\u2502 \u2502 \u251c\u2500\u2500 features_{features_name}\n\u2502 \u2502 \u2502 \u251c\u2500\u2500 wsi1.npy\n\u2502 \u2502 \u2502 \u251c\u2500\u2500 wsi2.npy\n\u2502 \u2502 \u2502 \u2514\u2500\u2500 ...\n\u2502 \u251c\u2500\u2500 labels\n\u2502 \u2502 \u251c\u2500\u2500 wsi1.npy\n\u2502 \u2502 \u251c\u2500\u2500 wsi2.npy\n\u2502 \u2502 \u2514\u2500\u2500 ...\n\u2502 \u251c\u2500\u2500 patch_labels\n\u2502 \u2502 \u251c\u2500\u2500 wsi1.npy\n\u2502 \u2502 \u251c\u2500\u2500 wsi2.npy\n\u2502 \u2502 \u2514\u2500\u2500 ...\n\u2502 \u251c\u2500\u2500 coords\n\u2502 \u2502 \u251c\u2500\u2500 wsi1.npy\n\u2502 \u2502 \u251c\u2500\u2500 wsi2.npy\n\u2502 \u2502 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 splits.csv\n</code></pre> Each <code>.npy</code> file corresponds to a single WSI. The <code>splits.csv</code> file defines train/test splits for standardized experimentation.</p>"},{"location":"api/datasets/pandamil_dataset/#torchmil.datasets.PANDAMILDataset.__init__","title":"<code>__init__(root, features='UNI', partition='train', bag_keys=['X', 'Y', 'y_inst', 'adj', 'coords'], patch_size=512, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>root</code>               (<code>str</code>)           \u2013            <p>Path to the root directory of the dataset.</p> </li> <li> <code>features</code>               (<code>str</code>, default:                   <code>'UNI'</code> )           \u2013            <p>Type of features to use. Must be one of ['UNI', 'resnet50_bt'].</p> </li> <li> <code>partition</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Partition of the dataset. Must be one of ['train', 'test'].</p> </li> <li> <code>bag_keys</code>               (<code>list</code>, default:                   <code>['X', 'Y', 'y_inst', 'adj', 'coords']</code> )           \u2013            <p>List of keys to use for the bags. Must be in ['X', 'Y', 'y_inst', 'coords'].</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Size of the patches. Currently, only 512 is supported.</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the patches features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/pandamil_dataset/#torchmil.datasets.PANDAMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the keys defined in <code>bag_keys</code> and their corresponding values.</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/processed_mil_dataset/","title":"Processed MIL dataset","text":""},{"location":"api/datasets/processed_mil_dataset/#torchmil.datasets.ProcessedMILDataset","title":"<code>torchmil.datasets.ProcessedMILDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>This class represents a general MIL dataset where the bags have been processed and saved as numpy files.  It enforces strict data availability for core components, failing fast if expected files are missing.</p> <p>MIL processing and directory structure. The dataset expects pre-processed bags saved as individual numpy files.  - A feature file should yield an array of shape <code>(bag_size, ...)</code>, where <code>...</code> represents the shape of the features.  - A label file should yield an array of shape arbitrary shape, e.g., <code>(1,)</code> for binary classification.  - An instance label file should yield an array of shape <code>(bag_size, ...)</code>, where <code>...</code> represents the shape of the instance labels.  - A coordinates file should yield an array of shape <code>(bag_size, coords_dim)</code>, where <code>coords_dim</code> is the dimension of the coordinates.</p> <p>Bag keys and directory structure.  The dataset can be initialized with a list of bag keys, which are used to choose which data to load.  This dataset expects the following directory structure:</p> <pre><code>features_path/ (if \"X\" in bag_keys)\n\u251c\u2500\u2500 bag1.npy\n\u251c\u2500\u2500 bag2.npy\n\u2514\u2500\u2500 ...\nlabels_path/ (if \"Y\" in bag_keys)\n\u251c\u2500\u2500 bag1.npy\n\u251c\u2500\u2500 bag2.npy\n\u2514\u2500\u2500 ...\ninst_labels_path/ (if \"y_inst\" in bag_keys)\n\u251c\u2500\u2500 bag1.npy\n\u251c\u2500\u2500 bag2.npy\n\u2514\u2500\u2500 ...\ncoords_path/ (if \"coords\" or \"adj\" in bag_keys)\n\u251c\u2500\u2500 bag1.npy\n\u251c\u2500\u2500 bag2.npy\n\u2514\u2500\u2500 ...\n</code></pre> <p>Adjacency matrix.  If the coordinates of the instances are available, the adjacency matrix will be built using the Euclidean distance between the coordinates.  Formally, the adjacency matrix \\(\\mathbf{A} = \\left[ A_{ij} \\right]\\) is defined as:</p> <p>\\begin{equation}  A_{ij} = \\begin{cases}  d_{ij}, &amp; \\text{if } \\left| \\mathbf{c}i - \\mathbf{c}_j \\right| \\leq \\text{dist_thr}, \\  0, &amp; \\text{otherwise},  \\end{cases} \\quad d{ij} = \\begin{cases}  1, &amp; \\text{if } \\text{adj_with_dist=False}, \\  \\exp\\left( -\\frac{\\left| \\mathbf{x}_i - \\mathbf{x}_j \\right|}{d} \\right), &amp; \\text{if } \\text{adj_with_dist=True}.  \\end{cases}  \\end{equation}</p> <p>where \\(\\mathbf{c}_i\\) and \\(\\mathbf{c}_j\\) are the coordinates of the instances \\(i\\) and \\(j\\), respectively, \\(\\text{dist_thr}\\) is a threshold distance,  and \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(\\mathbf{x}_j \\in \\mathbb{R}^d\\) are the features of instances \\(i\\) and \\(j\\), respectively.</p>"},{"location":"api/datasets/processed_mil_dataset/#torchmil.datasets.ProcessedMILDataset.__init__","title":"<code>__init__(features_path=None, labels_path=None, inst_labels_path=None, coords_path=None, bag_names=None, bag_keys=['X', 'Y', 'y_inst', 'adj', 'coords'], dist_thr=1.5, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>features_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the features.</p> </li> <li> <code>labels_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the bag labels.</p> </li> <li> <code>inst_labels_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the instance labels.</p> </li> <li> <code>coords_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the coordinates.</p> </li> <li> <code>bag_keys</code>               (<code>list</code>, default:                   <code>['X', 'Y', 'y_inst', 'adj', 'coords']</code> )           \u2013            <p>List of keys to load the bags data. The TensorDict returned by the <code>__getitem__</code> method will have these keys. Possible keys are: - \"X\": Load the features of the bag. - \"Y\": Load the label of the bag. - \"y_inst\": Load the instance labels of the bag. - \"adj\": Load the adjacency matrix of the bag. It requires the coordinates to be loaded. - \"coords\": Load the coordinates of the bag.</p> </li> <li> <code>bag_names</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of bag names to load. If None, all bags from the <code>features_path</code> are loaded.</p> </li> <li> <code>dist_thr</code>               (<code>float</code>, default:                   <code>1.5</code> )           \u2013            <p>Distance threshold for building the adjacency matrix.</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the instance features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/processed_mil_dataset/#torchmil.datasets.ProcessedMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the keys defined in <code>bag_keys</code> and their corresponding values.</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/rsnamil_dataset/","title":"RSNA dataset","text":""},{"location":"api/datasets/rsnamil_dataset/#torchmil.datasets.RSNAMILDataset","title":"<code>torchmil.datasets.RSNAMILDataset</code>","text":"<p>               Bases: <code>BinaryClassificationDataset</code>, <code>CTScanDataset</code></p> <p>RSNA Intracranial Hemorrhage Detection dataset for Multiple Instance Learning (MIL). Download it from Hugging Face Datasets.</p> <p>About the original RSNA Dataset. The original RSNA-ICH dataset contains head CT scans. The task is to identify whether a CT scan contains acute intracranial hemorrhage and its subtypes. The dataset includes a label for each slice. </p> <p>Dataset description. We have preprocessed the CT scans by computing features for each slice using various feature extractors.</p> <ul> <li>A slice is labeled as positive (<code>slice_label=1</code>) if it contains evidence of hemorrhage.</li> <li>A CT scan is labeled as positive (<code>label=1</code>) if it contains at least one positive slice.</li> </ul> <p>This means a CT scan is considered positive if there is any evidence of hemorrhage.</p> <p>Directory structure.</p> <p>After extracting the contents of the <code>.tar.gz</code> archives, the following directory structure is expected:</p> <pre><code>root\n\u251c\u2500\u2500 features\n\u2502   \u251c\u2500\u2500 features_{features}\n\u2502   \u2502   \u251c\u2500\u2500 ctscan_name1.npy\n\u2502   \u2502   \u251c\u2500\u2500 ctscan_name2.npy\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 labels\n\u2502   \u251c\u2500\u2500 ctscan_name1.npy\n\u2502   \u251c\u2500\u2500 ctscan_name2.npy\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 slice_labels\n\u2502   \u251c\u2500\u2500 ctscan_name1.npy\n\u2502   \u251c\u2500\u2500 ctscan_name2.npy\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 splits.csv\n</code></pre> <p>Each <code>.npy</code> file corresponds to a single CT scan. The <code>splits.csv</code> file defines train/test splits for standardized experimentation.</p>"},{"location":"api/datasets/rsnamil_dataset/#torchmil.datasets.RSNAMILDataset.__init__","title":"<code>__init__(root, features='resnet50', partition='train', bag_keys=['X', 'Y', 'y_inst', 'adj', 'coords'], adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>root</code>               (<code>str</code>)           \u2013            <p>Path to the root directory of the dataset.</p> </li> <li> <code>features</code>               (<code>str</code>, default:                   <code>'resnet50'</code> )           \u2013            <p>Type of features to use. Must be one of ['resnet18', 'resnet50', 'vit_b_32']</p> </li> <li> <code>partition</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Partition of the dataset. Must be one of ['train', 'test'].</p> </li> <li> <code>bag_keys</code>               (<code>list</code>, default:                   <code>['X', 'Y', 'y_inst', 'adj', 'coords']</code> )           \u2013            <p>List of keys to use for the bags. Must be in ['X', 'Y', 'y_inst', 'coords'].</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the patches features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/rsnamil_dataset/#torchmil.datasets.RSNAMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the keys defined in <code>bag_keys</code> and their corresponding values.</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/sc_standard_dataset/","title":"Single Concept Standard dataset","text":""},{"location":"api/datasets/sc_standard_dataset/#torchmil.datasets.SCStandardMILDataset","title":"<code>torchmil.datasets.SCStandardMILDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Single-Concept Standard MIL Dataset. Implementation from Algorithm 1 in Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests.</p>"},{"location":"api/datasets/sc_standard_dataset/#torchmil.datasets.SCStandardMILDataset.__init__","title":"<code>__init__(D, num_bags, B, pos_class_prob=0.5, train=True, seed=0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>D</code>               (<code>int</code>)           \u2013            <p>Dimensionality of the data.</p> </li> <li> <code>num_bags</code>               (<code>int</code>)           \u2013            <p>Number of bags in the dataset.</p> </li> <li> <code>B</code>               (<code>int</code>)           \u2013            <p>Number of negative instances in each bag.</p> </li> <li> <code>pos_class_prob</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Probability of a bag being positive.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Seed for the random number generator.</p> </li> </ul>"},{"location":"api/datasets/sc_standard_dataset/#torchmil.datasets.SCStandardMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Bag features of shape <code>(bag_size, feat_dim)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag.</li> </ul> </li> </ul>"},{"location":"api/datasets/toy_dataset/","title":"Toy dataset","text":""},{"location":"api/datasets/toy_dataset/#torchmil.datasets.ToyDataset","title":"<code>torchmil.datasets.ToyDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>This class represents a synthetic dataset for Multiple Instance Learning (MIL) tasks. It generates synthetic bags of instances from a given dataset, where each bag is labeled based on the presence or absence of specific \"positive\" instances. This class is particularly useful for simulating MIL scenarios, where the goal is to learn from bags of instances rather than individual data points.</p> <p>Bag generation. The dataset generates bags by sampling instances from the input  <code>(data, labels)</code> pair. A bag is labeled as positive if it contains at least one instance from a predefined set of positive labels (<code>obj_labels</code>). The probability of generating a positive bag can be controlled via the argument <code>pos_class_prob</code>. The size of each bag can be specified using the argument <code>bag_size</code>. Additionally, each bag includes instance-level labels, indicating whether individual instances belong to the positive class.</p> <p>Each bag is returned as a dictionary (TensorDict) with the following keys:</p> <ul> <li>X: The bag's feature matrix of shape <code>(bag_size, num_features)</code>.</li> <li>Y: The bag's label (1 for positive, 0 for negative).</li> <li>y_inst: The instance-level labels within the bag.</li> </ul> <p>MNIST example. We can create a MIL dataset from the original MNIST as follows:</p> <pre><code>import torch\nfrom torchvision import datasets, transforms\n\n# Load MNIST dataset\nmnist_train = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n\n# Extract features and labels\ndata = mnist_train.data.numpy().reshape(-1, 28*28) / 255\nlabels = mnist_train.targets.numpy()\n\n# Define positive labels\nobj_labels = [1, 2] # Digits 1 and 2 are considered positive\n\n# Create MIL dataset\ntoy_dataset = ToyDataset(data, labels, num_bags=1000, obj_labels=obj_labels, bag_size=10)\n\n# Retrieve a bag\nbag = toy_dataset[0]\nX, Y, y_inst = bag['X'], bag['Y'], bag['y_inst']\n</code></pre>"},{"location":"api/datasets/toy_dataset/#torchmil.datasets.ToyDataset.__init__","title":"<code>__init__(data, labels, num_bags, obj_labels, bag_size, pos_class_prob=0.5, seed=0)</code>","text":"<p>ToyMIL dataset class constructor.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>Data matrix of shape <code>(num_instances, num_features)</code>.</p> </li> <li> <code>labels</code>               (<code>ndarray</code>)           \u2013            <p>Labels vector of shape <code>(num_instances,)</code>.</p> </li> <li> <code>num_bags</code>               (<code>int</code>)           \u2013            <p>Number of bags to generate.</p> </li> <li> <code>obj_labels</code>               (<code>list[int]</code>)           \u2013            <p>List of labels to consider as positive.</p> </li> <li> <code>bag_size</code>               (<code>Union[int, tuple[int, int]]</code>)           \u2013            <p>Number of instances per bag. If a tuple <code>(min_size, max_size)</code> is provided, the bag size is sampled uniformly from this range.</p> </li> <li> <code>pos_class_prob</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Probability of generating a positive bag.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Random seed.</p> </li> </ul>"},{"location":"api/datasets/toy_dataset/#torchmil.datasets.ToyDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Bag features of shape <code>(bag_size, feat_dim)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag.</li> </ul> </li> </ul>"},{"location":"api/datasets/wsi_dataset/","title":"WSI dataset","text":""},{"location":"api/datasets/wsi_dataset/#torchmil.datasets.WSIDataset","title":"<code>torchmil.datasets.WSIDataset</code>","text":"<p>               Bases: <code>ProcessedMILDataset</code></p> <p>This class represents a dataset of Whole Slide Images (WSI) for Multiple Instance Learning (MIL).</p> <p>MIL and WSIs. Whole Slide Images (WSIs) are high-resolution images of tissue samples used in digital pathology. Due to their large size, WSIs are usually divided into smaller patches. In the context of MIL, a WSI is considered a bag, and the patches are considered instances.</p> <p>Patch and feature extraction. Different tools are available to obtain the previous directory structure from a set of WSIs. For example, given a set of WSIs in the original format (e.g., .tif extension), patches can be extracted using tools such as CLAM. This tool outputs the coordinates of the patches, which can be used to extract the patches from the WSIs. Then, a feature vector can be extracted from each patch using a pretrained model.</p> <p>Binary MIL for WSIs. In binary classification, the label \\(Y\\) of the WSI usually represents the presence (\\(Y=1\\)) or absence (\\(Y=0\\)) of a certain characteristic in the WSI. This characteristic can be present in one or more patches of the WSI, but the exact location of the characteristic is unknown. This translates into a patch having an unknown label \\(y_n\\), which is positive (\\(y_n=1\\)) if it contains the characteristic, and negative (\\(y_n=0\\)) otherwise. Consequently, \\(Y = \\max\\left\\{ y_1, y_2, \\ldots, y_N \\right\\}\\), where \\(N\\) is the number of patches in the WSI. This means that the WSI is positive (contains the characteristic) if at least one of its patches is positive (contains the characteristic). In the case that the WSI has been annotated at the patch level, the instance labels \\(y_n\\) can be used solely for evaluation purposes. See <code>torchmil.datasets.BinaryClassificationDataset</code> for more information.</p> <p>Directory structure. It is assumed that the bags have been processed and saved as numpy files. For more information on the processing of the bags, refer to the <code>ProcessedMILDataset</code> class. This dataset expects the following directory structure:</p> <pre><code>features_path\n\u251c\u2500\u2500 wsi1.npy\n\u251c\u2500\u2500 wsi2.npy\n\u2514\u2500\u2500 ...\nlabels_path\n\u251c\u2500\u2500 wsi1.npy\n\u251c\u2500\u2500 wsi2.npy\n\u2514\u2500\u2500 ...\ninst_labels_path\n\u251c\u2500\u2500 wsi1.npy\n\u251c\u2500\u2500 wsi2.npy\n\u2514\u2500\u2500 ...\ncoords_path\n\u251c\u2500\u2500 wsi1.npy\n\u251c\u2500\u2500 wsi2.npy\n\u2514\u2500\u2500 ...\n</code></pre> <p>Adjacency matrix. If the coordinates of the patches are available, an adjacency matrix representing the spatial relationships between the patches is built.</p> \\[\\begin{equation} A_{ij} = \\begin{cases} d_{ij}, &amp; \\text{if } \\left\\| \\mathbf{c}_i - \\mathbf{c}_j \\right\\| \\leq \\text{dist_thr}, \\\\ 0, &amp; \\text{otherwise}, \\end{cases} \\quad d_{ij} = \\begin{cases} 1, &amp; \\text{if } \\text{adj_with_dist=False}, \\\\ \\exp\\left( -\\frac{\\left\\| \\mathbf{x}_i - \\mathbf{x}_j \\right\\|}{d} \\right), &amp; \\text{if } \\text{adj_with_dist=True}. \\end{cases} \\end{equation}\\] <p>where \\(\\mathbf{c}_i\\) and \\(\\mathbf{c}_j\\) are the coordinates of the patches \\(i\\) and \\(j\\), respectively, \\(\\text{dist_thr}\\) is a threshold distance, and \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(\\mathbf{x}_j \\in \\mathbb{R}^d\\) are the features of patches \\(i\\) and \\(j\\), respectively. If no <code>dist_thr</code> is provided, it is set to \\(\\sqrt{2} \\times \\text{patch_size}\\).</p>"},{"location":"api/datasets/wsi_dataset/#torchmil.datasets.WSIDataset.__init__","title":"<code>__init__(features_path, labels_path, patch_labels_path=None, coords_path=None, wsi_names=None, bag_keys=['X', 'Y', 'y_inst', 'adj', 'coords'], patch_size=512, dist_thr=None, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>features_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing the feature matrices of the WSIs.</p> </li> <li> <code>labels_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing the labels of the WSIs.</p> </li> <li> <code>patch_labels_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the labels of the patches.</p> </li> <li> <code>coords_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the coordinates of the patches.</p> </li> <li> <code>wsi_names</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of the names of the WSIs to load. If None, all the WSIs in the <code>features_path</code> directory are loaded.</p> </li> <li> <code>bag_keys</code>               (<code>list</code>, default:                   <code>['X', 'Y', 'y_inst', 'adj', 'coords']</code> )           \u2013            <p>List of keys to use for the bags. Must be in ['X', 'Y', 'y_inst', 'coords'].</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Size of the patches.</p> </li> <li> <code>dist_thr</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Distance threshold for building the adjacency matrix. If None, it is set to <code>sqrt(2) * patch_size</code>.</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the patches features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/wsi_dataset/#torchmil.datasets.WSIDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the keys defined in <code>bag_keys</code> and their corresponding values.</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/models/","title":"torchmil.models","text":"<p>torchmil.models is a collection of popular Multiple Instance Learning (MIL) models implemented in PyTorch. They all inherit from the General MIL model class, which provides a common interface for all models.</p>"},{"location":"api/models/#available-models","title":"Available models","text":"<ul> <li>General MIL model</li> <li>ABMIL</li> <li>CAMIL</li> <li>CLAM</li> <li>DFTDMIL</li> <li>DSMIL</li> <li>GTP</li> <li>ProbSmoothABMIL</li> <li>SmABMIL</li> <li>SmTransformerABMIL</li> <li>TransformerABMIL</li> <li>TransformerProbSmoothABMIL</li> <li>TransMIL</li> <li>SETMIL</li> <li>IIBMIL</li> </ul>"},{"location":"api/models/abmil/","title":"ABMIL","text":""},{"location":"api/models/abmil/#torchmil.models.ABMIL","title":"<code>torchmil.models.ABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Attention-based Multiple Instance Learning (ABMIL) model, proposed in the paper Attention-based Multiple Instance Learning.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, it aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) using the attention-based pooling,</p> \\[ \\mathbf{z}, \\mathbf{f} = \\operatorname{AttentionPool}(\\mathbf{X}). \\] <p>where \\(\\mathbf{f} \\in \\mathbb{R}^{N}\\) are the attention values. See AttentionPool for more details on the attention-based pooling. The bag representation \\(\\mathbf{z}\\) is then fed into a classifier (one linear layer) to predict the bag label.</p>"},{"location":"api/models/abmil/#torchmil.models.ABMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=128, att_act='tanh', gated=False, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>att_act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for attention. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>gated</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use gated attention in the attention pooling.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/abmil/#torchmil.models.ABMIL.forward","title":"<code>forward(X, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/abmil/#torchmil.models.ABMIL.compute_loss","title":"<code>compute_loss(Y, X, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/abmil/#torchmil.models.ABMIL.predict","title":"<code>predict(X, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/camil/","title":"CAMIL","text":""},{"location":"api/models/camil/#torchmil.models.CAMIL","title":"<code>torchmil.models.CAMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Context-Aware Multiple Instance Learning (CAMIL) model, presented in the paper CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, a global bag representation is computed using a NystromTransformerLayer layer,</p> \\[ \\mathbf{T} = \\operatorname{NystromTransformerLayer}(\\mathbf{X})\\] <p>Next, a local bag representation is computed using the CAMILSelfAttention layer,</p> \\[ \\mathbf{L} = \\operatorname{CAMILSelfAttention}(\\mathbf{T}) \\] <p>Finally, the local and global information is fused as</p> \\[ \\mathbf{M} = \\operatorname{sigmoid}(\\mathbf{L}) \\odot \\mathbf{L} + (1 - \\operatorname{sigmoid}(\\mathbf{L})) \\odot \\mathbf{T},\\] <p>where \\(\\odot\\) denotes element-wise multiplication and \\(\\operatorname{sigmoid}\\) is the sigmoid function.</p> <p>Lastly, the final bag representation is computed using the CAMILAttentionPool, modification of the Gatted Attention Pool mechanism. The bag representation is then fed into a linear classifier to predict the bag label.</p>"},{"location":"api/models/camil/#torchmil.models.CAMIL.__init__","title":"<code>__init__(in_shape, nystrom_att_dim=512, pool_att_dim=128, gated_pool=False, n_heads=4, n_landmarks=None, pinv_iterations=6, dropout=0.0, use_mlp=False, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>pool_att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension for the attention pooling layer.</p> </li> <li> <code>gated_pool</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use gated attention pooling.</p> </li> <li> <code>nystrom_att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension for the Nystrom Transformer layer.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of attention heads in the Nystrom Transformer layer.</p> </li> <li> <code>n_landmarks</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of landmarks in the Nystrom Transformer layer.</p> </li> <li> <code>pinv_iterations</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of iterations for computing the pseudo-inverse in the Nystrom Transformer layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate of the Nystrom Transformer Layer.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use MLP in the Nystrom Transformer layer.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/camil/#torchmil.models.CAMIL.forward","title":"<code>forward(X, adj, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/camil/#torchmil.models.CAMIL.compute_loss","title":"<code>compute_loss(Y, X, adj, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/camil/#torchmil.models.CAMIL.predict","title":"<code>predict(X, adj, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/camil/#torchmil.models.camil.CAMILSelfAttention","title":"<code>torchmil.models.camil.CAMILSelfAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Self-attention layer used in CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images. This layer computes the self-attention values using the local information of the bag. The local information is captured using an adjacency matrix, which measures the similarity between the embeddings of instances in the bag.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), and an adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), this layer computes</p> \\[ \\mathbf{l}_i = \\frac{\\exp\\left(\\sum_{j=1}^N a_{ij} \\mathbf{q}_i^\\top \\mathbf{k}_j \\right)}{\\sum_{k=1}^N \\exp \\left(\\sum_{j=1}^N a_{kj} \\mathbf{q}_k^\\top \\mathbf{k}_j \\right)} \\mathbf{v}_i,\\] <p>where \\(\\mathbf{q}_i = \\mathbf{W_q}\\mathbf{x}_i\\), \\(\\mathbf{k}_i = \\mathbf{W_k}\\mathbf{x}_i\\), and \\(\\mathbf{v}_i = \\mathbf{W_v}\\mathbf{x}_i\\) are the query, key, and value vectors, respectively. Finally, it returns \\(\\mathbf{L} = \\left[ \\mathbf{l}_1, \\ldots, \\mathbf{l}_N \\right]^\\top\\).</p>"},{"location":"api/models/camil/#torchmil.models.camil.CAMILSelfAttention.forward","title":"<code>forward(X, adj, mask=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag of features of shape (batch_size, bag_size, in_dim)</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>. If None, no masking is applied.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>L</code> (              <code>Tensor</code> )          \u2013            <p>Self-attention vectors with shape (batch_size, bag_size, in_dim)</p> </li> </ul>"},{"location":"api/models/camil/#torchmil.models.camil.CAMILAttentionPool","title":"<code>torchmil.models.camil.CAMILAttentionPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Attention pooling layer as described in CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images.</p> <p>Given a bag of features \\(\\mathbf{T} = \\left[ \\mathbf{t}_1, \\ldots, \\mathbf{t}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\) and \\(\\mathbf{M} = \\left[ \\mathbf{m}_1, \\ldots, \\mathbf{m}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this layer computes the final bag representation \\(\\mathbf{z}\\) as</p> \\[\\begin{gather} \\mathbf{f} = \\mathbf{w}^\\top \\tanh(\\mathbf{T} \\mathbf{W} ) \\odot \\operatorname{sigmoid}(\\mathbf{T} \\mathbf{U}), \\\\ \\mathbf{s} = \\text{softmax}(\\mathbf{f}), \\\\ \\mathbf{z} = \\mathbf{M}^\\top \\mathbf{s}, \\end{gather}\\] <p>where \\(\\mathbf{W}, \\mathbf{U}\\) and \\(\\mathbf{w}\\) are learnable parameters. Note the difference with conventional AttentionPool layer, where the attention values and bag representation are computed from the same set of features.</p>"},{"location":"api/models/camil/#torchmil.models.camil.CAMILAttentionPool.forward","title":"<code>forward(T, M, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>T</code>               (<code>Tensor</code>)           \u2013            <p>(batch_size, bag_size, in_dim)</p> </li> <li> <code>M</code>               (<code>Tensor</code>)           \u2013            <p>(batch_size, bag_size, in_dim)</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>(batch_size, bag_size)</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values in addition to <code>z</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>Tensor</code> )          \u2013            <p>(batch_size, in_dim)</p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>(batch_size, bag_size) if `return_att</p> </li> </ul>"},{"location":"api/models/clam/","title":"CLAM","text":""},{"location":"api/models/clam/#torchmil.models.CLAM_SB","title":"<code>torchmil.models.CLAM_SB</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Clustering-constrained Attention Multiple Instance Learning (CLAM), proposed in the paper Data Efficient and Weakly Supervised Computational Pathology on Whole Slide Images.</p> <p>Overview. The forward pass of CLAM is identical to the forward pass of ABMIL. The difference lies in the instance-level regularization, which we describe below.</p> <p>Instance-level regularization. CLAM uses a binary clustering objective during training. For this, in the binary MIL setting, two clustering classifiers are considered: \\(c_0 \\colon \\mathbb{R}^D \\to \\mathbb{R}\\) and \\(c_1 \\colon \\mathbb{R}^D \\to \\mathbb{R}\\). To supervise this objective, the attention values computes by the attention pooling are used to generate pseudo labels.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with label \\(Y\\) and attention values \\(\\mathbf{f} = \\left[ f_1, \\ldots, f_N \\right]^\\top \\in \\mathbb{R}^{N}\\), the instance-level regularization is performed as follows:</p> <ol> <li> <p>The \\(k\\) instances with the highest attention values are selected as in-the-class instances. The \\(k\\) instances with the lowest attention values are selected as out-of-the-class instances,</p> \\[\\begin{gather}     D_{\\text{in}} = \\left\\{ \\mathbf{x}_i \\mid f_i \\in \\text{TopK}(\\mathbf{f}, k) \\right\\}, \\\\     D_{\\text{out}} = \\left\\{ \\mathbf{x}_i \\mid f_i \\in \\text{BottomK}(\\mathbf{f}, k) \\right\\}. \\end{gather}\\] </li> <li> <p>The instances in \\(D_{\\text{in}}\\) are assigned a pseudo label of 1 for \\(c_Y\\), and a pseudo label of 0 for \\(c_{1-Y}\\). The instances in \\(D_{\\text{out}}\\) are assigned a pseudo label of 0 for \\(c_Y\\). The pseudo labels are used to train the clustering classifiers,</p> \\[\\begin{gather}     \\ell_{\\text{in}} = \\frac{1}{2K} \\left( \\sum_{\\mathbf{x} \\in D_{\\text{in}}}\\ell_{\\text{inst}}(c_Y(\\mathbf{x}), 1) + \\sum_{\\mathbf{x} \\in D_{\\text{out}}}\\ell_{\\text{inst}}(c_{Y}(\\mathbf{x}), 0) \\right), \\\\     \\ell_{\\text{out}} = \\frac{1}{K} \\sum_{\\mathbf{x} \\in D_{\\text{in}}}\\ell_{\\text{inst}}(c_{1-Y}(\\mathbf{x}), 0), \\end{gather}\\] </li> </ol> <p>where \\(\\ell_{\\text{inst}}\\) is the instance-level loss function (the default is SmoothTop1SVM) and \\(Y\\) is the true bag label. The total instance-level loss is \\(\\ell_{\\text{in}} + \\ell_{\\text{out}}\\), which is added to the bag-level loss to train the model.</p>"},{"location":"api/models/clam/#torchmil.models.CLAM_SB.__init__","title":"<code>__init__(in_shape=None, att_dim=128, att_act='tanh', k_sample=10, gated=False, inst_loss_name='SmoothTop1SVM', feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>att_act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for attention. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>k_sample</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of instances to sample.</p> </li> <li> <code>gated</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use gated attention in the attention pooling.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/clam/#torchmil.models.CLAM_SB.forward","title":"<code>forward(X, mask=None, return_att=False, return_emb=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_emb</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns embeddings in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> <li> <code>emb</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_emb=True</code>. Embeddings of shape (batch_size, bag_size, feat_dim).</p> </li> </ul>"},{"location":"api/models/clam/#torchmil.models.CLAM_SB.compute_loss","title":"<code>compute_loss(Y, X, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/clam/#torchmil.models.CLAM_SB.predict","title":"<code>predict(X, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Predicted bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>Predicted instance labels of shape <code>(batch_size, bag_size)</code>. Only returned when <code>return_inst_pred=True</code>.</p> </li> </ul>"},{"location":"api/models/deepgraphsurv/","title":"DeepGraphSurv","text":""},{"location":"api/models/deepgraphsurv/#torchmil.models.DeepGraphSurv","title":"<code>torchmil.models.DeepGraphSurv</code>","text":"<p>               Bases: <code>Module</code></p> <p>DeepGraphSurv model, as proposed in Graph CNN for Survival Analysis on Whole Slide Pathological Images.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, the representation branch transforms the instance features using a Graph Convolutional Network (GCN), and the attention branch computes the attention values \\(\\mathbf{f}\\) using another GCN,</p> \\[\\begin{gather} \\mathbf{H} = \\operatorname{GCN}_{\\text{rep}}(\\mathbf{X}, \\mathbf{A}) \\in \\mathbb{R}^{N \\times \\texttt{hidden_dim}}, \\\\ \\mathbf{f} = \\operatorname{GCN}_{\\text{att}}(\\mathbf{X}, \\mathbf{A}) \\in \\mathbb{R}^{N \\times 1}. \\end{gather}\\] <p>These GCNs are implemented using the DeepGCN layer (see DeepGCNLayer) with GCNConv, LayerNorm, and ReLU activation (see GCNConv).</p> <p>Writing \\(\\mathbf{H} = \\left[ \\mathbf{h}_1, \\ldots, \\mathbf{h}_N \\right]^\\top\\), the attention values are used to compute the bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{\\texttt{hidden_dim}}\\) as</p> \\[\\begin{equation} \\mathbf{z} = \\mathbf{H}^\\top \\operatorname{Softmax}(\\mathbf{f}) = \\sum_{n=1}^N s_n \\mathbf{h}_n, \\end{equation}\\] <p>where \\(s_n\\) is the normalized attention score for the \\(n\\)-th instance. The bag representation \\(\\mathbf{z}\\) is then fed into a classifier (one linear layer) to predict the bag label.</p>"},{"location":"api/models/deepgraphsurv/#torchmil.models.DeepGraphSurv.__init__","title":"<code>__init__(in_shape=None, n_layers_rep=4, n_layers_att=2, hidden_dim=None, att_dim=128, dropout=0.0, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>n_layers_rep</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of GCN layers in the representation branch.</p> </li> <li> <code>n_layers_att</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Number of GCN layers in the attention branch.</p> </li> <li> <code>hidden_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Hidden dimension. If not provided, it will be set to the feature dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function.</p> </li> </ul>"},{"location":"api/models/deepgraphsurv/#torchmil.models.DeepGraphSurv.forward","title":"<code>forward(X, adj, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/deepgraphsurv/#torchmil.models.DeepGraphSurv.compute_loss","title":"<code>compute_loss(Y, X, adj, mask=None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss</p> </li> </ul>"},{"location":"api/models/deepgraphsurv/#torchmil.models.DeepGraphSurv.predict","title":"<code>predict(X, adj, mask=None, return_inst_pred=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns instance predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/dsmil/","title":"DSMIL","text":""},{"location":"api/models/dsmil/#torchmil.models.DSMIL","title":"<code>torchmil.models.DSMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Dual-stream Multiple Instance Learning (DSMIL) model, proposed in the paper Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning.</p> <p>Overview. Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, two streams are used. The first stream uses an instance classifier \\(c \\ \\colon \\mathbb{R}^D \\to \\mathbb{R}\\) (implemented as a linear layer) and retrieves the instance with the highest logit score,</p> \\[ m = \\arg \\max \\{ c(\\mathbf{x}_1), \\ldots, c(\\mathbf{x}_N) \\}. \\] <p>Then, the second stream computes the bag representation \\(\\mathbf{z} \\in \\mathbb{R}^D\\) as</p> \\[ \\mathbf{z} = \\frac{ \\exp \\left( \\mathbf{q}_i^\\top \\mathbf{q}_m \\right)}{\\sum_{k=1}^N \\exp \\left( \\mathbf{q}_k^\\top \\mathbf{q}_m \\right)} \\mathbf{v}_i, \\] <p>where \\(\\mathbf{q}_i = \\mathbf{W}_q \\mathbf{x}_i\\) and \\(\\mathbf{v}_i = \\mathbf{W}_v \\mathbf{x}_i\\). This is similar to self-attention with the difference that query-key matching is performed only with the critical instance.</p> <p>Finally, the bag representation is used to predict the bag label using a bag classifier implemented as a linear layer.</p> <p>Loss function. By default, the model is trained end-to-end using the followind per-bag loss:</p> \\[ \\ell = \\ell_{\\text{BCE}}(Y, \\hat{Y}) + \\ell_{\\text{BCE}}(Y, c(\\mathbf{x}_m)), \\] <p>where \\(\\ell_{\\text{BCE}}\\) is the Binary Cross-Entropy loss, \\(Y\\) is the true bag label, \\(\\hat{Y}\\) is the predicted bag label, and \\(c(\\mathbf{x}_m)\\) is the predicted label of the critical instance.</p>"},{"location":"api/models/dsmil/#torchmil.models.DSMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=128, nonlinear_q=False, nonlinear_v=False, dropout=0.0, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>nonlinear_q</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply nonlinearity to the query.</p> </li> <li> <code>nonlinear_v</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply nonlinearity to the value.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/dsmil/#torchmil.models.DSMIL.forward","title":"<code>forward(X, mask=None, return_att=False, return_inst_pred=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns instance label logits in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> <li> <code>y_pred</code> (              <code>tuple[Tensor, Tensor]</code> )          \u2013            <p>Only returned when <code>return_inst_pred=True</code>. Instance label logits of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/dsmil/#torchmil.models.DSMIL.compute_loss","title":"<code>compute_loss(Y, X, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/dsmil/#torchmil.models.DSMIL.predict","title":"<code>predict(X, mask=None, return_inst_pred=False)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/dtfdmil/","title":"DFTDMIL","text":""},{"location":"api/models/dtfdmil/#torchmil.models.DTFDMIL","title":"<code>torchmil.models.DTFDMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Double-Tier Feature Distillation Multiple Instance Learning (DFTD-MIL) model, proposed in the paper DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification.</p> <p>Overview. Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, the instances in a bag are randomly grouped in \\(M\\) pseudo-bags \\(\\{\\mathbf{X}_1, \\cdots, \\mathbf{X}_M\\}\\) with approximately the same number of instances. Each pseudo-bag is assigned its parent's bag label \\(Y_m = Y\\). Then, the model has two prediction tiers:</p> <p>In Tier 1, the model uses the attention pool (see AttentionPool for details) and a classifier, jointly noted as \\(T_1\\) to predict the label of each pseudo-bag,</p> \\[ \\widehat{Y}_m = T_1(\\mathbf{X}_m).\\] <p>The loss associated to this tier is the binary cross entropy computed using the pseudo-bag labels \\(Y_m\\) and the predicted label \\(\\widehat{Y}_m\\).</p> <p>In Tier 2, Grad-CAM (see Grad-CAM for details) is used to compute the probability of each instance. Based on that probability, a feature vector \\(\\mathbf{z}^m\\) is distilled for the \\(m\\)-th pseudo-bag. Then, the model uses another attention pool and a classifier, jointly noted as \\(T_2\\) to predict the final label of the bag,</p> \\[ \\widehat{Y} = T_2\\left( \\left[ \\mathbf{z}_1, \\ldots, \\mathbf{z}_M \\right]^\\top  \\right).\\] <p>The loss associated to this tier is the binary cross entropy computed using the bag labels \\(Y\\) and the predicted label \\(\\widehat{Y}\\).</p> <p>Loss function. By default, the model is trained end-to-end using the followind per-bag loss:</p> \\[ \\ell = \\ell_{\\text{BCE}}(Y, \\widehat{Y}) + \\frac{1}{M} \\sum_{m=1}^{M} \\ell_{\\text{BCE}}(Y_m, \\widehat{Y}_m),\\] <p>where \\(\\ell_{\\text{BCE}}\\) is the binary cross entropy loss.</p>"},{"location":"api/models/dtfdmil/#torchmil.models.DTFDMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=128, n_groups=8, distill_mode='maxmin', feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_groups</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of groups to split the bag instances.</p> </li> <li> <code>distill_mode</code>               (<code>str</code>, default:                   <code>'maxmin'</code> )           \u2013            <p>Distillation mode. Possible values: 'maxmin', 'max', 'afs'.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/dtfdmil/#torchmil.models.DTFDMIL.forward","title":"<code>forward(X, mask=None, return_pseudo_pred=False, return_inst_cam=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_pseudo_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns pseudo label logits in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_inst_cam</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns instance-level CAM values in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>inst_cam</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_inst_cam=True</code>. Instance-level CAM values of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/dtfdmil/#torchmil.models.DTFDMIL.compute_loss","title":"<code>compute_loss(Y, X, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/dtfdmil/#torchmil.models.DTFDMIL.predict","title":"<code>predict(X, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/gtp/","title":"GTP","text":""},{"location":"api/models/gtp/#torchmil.models.GTP","title":"<code>torchmil.models.GTP</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Method proposed in the paper GTP: Graph-Transformer for Whole Slide Image Classification.</p> <p>Forward pass. Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>The bags are processed using a Graph Convolutional Network (GCN) to extract high-level instance embeddings. This GCN leverages a graph \\(\\mathbf{A}\\) constructed from the bag, where nodes correspond to patches, and edges are determined based on spatial adjacency:</p> \\[ \\mathbf{H} = \\text{GCN}(\\mathbf{X}, \\mathbf{A}) \\in \\mathbb{R}^{N \\times D}.\\] <p>To reduce the number of nodes while preserving structural relationships, a min-cut pooling operation is applied:</p> \\[ \\mathbf{X}', \\mathbf{A}' = \\text{MinCutPool}(\\mathbf{H}, \\mathbf{A}).\\] <p>The pooled graph is then passed through a Transformer encoder, where a class token is introduced:</p> \\[ \\mathbf{Z} = \\text{Transformer}([\\text{CLS}; \\mathbf{X}']) \\in \\mathbb{R}^{(N' + 1) \\times D}.\\] <p>Finally, the class token representation is used for classification:</p> \\[ \\mathbf{z} = \\mathbf{Z}_{0}, \\quad Y_{\\text{pred}} = \\text{Classifier}(\\mathbf{z}).\\] <p>Optionally, GraphCAM can be used to generate class activation maps highlighting the most relevant regions for the classification decision.</p> <p>Loss function. By default, the model is trained end-to-end using the followind per-bag loss:</p> \\[ \\ell = \\ell_{\\text{BCE}}(Y_{\\text{pred}}, Y) + \\ell_{\\text{MinCut}}(\\mathbf{X}, \\mathbf{A}) + \\ell_{\\text{Ortho}}(\\mathbf{X}, \\mathbf{A}),\\] <p>where \\(\\ell_{\\text{BCE}}\\) is the Binary Cross-Entropy loss, \\(\\ell_{\\text{MinCut}}\\) is the MinCut loss, and \\(\\ell_{\\text{Ortho}}\\) is the Orthogonality loss, computed during the min-cut pooling operation, see Dense MinCut Pooling.</p>"},{"location":"api/models/gtp/#torchmil.models.GTP.__init__","title":"<code>__init__(in_shape, att_dim=512, n_clusters=100, n_layers=1, n_heads=8, use_mlp=True, dropout=0.0, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension for transformer encoder.</p> </li> <li> <code>n_clusters</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of clusters in mincut pooling.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in transformer encoder.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads in transformer encoder.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use MLP in transformer encoder.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in transformer encoder.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/gtp/#torchmil.models.GTP.forward","title":"<code>forward(X, adj, mask=None, return_cam=False, return_loss=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_cam</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the class activation map in addition to <code>Y_logits_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>cam</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_cam=True</code>. Class activation map of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/gtp/#torchmil.models.GTP.compute_loss","title":"<code>compute_loss(Y, X, adj, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/gtp/#torchmil.models.GTP.predict","title":"<code>predict(X, adj, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/iibmil/","title":"IIBMIL","text":""},{"location":"api/models/iibmil/#torchmil.models.IIBMIL","title":"<code>torchmil.models.IIBMIL</code>","text":"<p>               Bases: <code>Module</code></p> <p>Integrated Instance-Level and Bag-Level Multiple Instance Learning (IIB-MIL) model, proposed in the paper IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, a TransformerEncoder is applied to transform the instance features using context information. Subsequently, the model uses bag-level and instance-level supervision:</p> <p>Bag-level supervision: The instances are aggregated into a class token using a transformer decoder. A linear layer is then applied to predict the bag label.</p> <p>Instance-level supervision: Consists of four steps.</p> <ol> <li>Using an instance classifier, obtain the probability of instance \\(i\\) belonging to class \\(c\\), denoted as \\(p_{i,c}\\).</li> <li>The prototype \\(\\mathbf{p}_{c,t} \\in \\mathbf{R}^{D}\\) of class \\(c\\) at time \\(t\\) is updated using a momentum update rule based on the set of instances with the top \\(k\\) highest probabilities of belonging to class \\(c\\). Writing \\(\\mathbf{P}_t = \\left[ \\mathbf{p}_{1,t}, \\ldots, \\mathbf{p}_{C,t}  \\right]^\\top \\in \\mathbb{R}^{C \\times D}\\), the prototype label \\(z_{i}\\) of each instance is obtained as \\(z_{i} = \\text{argmax}_{c} \\ \\mathbf{P} \\mathbf{x}_i\\).</li> <li>Compute instance-level soft labels using the prototype labels and a momentum update.</li> <li>Compute the instance-level cross-entropy loss using the soft labels and the instance classifier.</li> </ol>"},{"location":"api/models/iibmil/#torchmil.models.IIBMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=256, n_layers_encoder=1, n_layers_decoder=1, use_mlp_encoder=True, use_mlp_decoder=False, n_heads=4, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_layers_encoder</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in the transformer encoder.</p> </li> <li> <code>n_layers_decoder</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in the transformer decoder.</p> </li> <li> <code>use_mlp_encoder</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, uses a multi-layer perceptron (MLP) in the encoder.</p> </li> <li> <code>use_mlp_decoder</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, uses a multi-layer perceptron (MLP) in the decoder.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of attention heads.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/iibmil/#torchmil.models.IIBMIL.forward","title":"<code>forward(X, mask=None, return_inst_pred=False, return_X_enc=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_X_enc</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns instance embeddings in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_inst_pred=True</code>. Instance label logits of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>X_enc</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_X_enc=True</code>. Instance embeddings of shape <code>(batch_size, bag_size, att_dim)</code>.</p> </li> </ul>"},{"location":"api/models/iibmil/#torchmil.models.IIBMIL.compute_loss","title":"<code>compute_loss(Y, X, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/iibmil/#torchmil.models.IIBMIL.predict","title":"<code>predict(X, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/iibmil/#torchmil.models.IIBMIL.update_prototypes","title":"<code>update_prototypes(X, mask=None, proto_m=0.9)</code>","text":"<p>Update prototypes.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>proto_m</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Momentum for updating prototypes</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul>"},{"location":"api/models/mil_model/","title":"General MIL model","text":""},{"location":"api/models/mil_model/#torchmil.models.MILModel","title":"<code>torchmil.models.MILModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for Multiple Instance Learning (MIL) models in torchmil.</p> <p>Subclasses should implement the following methods:</p> <ul> <li><code>forward</code>: Forward pass of the model. Accepts bag features (and optionally other arguments) and returns the bag label prediction (and optionally other outputs).</li> <li><code>compute_loss</code>: Compute inner losses of the model. Accepts bag features (and optionally other arguments) and returns the output of the forward method a dictionary of pairs (loss_name, loss_value). By default, the model has no inner losses, so this dictionary is empty.</li> <li><code>predict</code>: Predict bag and (optionally) instance labels. Accepts bag features (and optionally other arguments) and returns label predictions (and optionally instance label predictions).</li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the module.</p>"},{"location":"api/models/mil_model/#torchmil.models.MILModel.forward","title":"<code>forward(X, *args, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label prediction of shape <code>(batch_size,)</code>.</p> </li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModel.compute_loss","title":"<code>compute_loss(Y, X, *args, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label prediction of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss values.</p> </li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModel.predict","title":"<code>predict(X, return_inst_pred=False, *args, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label prediction of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModelWrapper","title":"<code>torchmil.models.MILModelWrapper</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>A wrapper class for MIL models in torchmil. It allows to use all models that inherit from <code>MILModel</code> using a common interface:</p> <pre><code>model_A = ... # forward accepts arguments 'X', 'adj'\nmodel_B = ... # forward accepts arguments 'X''\nmodel_A_w = MILModelWrapper(model_A)\nmodel_B_w = MILModelWrapper(model_B)\n\nbag = TensorDict({'X': ..., 'adj': ..., ...})\nY_pred_A = model_A_w(bag) # calls model_A.forward(X=bag['X'], adj=bag['adj'])\nY_pred_B = model_B_w(bag) # calls model_B.forward(X=bag['X'])\n</code></pre>"},{"location":"api/models/mil_model/#torchmil.models.MILModelWrapper.__init__","title":"<code>__init__(model)</code>","text":""},{"location":"api/models/mil_model/#torchmil.models.MILModelWrapper.forward","title":"<code>forward(bag, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>bag</code>               (<code>TensorDict</code>)           \u2013            <p>Dictionary containing one key for each argument accepted by the model's <code>forward</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>Any</code> )          \u2013            <p>Output of the model's <code>forward</code> method.</p> </li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModelWrapper.compute_loss","title":"<code>compute_loss(bag, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>bag</code>               (<code>TensorDict</code>)           \u2013            <p>Dictionary containing one key for each argument accepted by the model's <code>forward</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>tuple[Any, dict]</code> )          \u2013            <p>Output of the model's <code>compute_loss</code> method.</p> </li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModelWrapper.predict","title":"<code>predict(bag, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>bag</code>               (<code>TensorDict</code>)           \u2013            <p>Dictionary containing one key for each argument accepted by the model's <code>forward</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>Any</code> )          \u2013            <p>Output of the model's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/models/patch_gcn/","title":"PatchGCN","text":""},{"location":"api/models/patch_gcn/#torchmil.models.PatchGCN","title":"<code>torchmil.models.PatchGCN</code>","text":"<p>               Bases: <code>Module</code></p> <p>PatchGCN model, as proposed in Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction using Patch-based Graph Convolutional Networks.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, a Graph Convolutional Network (GCN) and a Multi-Layer Perceptron (MLP) are used to transform the instance features,</p> \\[\\begin{gather} \\mathbf{H} = \\operatorname{GCN}(\\mathbf{X}, \\mathbf{A}) \\in \\mathbb{R}^{N \\times \\texttt{out_gcn_dim}}, \\\\ \\mathbf{H} = \\operatorname{MLP}(\\mathbf{H}) \\in \\mathbb{R}^{N \\times \\texttt{hidden_dim}}, \\end{gather}\\] <p>where \\(\\texttt{out_gcn_dim} = \\texttt{hidden_dim} \\cdot \\texttt{n_gcn_layers}\\). These GCNs are implemented using the DeepGCN layer (see DeepGCNLayer) with GCNConv, LayerNorm, and ReLU activation (see GCNConv), along with residual connections and dense connections.</p> <p>Then, attention values \\(\\mathbf{f} \\in \\mathbb{R}^{N \\times 1}\\) and the bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{\\texttt{hidden_dim}}\\) are computed using the attention pooling mechanism (see Attention Pooling),</p> \\[\\begin{equation} \\mathbf{z}, \\mathbf{f} = \\operatorname{AttentionPool}(\\mathbf{H}). \\end{equation}\\] <p>Finally, the bag representation \\(\\mathbf{z}\\) is fed into a classifier (one linear layer) to predict the bag label.</p>"},{"location":"api/models/patch_gcn/#torchmil.models.PatchGCN.__init__","title":"<code>__init__(in_shape, n_gcn_layers=4, mlp_depth=1, hidden_dim=None, att_dim=128, dropout=0.0, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>n_gcn_layers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of GCN layers.</p> </li> <li> <code>mlp_depth</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in the MLP (applied after the GCN).</p> </li> <li> <code>hidden_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Hidden dimension. If not provided, it will be set to the feature dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function.</p> </li> </ul>"},{"location":"api/models/patch_gcn/#torchmil.models.PatchGCN.forward","title":"<code>forward(X, adj, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/patch_gcn/#torchmil.models.PatchGCN.compute_loss","title":"<code>compute_loss(Y, X, adj, mask=None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss</p> </li> </ul>"},{"location":"api/models/patch_gcn/#torchmil.models.PatchGCN.predict","title":"<code>predict(X, adj, mask=None, return_inst_pred=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns instance predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/prob_smooth_abmil/","title":"ProbSmoothABMIL","text":""},{"location":"api/models/prob_smooth_abmil/#torchmil.models.ProbSmoothABMIL","title":"<code>torchmil.models.ProbSmoothABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Attention-based Multiple Instance Learning (ABMIL) model with Probabilistic Smooth Attention Pooling. Proposed in Probabilistic Smooth Attention for Deep Multiple Instance Learning in Medical Imaging and Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection</p> <p>Overview. This model extends the ABMIL model by incorporating a probabilistic pooling mechanism.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Subsequently, it aggregates the instance features into a bag representation using a probabilistic attention-based pooling mechanism, as detailed in ProbSmoothAttentionPool.</p> <p>Specifically, it computes a mean vector \\(\\mathbf{\\mu}_{\\mathbf{f}} \\in \\mathbb{R}^N\\) and a variance vector \\(\\mathbf{\\sigma}_{\\mathbf{f}^2} \\in \\mathbb{R}^N\\) that define the attention distribution \\(q(\\mathbf{f} \\mid \\mathbf{X}) = \\mathcal{N}\\left(\\mathbf{f} \\mid \\mathbf{\\mu}_{\\mathbf{f}}, \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2) \\right)\\),</p> \\[ \\mathbf{\\mu}_{\\mathbf{f}}, \\mathbf{\\sigma}_{\\mathbf{f}} = \\operatorname{ProbSmoothAttentionPool}(\\mathbf{X}). \\] <p>If <code>covar_mode='zero'</code>, the variance vector \\(\\mathbf{\\sigma}_{\\mathbf{f}}^2\\) is set to zero, resulting in a deterministic attention distribution.</p> <p>Then, \\(m\\) attention vectors \\(\\widehat{\\mathbf{F}} = \\left[ \\widehat{\\mathbf{f}}^{(1)}, \\ldots, \\widehat{\\mathbf{f}}^{(m)} \\right]^\\top \\in \\mathbb{R}^{m \\times N}\\) are sampled from the attention distribution. The bag representation \\(\\widehat{\\mathbf{z}} \\in \\mathbb{R}^{m \\times D}\\) is then computed as:</p> \\[ \\widehat{\\mathbf{z}} = \\operatorname{Softmax}(\\widehat{\\mathbf{F}}) \\mathbf{X}. \\] <p>The bag representation \\(\\widehat{\\mathbf{z}}\\) is fed into a classifier, implemented as a linear layer, to produce bag label predictions \\(Y_{\\text{pred}} \\in \\mathbb{R}^{m}\\).</p> <p>Notably, the attention distribution naturally induces a distribution over the bag label predictions. This model thus generates multiple predictions for each bag, corresponding to different samples from this distribution.</p> <p>Regularization. The probabilistic pooling mechanism introduces a regularization term to the loss function that encourages smoothness in the attention values. Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), the regularization term corresponds to</p> \\[     \\ell_{\\text{KL}}(\\mathbf{X}, \\mathbf{A}) =         \\begin{cases}             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} \\quad &amp; \\text{if } \\texttt{covar_mode='zero'}, \\\\             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} + \\operatorname{Tr}(\\mathbf{L} \\mathbf{\\Sigma}_{\\mathbf{f}}) - \\frac{1}{2}\\log \\det( \\mathbf{\\Sigma}_{\\mathbf{f}} ) + \\operatorname{const} \\quad &amp; \\text{if } \\texttt{covar_mode='diag'}, \\\\         \\end{cases} \\] <p>where \\(\\operatorname{const}\\) is a constant term that does not depend on the parameters, \\(\\mathbf{\\Sigma}_{\\mathbf{f}} = \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2)\\), \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) is the graph Laplacian matrix, and \\(\\mathbf{D}\\) is the degree matrix of \\(\\mathbf{A}\\). This term is then averaged for all bags in the batch and added to the loss function.</p>"},{"location":"api/models/prob_smooth_abmil/#torchmil.models.ProbSmoothABMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=128, covar_mode='diag', n_samples_train=1000, n_samples_test=5000, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>covar_mode</code>               (<code>str</code>, default:                   <code>'diag'</code> )           \u2013            <p>Covariance mode for the Gaussian prior. Possible values: 'diag', 'full'.</p> </li> <li> <code>n_samples_train</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Number of samples for training.</p> </li> <li> <code>n_samples_test</code>               (<code>int</code>, default:                   <code>5000</code> )           \u2013            <p>Number of samples for testing.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/prob_smooth_abmil/#torchmil.models.ProbSmoothABMIL.forward","title":"<code>forward(X, adj=None, mask=None, return_att=False, return_samples=False, return_kl_div=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>. Only required when <code>return_kl_div=True</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_samples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and <code>return_att=True</code>, the attention values returned are samples from the attention distribution.</p> </li> <li> <code>return_kl_div</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the KL divergence between the attention distribution and the prior distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size, n_samples)</code> if <code>return_samples=True</code>, else <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape <code>(batch_size, bag_size, n_samples)</code> if <code>return_samples=True</code>, else <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>kl_div</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_kl_div=True</code>. KL divergence between the attention distribution and the prior distribution of shape <code>()</code>.</p> </li> </ul>"},{"location":"api/models/prob_smooth_abmil/#torchmil.models.ProbSmoothABMIL.compute_loss","title":"<code>compute_loss(Y, X, adj, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value and the KL divergence between the attention distribution and the prior distribution.</p> </li> </ul>"},{"location":"api/models/prob_smooth_abmil/#torchmil.models.ProbSmoothABMIL.predict","title":"<code>predict(X, mask=None, return_inst_pred=True, return_samples=False)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, returns the attention values as instance labels predictions, in addition to bag label predictions.</p> </li> <li> <code>return_samples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and <code>return_inst_pred=True</code>, the instance label predictions returned are samples from the instance label distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_inst_pred=True</code>. Attention values (before normalization) of shape <code>(batch_size, bag_size)</code> if <code>return_samples=False</code>, else <code>(batch_size, bag_size, n_samples)</code>.</p> </li> </ul>"},{"location":"api/models/setmil/","title":"SETMIL","text":""},{"location":"api/models/setmil/#torchmil.models.SETMIL","title":"<code>torchmil.models.SETMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>SETMIL: Spatial Encoding Transformer-Based Multiple Instance Learning for Pathological Image Analysis (SETMIL) model, proposed in the paper SETMIL: Spatial Encoding Transformer-Based Multiple Instance Learning for Pathological Image Analysis.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, the Pyramid Multi-Scale Fusion (PMF) module enriches the representation with multi-scale context information. The PMF module consists of three T2T modules with different kernel sizes, \\(k = 3, 5, 7\\), concatenated along the feature dimension,</p> \\[\\operatorname{PMF}\\left( \\mathbf{X} \\right) = \\text{Concat}(\\text{T2T}_{k=3}(\\mathbf{X}), \\text{T2T}_{k=5}(\\mathbf{X}), \\text{T2T}_{k=7}(\\mathbf{X})).\\] <p>See T2T and T2TLayer for further information.</p> <p>Then, the model applies a Spatial Encoding Transformer (SET), which consists of a stack of transformer layers with image Relative Positional Encoding (iRPE). See iRPETransformer for further information.</p> <p>Finally, using the class token computed by the SET module, the model predicts the bag label \\(\\hat{Y}\\) using a linear layer.</p> <p>Note. When <code>use_pmf=True</code>, the input bag is reshaped to a square shape, and the PMF module is applied. This modifies the bag structure unreversibly, and thus attention values cannot be computed.  If <code>return_att=True</code>, the attention values will be set to zeros.</p>"},{"location":"api/models/setmil/#torchmil.models.SETMIL.__init__","title":"<code>__init__(in_shape, att_dim=512, use_pmf=False, pmf_n_heads=4, pmf_use_mlp=True, pmf_dropout=0.0, pmf_kernel_list=[(3, 3), (5, 5), (7, 7)], pmf_stride_list=[(1, 1), (1, 1), (1, 1)], pmf_padding_list=[(1, 1), (2, 2), (3, 3)], pmf_dilation_list=[(1, 1), (1, 1), (1, 1)], set_n_layers=1, set_n_heads=4, set_use_mlp=True, set_dropout=0.0, rpe_ratio=1.9, rpe_method='product', rpe_mode='contextual', rpe_shared_head=True, rpe_skip=1, rpe_on='k', feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension used by the PMF and SET modules.</p> </li> <li> <code>use_pmf</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use Pyramid Multihead Feature (PMF) before the SET module.</p> </li> <li> <code>pmf_n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the PMF module.</p> </li> <li> <code>pmf_use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, use MLP in the PMF module.</p> </li> <li> <code>pmf_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in the PMF module.</p> </li> <li> <code>pmf_kernel_list</code>               (<code>list[tuple[int, int]]</code>, default:                   <code>[(3, 3), (5, 5), (7, 7)]</code> )           \u2013            <p>List of kernel sizes in the PMF module.</p> </li> <li> <code>pmf_stride_list</code>               (<code>list[tuple[int, int]]</code>, default:                   <code>[(1, 1), (1, 1), (1, 1)]</code> )           \u2013            <p>List of stride sizes in the PMF module.</p> </li> <li> <code>pmf_padding_list</code>               (<code>list[tuple[int, int]]</code>, default:                   <code>[(1, 1), (2, 2), (3, 3)]</code> )           \u2013            <p>List of padding sizes in the PMF module.</p> </li> <li> <code>pmf_dilation_list</code>               (<code>list[tuple[int, int]]</code>, default:                   <code>[(1, 1), (1, 1), (1, 1)]</code> )           \u2013            <p>List of dilation sizes in the PMF module.</p> </li> <li> <code>set_n_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in the SET module.</p> </li> <li> <code>set_n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the SET module.</p> </li> <li> <code>set_use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, use MLP in the SET module.</p> </li> <li> <code>set_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in the SET module.</p> </li> <li> <code>rpe_ratio</code>               (<code>float</code>, default:                   <code>1.9</code> )           \u2013            <p>Ratio for relative positional encoding.</p> </li> <li> <code>rpe_method</code>               (<code>str</code>, default:                   <code>'product'</code> )           \u2013            <p>Method for relative positional encoding. Possible values: ['euc', 'quant', 'cross', 'product']</p> </li> <li> <code>rpe_mode</code>               (<code>str</code>, default:                   <code>'contextual'</code> )           \u2013            <p>Mode for relative positional encoding. Possible values: [None, 'bias', 'contextual']</p> </li> <li> <code>rpe_shared_head</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, share weights across different heads.</p> </li> <li> <code>rpe_skip</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of tokens to skip in the relative positional encoding. Possible values: [0, 1].</p> </li> <li> <code>rpe_on</code>               (<code>str</code>, default:                   <code>'k'</code> )           \u2013            <p>Where to apply relative positional encoding. Possible values: ['q', 'k', 'v', 'qk', 'kv', 'qkv'].</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/setmil/#torchmil.models.SETMIL.forward","title":"<code>forward(X, coords, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, feat_dim)</code>.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>Coordinates of shape <code>(batch_size, bag_size, coord_dim)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/setmil/#torchmil.models.SETMIL.compute_loss","title":"<code>compute_loss(Y, X, coords)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>Coordinates of shape <code>(batch_size, bag_size, coord_dim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/setmil/#torchmil.models.SETMIL.predict","title":"<code>predict(X, coords, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>Coordinates of shape <code>(batch_size, bag_size, coord_dim)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/sm_abmil/","title":"SmABMIL","text":""},{"location":"api/models/sm_abmil/#torchmil.models.SmABMIL","title":"<code>torchmil.models.SmABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Attention-based Multiple Instance Learning (ABMIL) model with the \\(\\texttt{Sm}\\) operator. Proposed in Sm: enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, it aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) using an attention-based pooling mechanism that incorporates the \\(\\texttt{Sm}\\) operator,</p> \\[ \\mathbf{z}, \\mathbf{f} = \\operatorname{SmAttentionPool}(\\mathbf{X}), \\] <p>where \\(\\mathbf{f} \\in \\mathbb{R}^{N}\\) are the attention values. See SmAttentionPool for more details on the pooling operator The bag representation \\(\\mathbf{z}\\) is then fed into a classifier (one linear layer) to predict the bag label.</p>"},{"location":"api/models/sm_abmil/#torchmil.models.SmABMIL.__init__","title":"<code>__init__(in_shape, att_dim=128, att_act='tanh', sm_mode='approx', sm_alpha='trainable', sm_layers=0, sm_steps=10, sm_pre=False, sm_post=False, sm_spectral_norm=False, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>att_act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for attention. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>sm_mode</code>               (<code>str</code>, default:                   <code>'approx'</code> )           \u2013            <p>Mode for the Sm operator. Possible values: 'approx', 'exact'.</p> </li> <li> <code>sm_alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator. If 'trainable', alpha is trainable.</p> </li> <li> <code>sm_layers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of layers that use the Sm operator.</p> </li> <li> <code>sm_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps for the Sm operator.</p> </li> <li> <code>sm_pre</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator before the attention pooling.</p> </li> <li> <code>sm_post</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator after the attention pooling.</p> </li> <li> <code>sm_spectral_norm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply spectral normalization to all linear layers.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/sm_abmil/#torchmil.models.SmABMIL.forward","title":"<code>forward(X, adj, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/sm_abmil/#torchmil.models.SmABMIL.compute_loss","title":"<code>compute_loss(Y, X, adj, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/sm_abmil/#torchmil.models.SmABMIL.predict","title":"<code>predict(X, adj, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/sm_transformer_abmil/","title":"SmTransformerABMIL","text":""},{"location":"api/models/sm_transformer_abmil/#torchmil.models.SmTransformerABMIL","title":"<code>torchmil.models.SmTransformerABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Transformer Attention-based Multiple Instance Learning model with the \\(\\texttt{Sm}\\) operator. Proposed in Sm: enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, it transforms the instance features using a transformer encoder with the \\(\\texttt{Sm}\\) operator,</p> \\[ \\mathbf{X} = \\text{SmTransformerEncoder}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}. \\] <p>Subsequently, it aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) using an attention-based pooling mechanism that incorporates the \\(\\texttt{Sm}\\) operator,</p> \\[ \\mathbf{z}, \\mathbf{f} = \\operatorname{SmAttentionPool}(\\mathbf{X}). \\] <p>where \\(\\mathbf{f} \\in \\mathbb{R}^{N}\\) are the attention values. Finally, the bag representation \\(\\mathbf{z}\\) is then fed into a classifier (one linear layer) to predict the bag label.</p> <p>See SmAttentionPool for more details on the attention-based pooling, and SmTransformerEncoder for more details on the transformer encoder.</p>"},{"location":"api/models/sm_transformer_abmil/#torchmil.models.SmTransformerABMIL.__init__","title":"<code>__init__(in_shape, pool_att_dim=128, pool_act='tanh', pool_sm_mode='approx', pool_sm_alpha='trainable', pool_sm_layers=1, pool_sm_steps=10, pool_sm_pre=False, pool_sm_post=False, pool_sm_spectral_norm=False, feat_ext=torch.nn.Identity(), transf_att_dim=512, transf_n_layers=1, transf_n_heads=4, transf_use_mlp=True, transf_add_self=True, transf_dropout=0.0, transf_sm_alpha='trainable', transf_sm_mode='approx', transf_sm_steps=10, criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>pool_att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension for pooling.</p> </li> <li> <code>pool_act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for pooling. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>pool_sm_mode</code>               (<code>str</code>, default:                   <code>'approx'</code> )           \u2013            <p>Mode for the Sm operator in pooling. Possible values: 'approx', 'exact'.</p> </li> <li> <code>pool_sm_alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator in pooling. If 'trainable', alpha is trainable.</p> </li> <li> <code>pool_sm_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers that use the Sm operator in pooling.</p> </li> <li> <code>pool_sm_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps for the Sm operator in pooling.</p> </li> <li> <code>pool_sm_pre</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator before the attention pooling.</p> </li> <li> <code>pool_sm_post</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator after the attention pooling.</p> </li> <li> <code>pool_sm_spectral_norm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply spectral normalization to all linear layers in pooling.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>transf_att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension for transformer encoder.</p> </li> <li> <code>transf_n_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in transformer encoder.</p> </li> <li> <code>transf_n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in transformer encoder.</p> </li> <li> <code>transf_use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use MLP in transformer encoder.</p> </li> <li> <code>transf_add_self</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add input to output in transformer encoder.</p> </li> <li> <code>transf_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in transformer encoder.</p> </li> <li> <code>transf_sm_alpha</code>               (<code>float</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator in transformer encoder.</p> </li> <li> <code>transf_sm_mode</code>               (<code>str</code>, default:                   <code>'approx'</code> )           \u2013            <p>Mode for the Sm operator in transformer encoder.</p> </li> <li> <code>transf_sm_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps for the Sm operator in transformer encoder.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/sm_transformer_abmil/#torchmil.models.SmTransformerABMIL.forward","title":"<code>forward(X, adj, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/sm_transformer_abmil/#torchmil.models.SmTransformerABMIL.compute_loss","title":"<code>compute_loss(Y, X, adj, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/sm_transformer_abmil/#torchmil.models.SmTransformerABMIL.predict","title":"<code>predict(X, adj, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/transformer_abmil/","title":"TransformerABMIL","text":""},{"location":"api/models/transformer_abmil/#torchmil.models.TransformerABMIL","title":"<code>torchmil.models.TransformerABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Transformer Attention-based Multiple Instance Learning model.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\). Then, it transforms the instance features using a transformer encoder,</p> \\[ \\mathbf{X} = \\text{TransformerEncoder}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}, \\] <p>and finally it aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) using the attention-based pooling,</p> \\[ \\mathbf{z}, \\mathbf{f} = \\operatorname{AttentionPool}(\\mathbf{X}). \\] <p>where \\(\\mathbf{f} = \\operatorname{MLP}(\\mathbf{X}) \\in \\mathbb{R}^{N}\\) are the attention values. The bag representation \\(\\mathbf{z}\\) is then fed into a classifier (one linear layer) to predict the bag label.</p> <p>See AttentionPool for more details on the attention-based pooling, and TransformerEncoder for more details on the transformer encoder.</p>"},{"location":"api/models/transformer_abmil/#torchmil.models.TransformerABMIL.__init__","title":"<code>__init__(in_shape, pool_att_dim=128, pool_act='tanh', pool_gated=False, feat_ext=torch.nn.Identity(), transf_att_dim=512, transf_n_layers=1, transf_n_heads=8, transf_use_mlp=True, transf_add_self=True, transf_dropout=0.0, criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>pool_att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension for pooling.</p> </li> <li> <code>pool_act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for pooling. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>pool_gated</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use gated attention in the attention pooling.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>transf_att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension for transformer encoder.</p> </li> <li> <code>transf_n_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in transformer encoder.</p> </li> <li> <code>transf_n_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads in transformer encoder.</p> </li> <li> <code>transf_use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use MLP in transformer encoder.</p> </li> <li> <code>transf_add_self</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add input to output in transformer encoder.</p> </li> <li> <code>transf_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in transformer encoder.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/transformer_abmil/#torchmil.models.TransformerABMIL.forward","title":"<code>forward(X, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_logits_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/transformer_abmil/#torchmil.models.TransformerABMIL.compute_loss","title":"<code>compute_loss(Y, X, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/transformer_abmil/#torchmil.models.TransformerABMIL.predict","title":"<code>predict(X, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/transformer_prob_smooth_abmil/","title":"TransformerProbSmoothABMIL","text":""},{"location":"api/models/transformer_prob_smooth_abmil/#torchmil.models.TransformerProbSmoothABMIL","title":"<code>torchmil.models.TransformerProbSmoothABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Transformer Attention-based Multiple Instance Learning model, with probabilistic attention-based pooling. Proposed in Probabilistic Smooth Attention for Deep Multiple Instance Learning in Medical Imaging.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\). Then, it transforms the instance features using a transformer encoder,</p> \\[ \\mathbf{X} = \\text{TransformerEncoder}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}. \\] <p>Subsequently, it aggregates the instance features into a bag representation using a probabilistic attention-based pooling mechanism, as detailed in ProbSmoothAttentionPool.</p> <p>Specifically, it computes a mean vector \\(\\mathbf{\\mu}_{\\mathbf{f}} \\in \\mathbb{R}^N\\) and a variance vector \\(\\mathbf{\\sigma}_{\\mathbf{f}^2} \\in \\mathbb{R}^N\\) that define the attention distribution \\(q(\\mathbf{f} \\mid \\mathbf{X}) = \\mathcal{N}\\left(\\mathbf{f} \\mid \\mathbf{\\mu}_{\\mathbf{f}}, \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2) \\right)\\),</p> \\[ \\mathbf{\\mu}_{\\mathbf{f}}, \\mathbf{\\sigma}_{\\mathbf{f}} = \\operatorname{ProbSmoothAttentionPool}(\\mathbf{X}). \\] <p>If <code>covar_mode='zero'</code>, the variance vector \\(\\mathbf{\\sigma}_{\\mathbf{f}}^2\\) is set to zero, resulting in a deterministic attention distribution.</p> <p>Then, \\(m\\) attention vectors \\(\\widehat{\\mathbf{F}} = \\left[ \\widehat{\\mathbf{f}}^{(1)}, \\ldots, \\widehat{\\mathbf{f}}^{(m)} \\right]^\\top \\in \\mathbb{R}^{m \\times N}\\) are sampled from the attention distribution. The bag representation \\(\\widehat{\\mathbf{z}} \\in \\mathbb{R}^{m \\times D}\\) is then computed as:</p> \\[ \\widehat{\\mathbf{z}} = \\operatorname{Softmax}(\\widehat{\\mathbf{F}}) \\mathbf{X}. \\] <p>The bag representation \\(\\widehat{\\mathbf{z}}\\) is fed into a classifier, implemented as a linear layer, to produce bag label predictions \\(Y_{\\text{pred}} \\in \\mathbb{R}^{m}\\).</p> <p>Notably, the attention distribution naturally induces a distribution over the bag label predictions. This model thus generates multiple predictions for each bag, corresponding to different samples from this distribution.</p> <p>Regularization. The probabilistic pooling mechanism introduces a regularization term to the loss function that encourages smoothness in the attention values. Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), the regularization term corresponds to</p> \\[     \\ell_{\\text{KL}}(\\mathbf{X}, \\mathbf{A}) =         \\begin{cases}             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} \\quad &amp; \\text{if } \\texttt{covar_mode='zero'}, \\\\             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} + \\operatorname{Tr}(\\mathbf{L} \\mathbf{\\Sigma}_{\\mathbf{f}}) - \\frac{1}{2}\\log \\det( \\mathbf{\\Sigma}_{\\mathbf{f}} ) + \\operatorname{const} \\quad &amp; \\text{if } \\texttt{covar_mode='diag'}, \\\\         \\end{cases} \\] <p>where \\(\\operatorname{const}\\) is a constant term that does not depend on the parameters, \\(\\mathbf{\\Sigma}_{\\mathbf{f}} = \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2)\\), \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) is the graph Laplacian matrix, and \\(\\mathbf{D}\\) is the degree matrix of \\(\\mathbf{A}\\). This term is then averaged for all bags in the batch and added to the loss function.</p>"},{"location":"api/models/transformer_prob_smooth_abmil/#torchmil.models.TransformerProbSmoothABMIL.__init__","title":"<code>__init__(in_shape=None, pool_att_dim=128, covar_mode='diag', n_samples_train=1000, n_samples_test=5000, feat_ext=torch.nn.Identity(), transf_att_dim=512, transf_n_layers=1, transf_n_heads=8, transf_use_mlp=True, transf_add_self=True, transf_dropout=0.0, criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>pool_att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>covar_mode</code>               (<code>str</code>, default:                   <code>'diag'</code> )           \u2013            <p>Covariance mode for the Gaussian prior. Possible values: 'diag', 'full'.</p> </li> <li> <code>n_samples_train</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Number of samples for training.</p> </li> <li> <code>n_samples_test</code>               (<code>int</code>, default:                   <code>5000</code> )           \u2013            <p>Number of samples for testing.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>transf_att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension for transformer encoder.</p> </li> <li> <code>transf_n_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in transformer encoder.</p> </li> <li> <code>transf_n_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads in transformer encoder.</p> </li> <li> <code>transf_use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use MLP in transformer encoder.</p> </li> <li> <code>transf_add_self</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add input to output in transformer encoder.</p> </li> <li> <code>transf_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in transformer encoder.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/transformer_prob_smooth_abmil/#torchmil.models.TransformerProbSmoothABMIL.forward","title":"<code>forward(X, adj, mask=None, return_att=False, return_samples=False, return_kl_div=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>. Only required when <code>return_kl_div=True</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_samples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and <code>return_att=True</code>, the attention values returned are samples from the attention distribution.</p> </li> <li> <code>return_kl_div</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the KL divergence between the attention distribution and the prior distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size, n_samples)</code> if <code>return_samples=True</code>, else <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape <code>(batch_size, bag_size, n_samples)</code> if <code>return_samples=True</code>, else <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>kl_div</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_kl_div=True</code>. KL divergence between the attention distribution and the prior distribution of shape <code>()</code>.</p> </li> </ul>"},{"location":"api/models/transformer_prob_smooth_abmil/#torchmil.models.TransformerProbSmoothABMIL.compute_loss","title":"<code>compute_loss(Y, X, adj, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value and the KL divergence between the attention distribution and the prior distribution.</p> </li> </ul>"},{"location":"api/models/transformer_prob_smooth_abmil/#torchmil.models.TransformerProbSmoothABMIL.predict","title":"<code>predict(X, adj, mask=None, return_inst_pred=True, return_samples=False)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, returns the attention values as instance labels predictions, in addition to bag label predictions.</p> </li> <li> <code>return_samples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and <code>return_inst_pred=True</code>, the instance label predictions returned are samples from the instance label distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_inst_pred=True</code>. Attention values (before normalization) of shape <code>(batch_size, bag_size)</code> if <code>return_samples=False</code>, else <code>(batch_size, bag_size, n_samples)</code>.</p> </li> </ul>"},{"location":"api/models/transmil/","title":"TransMIL","text":""},{"location":"api/models/transmil/#torchmil.models.TransMIL","title":"<code>torchmil.models.TransMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Method proposed in the paper TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, following Algorithm 2 in the paper, it performs sequence squaring, adds a class token, and applies the novel TPT module. This module consists of two Nystr\u00f6mformer layers and the novel PPEG (Pyramid Positional Encoding Generator) layer.</p> <p>Finally, a linear classifier is used to predict the bag label from the class token.</p>"},{"location":"api/models/transmil/#torchmil.models.TransMIL.__init__","title":"<code>__init__(in_shape, att_dim=512, n_layers=2, n_heads=4, n_landmarks=None, pinv_iterations=6, dropout=0.0, use_mlp=False, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Embedding dimension. Should be divisible by <code>n_heads</code>.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Number of Nystr\u00f6mformer layers.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the Nystr\u00f6mformer layer.</p> </li> <li> <code>n_landmarks</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of landmarks in the Nystr\u00f6mformer layer.</p> </li> <li> <code>pinv_iterations</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of iterations for the pseudo-inverse in the Nystr\u00f6mformer layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in the Nystr\u00f6mformer layer.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a MLP after the Nystr\u00f6mformer layer.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor. By default, the identity function (no feature extraction).</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/transmil/#torchmil.models.TransMIL.forward","title":"<code>forward(X, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the attention values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/transmil/#torchmil.models.TransMIL.compute_loss","title":"<code>compute_loss(Y, X)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/transmil/#torchmil.models.TransMIL.predict","title":"<code>predict(X, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/nn/","title":"torchmil.nn","text":"<p>torchmil.nn is a collection of PyTorch modules frequently used in Multiple Instance Learning (MIL) models. These modules are designed to be flexible and easy to use, allowing you to build custom MIL models with ease.</p>"},{"location":"api/nn/#available-modules","title":"Available modules","text":"<ul> <li>Attention<ul> <li>Attention Pool</li> <li>Sm Attention Pool</li> <li>Probabilistic Smooth Attention Pool</li> <li>Multihead Self-Attention</li> <li>Multihead Self-Attention with Relative Positional Encoding (iRPE)</li> <li>Nystrom Attention</li> <li>Multihead Cross-Attention</li> </ul> </li> <li>Graph Neural Networks (GNNs)<ul> <li>Deep Graph Convolutional Network (DeepGCN) layer</li> <li>Graph Convolutional Network (GCN) convolution</li> <li>Dense MinCut pooling</li> </ul> </li> <li>Transformers<ul> <li>Transformer base class</li> <li>Conventional Transformer</li> <li>Sm Transformer</li> <li>Nystrom Transformer</li> <li>Transformer with image Relative Positional Encoding (iRPE)</li> <li>Tokens-2-Token</li> </ul> </li> <li>Sm operator</li> <li>Max Pool</li> <li>Mean Pool</li> </ul>"},{"location":"api/nn/max_pool/","title":"Max Pool","text":""},{"location":"api/nn/max_pool/#torchmil.nn.MaxPool","title":"<code>torchmil.nn.MaxPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Max pooling aggregation.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this model aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) as,</p> \\[ \\left[ \\mathbf{z} \\right]_d = \\max \\left\\{ \\left[ \\mathbf{x}_n \\right]_{d} \\ \\colon n \\in \\left\\{ 1, \\ldots, N \\right\\}  \\right\\}, \\] <p>where \\(\\left[ \\mathbf{a} \\right]_i\\) denotes the \\(i\\)-th element of the vector \\(\\mathbf{a}\\).</p>"},{"location":"api/nn/max_pool/#torchmil.nn.MaxPool.__init__","title":"<code>__init__()</code>","text":""},{"location":"api/nn/max_pool/#torchmil.nn.MaxPool.forward","title":"<code>forward(X, mask=None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/mean_pool/","title":"Mean Pool","text":""},{"location":"api/nn/mean_pool/#torchmil.nn.MeanPool","title":"<code>torchmil.nn.MeanPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Mean pooling aggregation.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this model aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) as,</p> \\[     \\mathbf{z} = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_n. \\]"},{"location":"api/nn/mean_pool/#torchmil.nn.MeanPool.__init__","title":"<code>__init__()</code>","text":""},{"location":"api/nn/mean_pool/#torchmil.nn.MeanPool.forward","title":"<code>forward(X, mask=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:     z: Output tensor of shape <code>(batch_size, in_dim)</code>.</p>"},{"location":"api/nn/sm/","title":"Sm operator","text":""},{"location":"api/nn/sm/#torchmil.nn.Sm","title":"<code>torchmil.nn.Sm</code>","text":"<p>               Bases: <code>Module</code></p> <p>The \\(\\texttt{Sm}\\) operator, proposed in the paper \\(\\texttt{Sm}\\): enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input graph with node features \\(\\mathbf{U} \\in \\mathbb{R}^{N \\times D}\\) and adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), in the exact mode the \\(\\texttt{Sm}\\) operator is defined as:</p> \\[\\begin{align}     \\texttt{Sm}(\\mathbf{U}) = ( \\mathbf{I} + \\gamma \\mathbf{L} )^{-1} \\mathbf{U}, \\end{align}\\] <p>where \\(\\gamma \\in (0, \\infty)\\) is a hyperparameter, \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) is the graph Laplacian, and \\(\\mathbf{D}\\) is the degree matrix. If <code>mode='approx'</code>, the \\(\\texttt{Sm}\\) operator is approximated as \\(\\texttt{Sm}(\\mathbf{U}) = G(T)\\), where</p> \\[\\begin{align}     G(0) = \\mathbf{U}, \\quad G(t) = \\alpha ( \\mathbf{I} - \\mathbf{L} ) G(t-1) + (1-\\alpha) \\mathbf{U}, \\end{align}\\] <p>for \\(t \\in \\{1, \\ldots, T\\}\\), and \\(\\alpha \\in (0, 1)\\) is a hyperparameter.</p>"},{"location":"api/nn/sm/#torchmil.nn.Sm.__init__","title":"<code>__init__(alpha='trainable', num_steps=10, mode='approx')</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator. If 'trainable', alpha is a trainable parameter.</p> </li> <li> <code>num_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps to approximate the exact Sm operator.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'approx'</code> )           \u2013            <p>Mode of the Sm operator. Possible values: 'approx', 'exact'.</p> </li> </ul>"},{"location":"api/nn/sm/#torchmil.nn.Sm.forward","title":"<code>forward(f, adj_mat)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj_mat</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix tensor of shape <code>(batch_size, bag_size, bag_size)</code>. Sparse tensor is supported.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>g</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul>"},{"location":"api/nn/sm/#torchmil.nn.ApproxSm","title":"<code>torchmil.nn.ApproxSm</code>","text":"<p>               Bases: <code>Module</code></p> <p>\\(\\texttt{Sm}\\) operator in the approximate mode, proposed in the paper \\(\\texttt{Sm}\\): enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input graph with node features \\(\\mathbf{U} \\in \\mathbb{R}^{N \\times D}\\) and adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), it computes \\(\\texttt{Sm}(\\mathbf{U}) = G(T)\\), where</p> \\[\\begin{align}     G(0) = \\mathbf{U}, \\quad G(t) = \\alpha ( \\mathbf{I} - \\mathbf{L} ) G(t-1) + (1-\\alpha) \\mathbf{U}, \\end{align}\\] <p>for \\(t \\in \\{1, \\ldots, T\\}\\), and \\(\\alpha \\in (0, 1)\\) is a hyperparameter.</p>"},{"location":"api/nn/sm/#torchmil.nn.ApproxSm.__init__","title":"<code>__init__(alpha='trainable', num_steps=10)</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator. If 'trainable', alpha is a trainable parameter.</p> </li> <li> <code>num_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps to approximate the exact Sm operator.</p> </li> </ul>"},{"location":"api/nn/sm/#torchmil.nn.ApproxSm.forward","title":"<code>forward(f, adj_mat)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj_mat</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix tensor of shape <code>(batch_size, bag_size, bag_size)</code>. Sparse tensor is supported.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>g</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul>"},{"location":"api/nn/sm/#torchmil.nn.ExactSm","title":"<code>torchmil.nn.ExactSm</code>","text":"<p>               Bases: <code>Module</code></p> <p>\\(\\texttt{Sm}\\) operator in the exact mode, proposed in the paper \\(\\texttt{Sm}\\): enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input graph with node features \\(\\mathbf{U} \\in \\mathbb{R}^{N \\times D}\\) and adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), it computes</p> \\[\\begin{align}     \\texttt{Sm}(\\mathbf{U}) = ( \\mathbf{I} + \\gamma \\mathbf{L} )^{-1} \\mathbf{U}, \\end{align}\\] <p>where \\(\\gamma \\in (0, \\infty)\\) is a hyperparameter, \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) is the graph Laplacian, and \\(\\mathbf{D}\\) is the degree matrix.</p>"},{"location":"api/nn/sm/#torchmil.nn.ExactSm.__init__","title":"<code>__init__(alpha='trainable')</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator. If 'trainable', alpha is a trainable parameter.</p> </li> </ul>"},{"location":"api/nn/sm/#torchmil.nn.ExactSm.forward","title":"<code>forward(f, adj_mat)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj_mat</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix tensor of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>g</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul>"},{"location":"api/nn/attention/","title":"torchmil.nn.attention","text":"<ul> <li>Attention Pool</li> <li>Sm Attention Pool</li> <li>Probabilistic Smooth Attention Pool</li> <li>Multihead Self-Attention</li> <li>Multihead Self-Attention with Relative Positional Encoding (iRPE)</li> <li>Nystrom Attention</li> <li>Multihead Cross-Attention</li> </ul>"},{"location":"api/nn/attention/attention_pool/","title":"Attention Pool","text":""},{"location":"api/nn/attention/attention_pool/#torchmil.nn.attention.AttentionPool","title":"<code>torchmil.nn.attention.AttentionPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Attention-based pooling, as proposed in the paper Attention-based Multiple Instance Learning.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times \\texttt{in_dim}}\\), this model aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{\\texttt{in_dim}}\\) as,</p> \\[ \\mathbf{z} = \\mathbf{X}^\\top \\operatorname{Softmax}(\\mathbf{f}) = \\sum_{n=1}^N s_n \\mathbf{x}_n, \\] <p>where \\(\\mathbf{f} = \\operatorname{MLP}(\\mathbf{X}) \\in \\mathbb{R}^{N}\\) are the attention values and \\(s_n\\) is the normalized attention score for the \\(n\\)-th instance.</p> <p>To compute the attention values, the \\(\\operatorname{MLP}\\) is defined as</p> \\[\\begin{equation} \\operatorname{MLP}(\\mathbf{X}) = \\begin{cases} \\operatorname{act}(\\mathbf{X}\\mathbf{W}_1)\\mathbf{w}, &amp; \\text{if }\\texttt{gated=False}, \\\\ \\left(\\operatorname{act}(\\mathbf{X}\\mathbf{W}_1)\\odot\\operatorname{sigm}(\\mathbf{X}\\mathbf{W}_2)\\right)\\mathbf{w}, &amp; \\text{if }\\texttt{gated=True}, \\end{cases} \\end{equation}\\] <p>where \\(\\mathbf{W}_1 \\in \\mathbb{R}^{\\texttt{in_dim} \\times \\texttt{att_dim}}\\), \\(\\mathbf{W}_2 \\in \\mathbb{R}^{\\texttt{in_dim} \\times \\texttt{att_dim}}\\), \\(\\mathbf{w} \\in \\mathbb{R}^{\\texttt{att_dim}}\\), \\(\\operatorname{act} \\ \\colon \\mathbb{R} \\to \\mathbb{R}\\) is the activation function, \\(\\operatorname{sigm} \\ \\colon \\mathbb{R} \\to \\left] 0, 1 \\right[\\) is the sigmoid function, and \\(\\odot\\) denotes element-wise multiplication.</p>"},{"location":"api/nn/attention/attention_pool/#torchmil.nn.attention.AttentionPool.__init__","title":"<code>__init__(in_dim=None, att_dim=128, act='tanh', gated=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Input dimension. If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for attention. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>gated</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use gated attention.</p> </li> </ul>"},{"location":"api/nn/attention/attention_pool/#torchmil.nn.attention.AttentionPool.forward","title":"<code>forward(X, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>z</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>Tensor</code> )          \u2013            <p>Bag representation of shape <code>(batch_size, in_dim)</code>.</p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/nn/attention/irpe_multihead_self_attention/","title":"iRPE Multihead Self-Attention","text":""},{"location":"api/nn/attention/irpe_multihead_self_attention/#torchmil.nn.attention.iRPEMultiheadSelfAttention","title":"<code>torchmil.nn.attention.iRPEMultiheadSelfAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multihead Self-Attention with image Relative Position Encoding (iRPE), as described in Rethinking and Improving Relative Position Encoding for Vision Transformer.</p> <p>The iRPE implementation is based on the official codebase.</p>"},{"location":"api/nn/attention/irpe_multihead_self_attention/#torchmil.nn.attention.iRPEMultiheadSelfAttention.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, dropout=0.0, learn_weights=True, rpe_ratio=1.9, rpe_method='product', rpe_mode='contextual', rpe_shared_head=True, rpe_skip=1, rpe_on='k')</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension. Must be divisible by <code>n_heads</code>.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, <code>out_dim</code> = <code>in_dim</code>.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>learn_weights</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, learn the weights for query, key, and value. If False, q, k, and v are the same as the input, and therefore <code>in_dim</code> must be divisible by <code>n_heads</code>.</p> </li> <li> <code>rpe_ratio</code>               (<code>float</code>, default:                   <code>1.9</code> )           \u2013            <p>Relative position encoding ratio.</p> </li> <li> <code>rpe_method</code>               (<code>str</code>, default:                   <code>'product'</code> )           \u2013            <p>Relative position encoding method. Possible values: ['euc', 'quant', 'cross', 'product']</p> </li> <li> <code>rpe_mode</code>               (<code>str</code>, default:                   <code>'contextual'</code> )           \u2013            <p>Relative position encoding mode. Possible values: [None, 'bias', 'contextual']</p> </li> <li> <code>rpe_shared_head</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to share weights across heads.</p> </li> <li> <code>rpe_skip</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Relative position encoding skip. Possible values: [0, 1]. </p> </li> <li> <code>rpe_on</code>               (<code>str</code>, default:                   <code>'k'</code> )           \u2013            <p>Where to apply relative positional encoding. Possible values: ['q', 'k', 'v', 'qk', 'kv', 'qkv'].</p> </li> </ul> <p>Note. When 'v' is in <code>rpe_on</code>, <code>rpe_mode</code> must be 'contextual'.</p>"},{"location":"api/nn/attention/irpe_multihead_self_attention/#torchmil.nn.attention.iRPEMultiheadSelfAttention.forward","title":"<code>forward(x, mask=None, return_att=False, height=None, width=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, seq_len)</code>.</p> </li> <li> <code>height</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Height of the input sequence. If None, <code>height = floor(sqrt(seq_len))</code>.</p> </li> <li> <code>width</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Width of the input sequence. If None, <code>width = floor(sqrt(seq_len))</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, seq_len, att_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/attention/multihead_cross_attention/","title":"Multihead Cross-Attention","text":""},{"location":"api/nn/attention/multihead_cross_attention/#torchmil.nn.attention.MultiheadCrossAttention","title":"<code>torchmil.nn.attention.MultiheadCrossAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>The Multihead Cross Attention module, as described in Attention is All You Need.</p> <p>Given input bags \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times \\texttt{in_dim}}\\), and \\(\\mathbf{Y} = \\left[ \\mathbf{y}_1, \\ldots, \\mathbf{y}_M \\right]^\\top \\in \\mathbb{R}^{M \\times \\texttt{in_dim}}\\), this module computes:</p> \\[\\begin{gather*} \\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{Y}\\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{Y}\\mathbf{W}_V,\\\\ \\mathbf{Z} = \\operatorname{Softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d}} \\right) \\mathbf{V}, \\end{gather*}\\] <p>where \\(d = \\texttt{att_dim}\\) and \\(\\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V \\in \\mathbb{R}^{\\texttt{in_dim} \\times \\texttt{att_dim}}\\) are learnable weight matrices.</p> <p>If \\(\\texttt{out_dim} \\neq \\texttt{att_dim}\\), \\(\\mathbf{Y}\\) is passed through a linear layer with output dimension \\(\\texttt{out_dim}\\).</p>"},{"location":"api/nn/attention/multihead_cross_attention/#torchmil.nn.attention.MultiheadCrossAttention.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, dropout=0.0, learn_weights=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, <code>out_dim</code> = <code>in_dim</code>.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension, must be divisible by <code>n_heads</code>.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>learn_weights</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, learn the weights for query, key, and value. If False, q, k, and v are the same as the input, and therefore <code>in_dim</code> must be divisible by <code>n_heads</code>.</p> </li> </ul>"},{"location":"api/nn/attention/multihead_cross_attention/#torchmil.nn.attention.MultiheadCrossAttention.forward","title":"<code>forward(x, y, mask=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len_x, in_dim)</code>.</p> </li> <li> <code>y</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len_y, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, seq_len_x)</code>.</p> </li> </ul> <p>Returns:     y: Output tensor of shape <code>(batch_size, seq_len_x, att_dim)</code>.</p>"},{"location":"api/nn/attention/multihead_self_attention/","title":"Multihead Self-Attention","text":""},{"location":"api/nn/attention/multihead_self_attention/#torchmil.nn.attention.MultiheadSelfAttention","title":"<code>torchmil.nn.attention.MultiheadSelfAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>The Multihead Self Attention module, as described in Attention is All You Need.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times \\texttt{in_dim}}\\), this module computes:</p> \\[\\begin{gather*} \\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{X}\\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{X}\\mathbf{W}_V,\\\\ \\mathbf{Y} = \\operatorname{Softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d}} \\right) \\mathbf{V}, \\end{gather*}\\] <p>where \\(d = \\texttt{att_dim}\\) and \\(\\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V \\in \\mathbb{R}^{\\texttt{in_dim} \\times \\texttt{att_dim}}\\) are learnable weight matrices.</p> <p>If \\(\\texttt{out_dim} \\neq \\texttt{att_dim}\\), \\(\\mathbf{Y}\\) is passed through a linear layer with output dimension \\(\\texttt{out_dim}\\).</p>"},{"location":"api/nn/attention/multihead_self_attention/#torchmil.nn.attention.MultiheadSelfAttention.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, dropout=0.0, learn_weights=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension, must be divisible by <code>n_heads</code>.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, <code>out_dim</code> = <code>in_dim</code>.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>learn_weights</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, learn the weights for query, key, and value. If False, q, k, and v are the same as the input, and therefore <code>in_dim</code> must be divisible by <code>n_heads</code>.</p> </li> </ul>"},{"location":"api/nn/attention/multihead_self_attention/#torchmil.nn.attention.MultiheadSelfAttention.forward","title":"<code>forward(x, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, seq_len)</code>.</p> </li> </ul> <p>Returns:     y: Output tensor of shape <code>(batch_size, seq_len, att_dim)</code>.     att: Only returned when <code>return_att=True</code>. Attention weights of shape <code>(batch_size, n_heads, seq_len, seq_len)</code>.</p>"},{"location":"api/nn/attention/nystrom_attention/","title":"Nystr\u00f6m Attention","text":""},{"location":"api/nn/attention/nystrom_attention/#torchmil.nn.attention.NystromAttention","title":"<code>torchmil.nn.attention.NystromAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Nystrom attention, as described in the paper Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention.</p> <p>Implementation based on the official code.</p>"},{"location":"api/nn/attention/nystrom_attention/#torchmil.nn.attention.NystromAttention.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, learn_weights=True, n_landmarks=256, pinv_iterations=6)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension. Must be divisible by <code>n_heads</code>.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>learn_weights</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, learn the weights for query, key, and value. If False, q, k, and v are the same as the input, and therefore <code>in_dim</code> must be divisible by <code>n_heads</code>.</p> </li> <li> <code>n_landmarks</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Number of landmarks.</p> </li> <li> <code>pinv_iterations</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of iterations for Moore-Penrose pseudo-inverse.</p> </li> </ul>"},{"location":"api/nn/attention/nystrom_attention/#torchmil.nn.attention.NystromAttention.forward","title":"<code>forward(x, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, seq_len)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return attention weights.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, seq_len, att_dim)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention weights of shape <code>(batch_size, n_heads, seq_len, seq_len)</code>.</p> </li> </ul>"},{"location":"api/nn/attention/prob_smooth_attention_pool/","title":"Prob Smooth Attention Pool","text":""},{"location":"api/nn/attention/prob_smooth_attention_pool/#torchmil.nn.attention.ProbSmoothAttentionPool","title":"<code>torchmil.nn.attention.ProbSmoothAttentionPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Probabilistic Smooth Attention Pooling, proposed in in Probabilistic Smooth Attention for Deep Multiple Instance Learning in Medical Imaging and Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times \\texttt{in_dim}}\\), this model computes an attention distribution \\(q(\\mathbf{f} \\mid \\mathbf{X}) = \\mathcal{N}\\left(\\mathbf{f} \\mid \\mathbf{\\mu}_{\\mathbf{f}}, \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2) \\right)\\), where:</p> \\[\\begin{gather}     \\mathbf{H} = \\operatorname{MLP}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times 2\\texttt{att_dim}}, \\\\     \\mathbf{\\mu}_{\\mathbf{f}} = \\mathbf{H}\\mathbf{w}_{\\mu} \\in \\mathbb{R}^{N}, \\\\     \\log \\mathbf{\\sigma}_{\\mathbf{f}}^2 = \\mathbf{H}\\mathbf{w}_{\\sigma} \\in \\mathbb{R}^{N}, \\end{gather}\\] <p>where \\(\\operatorname{MLP}\\) is a multi-layer perceptron, and \\(\\mathbf{w}_{\\mu},\\mathbf{w}_{\\sigma} \\in \\mathbb{R}^{2\\texttt{att_dim} \\times 1}\\). If <code>covar_mode='zero'</code>, the variance vector \\(\\mathbf{\\sigma}_{\\mathbf{f}}^2\\) is set to zero, resulting in a deterministic attention distribution.</p> <p>Then, \\(M\\) samples from the attention distribution are drawn as \\(\\widehat{\\mathbf{f}}^{(m)} \\sim q(\\mathbf{f} \\mid \\mathbf{X})\\). With these samples, the bag representation is computed as:</p> \\[ \\widehat{\\mathbf{z}} = \\operatorname{Softmax}(\\widehat{\\mathbf{F}}) \\mathbf{X} \\in \\mathbb{R}^{\\texttt{in_dim} \\times M}, \\] <p>where \\(\\widehat{\\mathbf{F}} = \\left[ \\widehat{\\mathbf{f}}^{(1)}, \\ldots, \\widehat{\\mathbf{f}}^{(M)} \\right]^\\top \\in \\mathbb{R}^{N \\times M}\\).</p> <p>Kullback-Leibler Divergence. Given a bag with adjancency matrix \\(\\mathbf{A}\\), the KL divergence between the attention distribution and the prior distribution is computed as:</p> \\[     \\ell_{\\text{KL}} =         \\begin{cases}             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} \\quad &amp; \\text{if } \\texttt{covar_mode='zero'}, \\\\             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} + \\operatorname{Tr}(\\mathbf{L} \\mathbf{\\Sigma}_{\\mathbf{f}}) - \\frac{1}{2}\\log \\det( \\mathbf{\\Sigma}_{\\mathbf{f}} ) + \\operatorname{const} \\quad &amp; \\text{if } \\texttt{covar_mode='diag'}, \\\\         \\end{cases} \\] <p>where \\(\\operatorname{const}\\) is a constant term that does not depend on the parameters, \\(\\mathbf{\\Sigma}_{\\mathbf{f}} = \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2)\\), \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) is the graph Laplacian matrix, and \\(\\mathbf{D}\\) is the degree matrix of \\(\\mathbf{A}\\).</p>"},{"location":"api/nn/attention/prob_smooth_attention_pool/#torchmil.nn.attention.ProbSmoothAttentionPool.__init__","title":"<code>__init__(in_dim=None, att_dim=128, covar_mode='diag', n_samples_train=1000, n_samples_test=5000)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Input dimension. If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>covar_mode</code>               (<code>str</code>, default:                   <code>'diag'</code> )           \u2013            <p>Covariance mode. Must be 'diag' or 'zero'.</p> </li> <li> <code>n_samples_train</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Number of samples during training.</p> </li> <li> <code>n_samples_test</code>               (<code>int</code>, default:                   <code>5000</code> )           \u2013            <p>Number of samples during testing.</p> </li> </ul>"},{"location":"api/nn/attention/prob_smooth_attention_pool/#torchmil.nn.attention.ProbSmoothAttentionPool.forward","title":"<code>forward(X, adj=None, mask=None, return_att_samples=False, return_att_dist=False, return_kl_div=False, n_samples=None)</code>","text":"<p>In the following, if <code>covar_mode='zero'</code> then <code>n_samples</code> is automatically set to 1 and <code>diag_Sigma_f</code> is set to None.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>. Only required when <code>return_kl_div=True</code>.</p> </li> <li> <code>return_att</code>           \u2013            <p>If True, returns a sample from the attention distribution <code>f</code> in addition to <code>z</code>.</p> </li> <li> <code>return_attdist</code>           \u2013            <p>If True, returns the attention distribution (<code>mu_f</code>, <code>diag_Sigma_f</code>) in addition to <code>z</code>.</p> </li> <li> <code>return_kl_div</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the KL divergence between the attention distribution and the prior distribution.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of samples to draw. If not provided, it will use <code>n_samples_train</code> during training and <code>n_samples_test</code> during testing.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>Tensor</code> )          \u2013            <p>Bag representation of shape <code>(batch_size, dim, n_samples)</code>.</p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>Sample from the attention distribution of shape <code>(batch_size, bag_size, n_samples)</code>. Only returned when <code>return_att_samples=True</code>.</p> </li> <li> <code>mu_f</code> (              <code>Tensor</code> )          \u2013            <p>Mean of the attention distribution of shape <code>(batch_size, bag_size, 1)</code>. Only returned when <code>return_att_dist=True</code>.</p> </li> <li> <code>diag_Sigma_f</code> (              <code>Tensor</code> )          \u2013            <p>Covariance of the attention distribution of shape <code>(batch_size, bag_size, 1)</code>. Only returned when <code>return_att_dist=True</code>.</p> </li> <li> <code>kl_div</code> (              <code>Tensor</code> )          \u2013            <p>KL divergence between the attention distribution and the prior distribution, of shape <code>()</code>. Only returned when <code>return_kl_div=True</code>.</p> </li> </ul>"},{"location":"api/nn/attention/sm_attention_pool/","title":"Sm Attention Pool","text":""},{"location":"api/nn/attention/sm_attention_pool/#torchmil.nn.attention.SmAttentionPool","title":"<code>torchmil.nn.attention.SmAttentionPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Attention-based pooling with the Sm operator, as proposed in Sm: enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times \\texttt{in_dim}}\\), this model aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{\\texttt{in_dim}}\\) as,</p> \\[\\begin{gather}     \\mathbf{f} = \\operatorname{SmMLP}(\\mathbf{X}) \\in \\mathbb{R}^{N}, \\\\     \\mathbf{z} = \\mathbf{X}^\\top \\operatorname{Softmax}(\\mathbf{f}) = \\sum_{n=1}^N s_n \\mathbf{x}_n, \\end{gather}\\] <p>where \\(s_n\\) is the normalized attention score for the \\(n\\)-th instance.</p> <p>To compute the attention values, \\(\\operatorname{SmMLP}\\) is defined as \\(\\operatorname{SmMLP}(\\mathbf{X}) = \\mathbf{Y}^L\\) where</p> \\[\\begin{gather}     \\mathbf{Y}^0 = \\mathbf{X}\\mathbf{W^0}, \\\\     \\mathbf{Y}^l = \\operatorname{act}( \\texttt{Sm}(\\mathbf{Y}^{l-1}\\mathbf{W}^l)), \\quad \\text{for } l = 1, \\ldots, L-1, \\\\     \\mathbf{Y}^L = \\mathbf{Y}^{L-1}\\mathbf{w}, \\end{gather}\\] <p>where \\(\\mathbf{W^0} \\in \\mathbb{R}^{\\texttt{in_dim} \\times \\texttt{att_dim}}\\), \\(\\mathbf{W}^l \\in \\mathbb{R}^{\\texttt{att_dim} \\times \\texttt{att_dim}}\\), \\(\\mathbf{w} \\in \\mathbb{R}^{\\texttt{att_dim} \\times 1}\\), \\(\\operatorname{act} \\ \\colon \\mathbb{R} \\to \\mathbb{R}\\) is the activation function, and \\(\\texttt{Sm}\\) is the Sm operator, see Sm for more details.</p> <p>Note: If <code>sm_pre=True</code>, the Sm operator is applied before \\(\\operatorname{SmMLP}\\). If <code>sm_post=True</code>, the Sm operator is applied after \\(\\operatorname{SmMLP}\\).</p>"},{"location":"api/nn/attention/sm_attention_pool/#torchmil.nn.attention.SmAttentionPool.__init__","title":"<code>__init__(in_dim, att_dim=128, act='gelu', sm_mode='approx', sm_alpha='trainable', sm_layers=1, sm_steps=10, sm_pre=False, sm_post=False, sm_spectral_norm=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>act</code>               (<code>str</code>, default:                   <code>'gelu'</code> )           \u2013            <p>Activation function for attention. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>sm_mode</code>               (<code>str</code>, default:                   <code>'approx'</code> )           \u2013            <p>Mode for the Sm operator. Possible values: 'approx', 'exact'.</p> </li> <li> <code>sm_alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator. If 'trainable', alpha is trainable.</p> </li> <li> <code>sm_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers that use the Sm operator.</p> </li> <li> <code>sm_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps for the Sm operator.</p> </li> <li> <code>sm_pre</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator before the attention pooling.</p> </li> <li> <code>sm_post</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator after the attention pooling.</p> </li> <li> <code>sm_spectral_norm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply spectral normalization to all linear layers.</p> </li> </ul>"},{"location":"api/nn/attention/sm_attention_pool/#torchmil.nn.attention.SmAttentionPool.forward","title":"<code>forward(X, adj, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>z</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>Tensor</code> )          \u2013            <p>Bag representation of shape <code>(batch_size, in_dim)</code>.</p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/nn/gnns/","title":"torchmil.nn.gnns","text":"<ul> <li>Deep Graph Convolutional Network (DeepGCN) layer</li> <li>Graph Convolutional Network (GCN) convolution</li> <li>Dense MinCut pooling</li> </ul>"},{"location":"api/nn/gnns/deepgcn/","title":"DeepGCNLayer","text":""},{"location":"api/nn/gnns/deepgcn/#torchmil.nn.gnns.DeepGCNLayer","title":"<code>torchmil.nn.gnns.DeepGCNLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implementation of a DeepGCN layer.</p> <p>Adapts the implementation from torch_geometric.</p>"},{"location":"api/nn/gnns/deepgcn/#torchmil.nn.gnns.DeepGCNLayer.__init__","title":"<code>__init__(conv=None, norm=None, act=None, block='plain', dropout=0.0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>conv</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Convolutional layer.</p> </li> <li> <code>norm</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Normalization layer.</p> </li> <li> <code>act</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Activation layer.</p> </li> <li> <code>block</code>               (<code>str</code>, default:                   <code>'plain'</code> )           \u2013            <p>Skip connection type. Possible values: 'res', 'res+', 'dense', 'plain'.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/gnns/deepgcn/#torchmil.nn.gnns.DeepGCNLayer.forward","title":"<code>forward(x, adj)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Node features of shape <code>(batch_size, n_nodes, in_dim)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, n_nodes, n_nodes)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, n_nodes, out_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/gnns/dense_mincut_pool/","title":"Dense MinCut pooling","text":""},{"location":"api/nn/gnns/dense_mincut_pool/#torchmil.nn.gnns.dense_mincut_pool","title":"<code>torchmil.nn.gnns.dense_mincut_pool</code>","text":""},{"location":"api/nn/gnns/dense_mincut_pool/#torchmil.nn.gnns.dense_mincut_pool.dense_mincut_pool","title":"<code>dense_mincut_pool(x, adj, s, mask=None, temp=1.0)</code>","text":"<p>Dense MinCut Pooling.</p> <p>Adapts the implementation from torch_geometric.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, n_nodes, in_dim)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency tensor of shape <code>(batch_size, n_nodes, n_nodes)</code>.</p> </li> <li> <code>s</code>               (<code>Tensor</code>)           \u2013            <p>Dense learned assignments tensor of shape <code>(batch_size, n_nodes, n_cluster)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, n_nodes)</code>.</p> </li> <li> <code>temp</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_</code> (              <code>Tensor</code> )          \u2013            <p>Pooled node feature tensor of shape <code>(batch_size, n_cluster, in_dim)</code>.</p> </li> <li> <code>adj_</code> (              <code>Tensor</code> )          \u2013            <p>Coarsened adjacency tensor of shape <code>(batch_size, n_cluster, n_cluster)</code>.</p> </li> <li> <code>mincut_loss</code> (              <code>Tensor</code> )          \u2013            <p>MinCut loss.</p> </li> <li> <code>ortho_loss</code> (              <code>Tensor</code> )          \u2013            <p>Orthogonality loss.</p> </li> </ul>"},{"location":"api/nn/gnns/gcn_conv/","title":"GCNConv","text":""},{"location":"api/nn/gnns/gcn_conv/#torchmil.nn.gnns.GCNConv","title":"<code>torchmil.nn.gnns.GCNConv</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implementation of a Graph Convolutional Network (GCN) layer.</p> <p>Adapts the implementation from torch_geometric.</p>"},{"location":"api/nn/gnns/gcn_conv/#torchmil.nn.gnns.GCNConv.__init__","title":"<code>__init__(in_dim, out_dim=None, add_self_loops=False, learn_weights=False, layer_norm=False, normalize=False, dropout=0.0, activation=torch.nn.Identity(), bias=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension.</p> </li> <li> <code>add_self_loops</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add self-loops.</p> </li> <li> <code>learn_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a linear layer after the convolution.</p> </li> <li> <code>layer_norm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use layer normalization.</p> </li> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to l2-normalize the output.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>activation</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Activation function to apply after the convolution.</p> </li> <li> <code>bias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use bias.</p> </li> </ul>"},{"location":"api/nn/gnns/gcn_conv/#torchmil.nn.gnns.GCNConv.forward","title":"<code>forward(x, adj)</code>","text":"<p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <p>Node features of shape (batch_size, n_nodes, in_dim).</p> </li> <li> <code>adj</code>           \u2013            <p>Adjacency matrix of shape (batch_size, n_nodes, n_nodes).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape (batch_size, n_nodes, out_dim).</p> </li> </ul>"},{"location":"api/nn/transformers/","title":"torchmil.nn.transformers","text":"<ul> <li>Transformer base class</li> <li>Conventional Transformer</li> <li>Sm Transformer</li> <li>Nystrom Transformer</li> <li>Transformer with image Relative Positional Encoding (iRPE)</li> <li>Tokens-2-Token</li> </ul>"},{"location":"api/nn/transformers/base_transformer/","title":"Transformer base class","text":""},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Encoder","title":"<code>torchmil.nn.transformers.Encoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generic Transformer encoder class.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\) and (optional) additional arguments, this module computes:</p> \\[\\begin{align*} \\mathbf{X}^{0} &amp; = \\mathbf{X} \\\\ \\mathbf{X}^{l} &amp; = \\operatorname{Layer}^{l}\\left( \\mathbf{X}^{l-1}, \\ldots \\right), \\quad l = 1, \\ldots, L \\\\ \\end{align*}\\] <p>where \\(\\ldots\\) denotes additional arguments. The list of layers, \\(\\operatorname{Layer}^{l}\\) for \\(l = 1, \\ldots, L\\), is given by the <code>layers</code> argument, and should be a subclass of Layer.</p> <p>This module outputs \\(\\operatorname{Encoder}(\\mathbf{X}) = \\mathbf{X}^{L}\\) if <code>add_self=False</code>, and \\(\\operatorname{Encoder}(\\mathbf{X}) = \\mathbf{X}^{L} + \\mathbf{X}\\) if <code>add_self=True</code>.</p>"},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Encoder.__init__","title":"<code>__init__(layers, add_self=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>layers</code>               (<code>ModuleList</code>)           \u2013            <p>List of encoder layers.</p> </li> <li> <code>add_self</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add input to output. If True, the input and output dimensions must match.</p> </li> </ul>"},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Encoder.forward","title":"<code>forward(X, return_att=False, **kwargs)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Layer","title":"<code>torchmil.nn.transformers.Layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generic Transformer layer class.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), and (optional) additional arguments, this module computes:</p> \\[\\begin{align*} \\mathbf{Z} &amp; = \\mathbf{X} + \\operatorname{Att}( \\operatorname{LayerNorm}(\\mathbf{X}) ) \\\\ \\mathbf{Y} &amp; = \\mathbf{Z} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z})), \\\\ \\end{align*}\\] <p>and outputs \\(\\mathbf{Y}\\). \\(\\operatorname{Att}\\) is given by the <code>att_module</code> argument, and \\(\\operatorname{MLP}\\) is given by the <code>mlp_module</code> argument.</p>"},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Layer.__init__","title":"<code>__init__(att_module, in_dim, att_in_dim, out_dim=None, att_out_dim=None, use_mlp=True, mlp_module=None, dropout=0.0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>att_module</code>               (<code>Module</code>)           \u2013            <p>Attention module. Assumes input of shape <code>(batch_size, seq_len, att_in_dim)</code> and outputs of shape <code>(batch_size, seq_len, att_out_dim)</code>.</p> </li> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>att_in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension for the attention module.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension for the attention module. If None, att_out_dim = in_dim.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use a MLP after the attention layer.</p> </li> <li> <code>mlp_module</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>MLP module.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Layer.forward","title":"<code>forward(X, return_att=False, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len, in_dim)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention weights, of shape <code>(batch_size, n_heads, seq_len, seq_len)</code>.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments for the attention module.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, seq_len, out_dim)</code>.</p> </li> <li> <code>Tensor</code>           \u2013            <p>If <code>return_att</code> is True, also returns attention weights, of shape <code>(batch_size, n_heads, seq_len, seq_len)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/conventional_transformer/","title":"Conventional Transformer","text":""},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerEncoder","title":"<code>torchmil.nn.transformers.TransformerEncoder</code>","text":"<p>               Bases: <code>Encoder</code></p> <p>A Transformer encoder with skip connections and layer normalization.</p> <p>Given an input bag input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), it computes:</p> \\[\\begin{align*} \\mathbf{X}^{0} &amp; = \\mathbf{X} \\\\ \\mathbf{Z}^{l} &amp; = \\mathbf{X}^{l-1} + \\operatorname{SelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}^{l-1}) ), \\quad l = 1, \\ldots, L \\\\ \\mathbf{X}^{l} &amp; = \\mathbf{Z}^{l} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z}^{l})), \\quad l = 1, \\ldots, L. \\\\ \\end{align*}\\] <p>This module outputs \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L}\\) if <code>add_self=False</code>, and \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L} + \\mathbf{X}\\) if <code>add_self=True</code>.</p>"},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerEncoder.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, n_layers=4, use_mlp=True, add_self=False, dropout=0.0)</code>","text":"<p>Class constructor</p> <p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, <code>out_dim = in_dim</code>.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of layers.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>add_self</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add input to output. If True, <code>att_dim</code> must be equal to <code>in_dim</code>.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerEncoder.forward","title":"<code>forward(X, mask=None, return_att=False)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention weights, of shape <code>(n_layers, batch_size, n_heads, bag_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerLayer","title":"<code>torchmil.nn.transformers.TransformerLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>One layer of the Transformer encoder.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this module computes:</p> \\[\\begin{align*} \\mathbf{Z} &amp; = \\mathbf{X} + \\operatorname{SelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}) ) \\\\ \\mathbf{Y} &amp; = \\mathbf{Z} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z})), \\\\ \\end{align*}\\] <p>and outputs \\(\\mathbf{Y}\\).</p>"},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerLayer.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, use_mlp=True, dropout=0.0)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate</p> </li> </ul>"},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerLayer.forward","title":"<code>forward(X, mask=None, return_att=False)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention weights, of shape <code>(batch_size, n_heads, bag_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, out_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/irpe_transformer/","title":"iRPE Transformer","text":""},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerEncoder","title":"<code>torchmil.nn.transformers.iRPETransformerEncoder</code>","text":"<p>               Bases: <code>Encoder</code></p> <p>Given an input bag input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), it computes:</p> \\[\\begin{align*} \\mathbf{X}^{0} &amp; = \\mathbf{X} \\\\ \\mathbf{Z}^{l} &amp; = \\mathbf{X}^{l-1} + \\operatorname{iRPESelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}^{l-1}) ), \\quad l = 1, \\ldots, L \\\\ \\mathbf{X}^{l} &amp; = \\mathbf{Z}^{l} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z}^{l})), \\quad l = 1, \\ldots, L. \\\\ \\end{align*}\\] <p>See iRPEMultiheadSelfAttention for more details about \\(\\operatorname{iRPESelfAttention}\\).</p> <p>This module outputs \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L}\\) if <code>add_self=False</code>, and \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L} + \\mathbf{X}\\) if <code>add_self=True</code>.</p>"},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerEncoder.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, n_layers=4, use_mlp=True, add_self=False, dropout=0.0, rpe_ratio=1.9, rpe_method='product', rpe_mode='contextual', rpe_shared_head=True, rpe_skip=1, rpe_on='k')</code>","text":"<p>Class constructor</p> <p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of layers.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>add_self</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add input to output.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>rpe_ratio</code>               (<code>float</code>, default:                   <code>1.9</code> )           \u2013            <p>Relative position encoding ratio.</p> </li> <li> <code>rpe_method</code>               (<code>str</code>, default:                   <code>'product'</code> )           \u2013            <p>Relative position encoding method. Possible values: ['euc', 'quant', 'cross', 'product']</p> </li> <li> <code>rpe_mode</code>               (<code>str</code>, default:                   <code>'contextual'</code> )           \u2013            <p>Relative position encoding mode. Possible values: [None, 'bias', 'contextual']</p> </li> <li> <code>rpe_shared_head</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to share weights across heads.</p> </li> <li> <code>rpe_skip</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Relative position encoding skip. Possible values: [0, 1]. </p> </li> <li> <code>rpe_on</code>               (<code>str</code>, default:                   <code>'k'</code> )           \u2013            <p>Where to apply relative positional encoding. Possible values: ['q', 'k', 'v', 'qk', 'kv', 'qkv'].</p> </li> </ul>"},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerEncoder.forward","title":"<code>forward(X, return_att=False)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention weights, of shape <code>(n_layers, batch_size, n_heads, bag_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerLayer","title":"<code>torchmil.nn.transformers.iRPETransformerLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Transformer layer with image Relative Position Encoding (iRPE), as described in Rethinking and Improving Relative Position Encoding for Vision Transformer.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this module computes:</p> \\[\\begin{align*} \\mathbf{Z} &amp; = \\mathbf{X} + \\operatorname{iRPESelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}) ) \\\\ \\mathbf{Y} &amp; = \\mathbf{Z} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z})), \\\\ \\end{align*}\\] <p>and outputs \\(\\mathbf{Y}\\). See iRPEMultiheadSelfAttention for more details about \\(\\operatorname{iRPESelfAttention}\\).</p>"},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerLayer.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, use_mlp=True, dropout=0.0, rpe_ratio=1.9, rpe_method='product', rpe_mode='contextual', rpe_shared_head=True, rpe_skip=1, rpe_on='k')</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension. If None, in_dim = att_dim.</p> </li> <li> <code>out_dim</code>           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>rpe_ratio</code>               (<code>float</code>, default:                   <code>1.9</code> )           \u2013            <p>Relative position encoding ratio.</p> </li> <li> <code>rpe_method</code>               (<code>str</code>, default:                   <code>'product'</code> )           \u2013            <p>Relative position encoding method. Possible values: ['euc', 'quant', 'cross', 'product']</p> </li> <li> <code>rpe_mode</code>               (<code>str</code>, default:                   <code>'contextual'</code> )           \u2013            <p>Relative position encoding mode. Possible values: [None, 'bias', 'contextual']</p> </li> <li> <code>rpe_shared_head</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to share weights across heads.</p> </li> <li> <code>rpe_skip</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Relative position encoding skip. Possible values: [0, 1]. </p> </li> <li> <code>rpe_on</code>               (<code>str</code>, default:                   <code>'k'</code> )           \u2013            <p>Where to apply relative positional encoding. Possible values: ['q', 'k', 'v', 'qk', 'kv', 'qkv'].</p> </li> </ul>"},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerLayer.forward","title":"<code>forward(X, return_att=False)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, out_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/nystrom_transformer/","title":"Nystr\u00f6m Transformer","text":""},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerEncoder","title":"<code>torchmil.nn.transformers.NystromTransformerEncoder</code>","text":"<p>               Bases: <code>Encoder</code></p> <p>Nystrom Transformer encoder with skip connections and layer normalization.</p> <p>Given an input bag input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), it computes:</p> \\[\\begin{align*} \\mathbf{X}^{0} &amp; = \\mathbf{X} \\\\ \\mathbf{Z}^{l} &amp; = \\mathbf{X}^{l-1} + \\operatorname{NystromSelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}^{l-1}) ), \\quad l = 1, \\ldots, L \\\\ \\mathbf{X}^{l} &amp; = \\mathbf{Z}^{l} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z}^{l})), \\quad l = 1, \\ldots, L \\\\ \\end{align*}\\] <p>This module outputs \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L}\\) if <code>add_self=False</code>, and \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L} + \\mathbf{X}\\) if <code>add_self=True</code>.</p> <p>\\(\\operatorname{NystromSelfAttention}\\) is implemented using the NystromAttention module, see NystromAttention.</p>"},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerEncoder.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=8, n_layers=4, n_landmarks=256, pinv_iterations=6, dropout=0.0, use_mlp=False, add_self=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, <code>out_dim = in_dim</code>.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of layers.</p> </li> <li> <code>n_landmarks</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Number of landmarks.</p> </li> <li> <code>pinv_iterations</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of iterations for the pseudo-inverse.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a MLP after the attention layer.</p> </li> <li> <code>add_self</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add the input to the output. If True, <code>att_dim</code> must be equal to <code>in_dim</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerEncoder.forward","title":"<code>forward(X, mask=None, return_att=False)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, att_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return attention weights.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, att_dim)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention weights of shape <code>(batch_size, n_heads, bag_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerLayer","title":"<code>torchmil.nn.transformers.NystromTransformerLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>One layer of the NystromTransformer encoder.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this module computes:</p> \\[\\begin{align*} \\mathbf{Z} &amp; = \\mathbf{X} + \\operatorname{NystromSelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}) ) \\\\ \\mathbf{Y} &amp; = \\mathbf{Z} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z})), \\\\ \\end{align*}\\] <p>and outputs \\(\\mathbf{Y}\\). \\(\\operatorname{NystromSelfAttention}\\) is implemented using the NystromAttention module, see NystromAttention.</p>"},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerLayer.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, learn_weights=True, n_landmarks=256, pinv_iterations=6, dropout=0.0, use_mlp=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>n_landmarks</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Number of landmarks.</p> </li> <li> <code>pinv_iterations</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of iterations for the pseudo-inverse.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a MLP after the attention layer.</p> </li> </ul>"},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerLayer.forward","title":"<code>forward(X, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, att_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return attention weights.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>X</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, att_dim)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention weights of shape <code>(batch_size, n_heads, bag_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/sm_transformer/","title":"Sm Transformer","text":""},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerEncoder","title":"<code>torchmil.nn.transformers.SmTransformerEncoder</code>","text":"<p>               Bases: <code>Encoder</code></p> <p>A Transformer encoder with the \\(\\texttt{Sm}\\) operator, skip connections and layer normalization.</p> <p>Given an input bag input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), it computes:</p> \\[\\begin{align*} \\mathbf{X}^{0} &amp; = \\mathbf{X} \\\\ \\mathbf{Z}^{l} &amp; = \\mathbf{X}^{l-1} + \\texttt{Sm}( \\text{SelfAttention}( \\text{LayerNorm}(\\mathbf{X}^{l-1}) ) ), \\quad l = 1, \\ldots, L \\\\ \\mathbf{X}^{l} &amp; = \\mathbf{Z}^{l} + \\text{MLP}(\\text{LayerNorm}(\\mathbf{Z}^{l})), \\quad l = 1, \\ldots, L \\\\ \\end{align*}\\] <p>This module outputs \\(\\text{SmTransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L}\\) if <code>add_self=False</code>, and \\(\\text{SmTransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L} + \\mathbf{X}\\) if <code>add_self=True</code>.</p> <p>See Sm for more details on the Sm operator.</p>"},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerEncoder.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, n_layers=4, use_mlp=True, add_self=False, dropout=0.0, sm_alpha='trainable', sm_mode='approx', sm_steps=10)</code>","text":"<p>Class constructor</p> <p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of layers.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>add_self</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add input to output. If True, <code>att_dim</code> must be equal to <code>in_dim</code>.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerEncoder.forward","title":"<code>forward(X, adj, mask=None, return_att=False)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerLayer","title":"<code>torchmil.nn.transformers.SmTransformerLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>One layer of the Transformer encoder with the \\(\\texttt{Sm}\\) operator.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this module computes:</p> \\[\\begin{align*} \\mathbf{Z} &amp; = \\mathbf{X} + \\texttt{Sm}( \\text{SelfAttention}( \\text{LayerNorm}(\\mathbf{X}) ) )\\\\ \\mathbf{Y} &amp; = \\mathbf{Z} + \\text{MLP}(\\text{LayerNorm}(\\mathbf{Z})), \\\\ \\end{align*}\\] <p>and outputs \\(\\mathbf{Y}\\).</p> <p>See Sm for more details on the Sm operator.</p>"},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerLayer.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, use_mlp=True, dropout=0.0, sm_alpha='trainable', sm_mode='approx', sm_steps=10)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate</p> </li> <li> <code>sm_alpha</code>               (<code>float</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator.</p> </li> <li> <code>sm_mode</code>               (<code>str</code>, default:                   <code>'approx'</code> )           \u2013            <p>Sm mode.</p> </li> <li> <code>sm_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps to approximate the exact Sm operator.</p> </li> </ul>"},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerLayer.forward","title":"<code>forward(X, adj, mask=None, return_att=False)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention weights, of shape <code>(batch_size, n_heads, bag_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/t2t/","title":"Tokens-to-Token (T2T)","text":""},{"location":"api/nn/transformers/t2t/#torchmil.nn.transformers.T2TLayer","title":"<code>torchmil.nn.transformers.T2TLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Tokens-to-Token (T2T) Transformer layer from Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</p>"},{"location":"api/nn/transformers/t2t/#torchmil.nn.transformers.T2TLayer.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(1, 1), n_heads=4, use_mlp=True, dropout=0.0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, output dimension will be <code>kernel_size[0] * kernel_size[1] * att_dim</code>.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>kernel_size</code>               (<code>tuple[int, int]</code>, default:                   <code>(3, 3)</code> )           \u2013            <p>Kernel size.</p> </li> <li> <code>stride</code>               (<code>tuple[int, int]</code>, default:                   <code>(1, 1)</code> )           \u2013            <p>Stride.</p> </li> <li> <code>padding</code>               (<code>tuple[int, int]</code>, default:                   <code>(2, 2)</code> )           \u2013            <p>Padding.</p> </li> <li> <code>dilation</code>               (<code>tuple[int, int]</code>, default:                   <code>(1, 1)</code> )           \u2013            <p>Dilation.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/transformers/t2t/#torchmil.nn.transformers.T2TLayer.forward","title":"<code>forward(X)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len, in_dim)</code>.</p> </li> </ul> <p>Returns:     Y: Output tensor of shape <code>(batch_size, new_seq_len, out_dim)</code>. If <code>out_dim</code> is None, <code>out_dim</code> will be <code>att_dim * kernel_size[0] * kernel_size[1]</code>.</p>"},{"location":"api/utils/","title":"torchmil.utils","text":"<p>torchmil.utils contains a collection of utility functions and classes that are commonly used in torchmil and, more generally, in the context of Multiple Instance Learning (MIL) tasks. These utilities can help streamline the development process and improve the overall efficiency of your MIL models.</p> <ul> <li>Annealing Scheduler</li> <li>Graph utils</li> <li>Trainer</li> </ul>"},{"location":"api/utils/annealing_scheduler/","title":"Annealing Scheduler","text":"<p>Here we provide a collection of annealing schedulers that can be used to adjust the weighting coefficient of the loss functions during training. These schedulers can help improve the convergence and performance of your model.</p> <p>Deep Learning models are often trained using a combination of multiple loss functions:</p> \\[ L = \\sum_{i=1}^{n} \\lambda_i L_i \\] <p>where \\(L_i\\) is the \\(i\\)-th loss function and \\(\\lambda_i\\) is the weighting coefficient for that loss function. The weighting coefficients can be adjusted during training to improve the performance of the model:</p> <p>$$ L(t) = \\sum_{i=1}^{n} \\lambda_i(t) L_i $$ where \\(t\\) is the training step or epoch. The value of \\(\\lambda_i(t)\\) can be adjusted using different annealing strategies, such as linear, cyclical, or constant annealing.</p>"},{"location":"api/utils/annealing_scheduler/#torchmil.utils.AnnealingScheduler","title":"<code>torchmil.utils.AnnealingScheduler</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.AnnealingScheduler.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.AnnealingScheduler.step","title":"<code>step()</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.AnnealingScheduler.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.ConstantAnnealingScheduler","title":"<code>torchmil.utils.ConstantAnnealingScheduler</code>","text":"<p>               Bases: <code>AnnealingScheduler</code></p>"},{"location":"api/utils/annealing_scheduler/#torchmil.utils.ConstantAnnealingScheduler.__init__","title":"<code>__init__(coef=1.0, *args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.ConstantAnnealingScheduler.step","title":"<code>step()</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.ConstantAnnealingScheduler.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.LinearAnnealingScheduler","title":"<code>torchmil.utils.LinearAnnealingScheduler</code>","text":"<p>               Bases: <code>AnnealingScheduler</code></p>"},{"location":"api/utils/annealing_scheduler/#torchmil.utils.LinearAnnealingScheduler.__init__","title":"<code>__init__(coef_init=0.0, coef_end=1.0, n_steps=100, *args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.LinearAnnealingScheduler.step","title":"<code>step()</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.LinearAnnealingScheduler.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.CyclicalAnnealingScheduler","title":"<code>torchmil.utils.CyclicalAnnealingScheduler</code>","text":"<p>               Bases: <code>AnnealingScheduler</code></p>"},{"location":"api/utils/annealing_scheduler/#torchmil.utils.CyclicalAnnealingScheduler.__init__","title":"<code>__init__(cycle_len, min_coef=0.0, max_coef=1.0, cycle_prop=0.5, warmup_steps=0, verbose=False, *args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.CyclicalAnnealingScheduler.step","title":"<code>step()</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.CyclicalAnnealingScheduler.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":""},{"location":"api/utils/graph_utils/","title":"Graph utils","text":""},{"location":"api/utils/graph_utils/#torchmil.utils.degree","title":"<code>torchmil.utils.degree(index, edge_weight=None, n_nodes=None)</code>","text":"<p>Compute the degree of the adjacency matrix. Assumes that the graph is undirected.</p> <p>Parameters:</p> <ul> <li> <code>index</code>               (<code>ndarray</code>)           \u2013            <p>Edge index of the adjacency matrix, shape (2, n_edges).</p> </li> <li> <code>edge_weight</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>Edge weight of the adjacency matrix, shape (n_edges,).</p> </li> <li> <code>n_nodes</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of nodes in the graph.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>degree</code> (              <code>ndarray</code> )          \u2013            <p>Degree of the adjacency matrix.</p> </li> </ul>"},{"location":"api/utils/graph_utils/#torchmil.utils.add_self_loops","title":"<code>torchmil.utils.add_self_loops(edge_index, edge_weight=None, n_nodes=None)</code>","text":"<p>Add self-loops to the adjacency matrix.</p> <p>Parameters:</p> <ul> <li> <code>edge_index</code>               (<code>ndarray</code>)           \u2013            <p>Edge index of the adjacency matrix, shape (2, n_edges).</p> </li> <li> <code>edge_weight</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>Edge weight of the adjacency matrix, shape (n_edges,).</p> </li> <li> <code>n_nodes</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of nodes in the graph.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_edge_index</code> (              <code>ndarray</code> )          \u2013            <p>Edge index of the adjacency matrix with self-loops.</p> </li> <li> <code>new_edge_weight</code> (              <code>ndarray</code> )          \u2013            <p>Edge weight of the adjacency matrix with self-loops.</p> </li> </ul>"},{"location":"api/utils/graph_utils/#torchmil.utils.normalize_adj","title":"<code>torchmil.utils.normalize_adj(edge_index, edge_weight=None, n_nodes=None)</code>","text":"<p>Normalize the adjacency matrix.</p> <p>Parameters:</p> <ul> <li> <code>edge_index</code>               (<code>ndarray</code>)           \u2013            <p>Edge index of the adjacency matrix.</p> </li> <li> <code>edge_weight</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>Edge weight of the adjacency matrix.</p> </li> <li> <code>n_nodes</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of nodes in the graph.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>edge_weight</code> (              <code>ndarray</code> )          \u2013            <p>Edge weight of the normalized adjacency matrix.</p> </li> </ul>"},{"location":"api/utils/graph_utils/#torchmil.utils.build_adj","title":"<code>torchmil.utils.build_adj(coords, feat=None, dist_thr=1.0, add_self_loops=False)</code>","text":"<p>Build the adjacency matrix for a general graph given the coordinates and features of the nodes.</p> <p>Parameters:</p> <ul> <li> <code>coords</code>               (<code>ndarray</code>)           \u2013            <p>Coordinates of the nodes.</p> </li> <li> <code>feat</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>Features of the nodes, used to compute the edge weights. If None, the adjacency matrix is binary.</p> </li> <li> <code>dist_thr</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Distance threshold to consider two nodes as neighbors. Default is 1.0.</p> </li> <li> <code>add_self_loops</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add self-loops.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>edge_index</code> (              <code>ndarray</code> )          \u2013            <p>Edge index of the adjacency matrix.</p> </li> <li> <code>edge_weight</code> (              <code>ndarray</code> )          \u2013            <p>Edge weight of the adjacency matrix</p> </li> </ul>"},{"location":"api/utils/trainer/","title":"Trainer","text":""},{"location":"api/utils/trainer/#torchmil.utils.Trainer","title":"<code>torchmil.utils.Trainer</code>","text":"<p>Generic trainer class for training MIL models.</p>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer.__init__","title":"<code>__init__(model, optimizer, metrics_dict={'accuracy': torchmetrics.Accuracy(task='binary')}, obj_metric='accuracy', obj_metric_mode='max', lr_scheduler=None, annealing_scheduler_dict=None, device='cuda', logger=None, early_stop_patience=None, disable_pbar=False, verbose=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>model</code>               (<code>MILModel</code>)           \u2013            <p>MIL model to be trained. Must be an instance of MILModel.</p> </li> <li> <code>optimizer</code>               (<code>Optimizer</code>)           \u2013            <p>Optimizer for training the model.</p> </li> <li> <code>metrics_dict</code>               (<code>dict[str:Metric]</code>, default:                   <code>{'accuracy': Accuracy(task='binary')}</code> )           \u2013            <p>Dictionary of metrics to be computed during training. Metrics should be instances of torchmetrics.Metric.</p> </li> <li> <code>obj_metric</code>               (<code>str</code>, default:                   <code>'accuracy'</code> )           \u2013            <p>Objective metric to be used for early stopping and to track the best model. Must be one of the keys in <code>metrics_dict</code>.</p> </li> <li> <code>obj_metric_mode</code>               (<code>str</code>, default:                   <code>'max'</code> )           \u2013            <p>Mode for the objective metric. Must be one of 'max' or 'min'. If 'max', the best model is the one with the highest value of the objective metric. If 'min', the best model is the one with the lowest value of the objective metric.</p> </li> <li> <code>lr_scheduler</code>               (<code>_LRScheduler</code>, default:                   <code>None</code> )           \u2013            <p>Learning rate scheduler.</p> </li> <li> <code>annealing_scheduler_dict</code>               (<code>dict[str:AnnealingScheduler]</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary of annealing schedulers for loss coefficients. Keys should be the loss names and values should be instances of AnnealingScheduler.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>Device to be used for training.</p> </li> <li> <code>logger</code>           \u2013            <p>Logger to log metrics. Must have a <code>log</code> method. It can be, for example, a Wandb Run.</p> </li> <li> <code>early_stop_patience</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Patience for early stopping. If None, early stopping is disabled.</p> </li> <li> <code>disable_pbar</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Disable progress bar.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer.train","title":"<code>train(max_epochs, train_dataloader, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Train the model.</p> <p>Parameters:</p> <ul> <li> <code>max_epochs</code>               (<code>int</code>)           \u2013            <p>Maximum number of epochs to train.</p> </li> <li> <code>train_dataloader</code>               (<code>DataLoader</code>)           \u2013            <p>Train dataloader.</p> </li> <li> <code>val_dataloader</code>               (<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>Validation dataloader. If None, the train dataloader is used.</p> </li> <li> <code>test_dataloader</code>               (<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>Test dataloader. If None, test metrics are not computed.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer.get_model_state_dict","title":"<code>get_model_state_dict()</code>","text":"<p>Get (a deepcopy of) the state dictionary of the model.</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>State dictionary of the model.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer.get_best_model_state_dict","title":"<code>get_best_model_state_dict()</code>","text":"<p>Get the state dictionary of the best model (the model with the best objective metric).</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>State dictionary of the best model.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer.get_best_model","title":"<code>get_best_model()</code>","text":"<p>Get the best model (the model with the best objective metric).</p> <p>Returns:</p> <ul> <li> <code>MILModel</code>           \u2013            <p>Best model.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer._log","title":"<code>_log(metrics)</code>","text":"<p>Log metrics using the logger.</p> <p>Parameters:</p> <ul> <li> <code>metrics</code>               (<code>dict[str:float]</code>)           \u2013            <p>Dictionary of metrics to be logged.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer._shared_loop","title":"<code>_shared_loop(dataloader, epoch=0, mode='train')</code>","text":"<p>Shared training/validation/test loop.</p> <p>Parameters:</p> <ul> <li> <code>dataloader</code>               (<code>DataLoader</code>)           \u2013            <p>Dataloader.</p> </li> <li> <code>epoch</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Epoch number.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Mode of the loop. Must be one of 'train', 'val', 'test'.</p> </li> </ul>"},{"location":"api/visualize/","title":"torchmil.visualize","text":"<p>torchmil.visualize provides a collection of visualization tools for Multiple Instance Learning (MIL) tasks. These tools are designed to help you understand and interpret the results of your MIL models, making it easier to analyze their performance and gain insights from the data.</p> <ul> <li>Visualizing CT scans</li> <li>Visualizing WSIs</li> </ul>"},{"location":"api/visualize/vis_ctscan/","title":"Visualizing CT scans","text":""},{"location":"api/visualize/vis_ctscan/#torchmil.visualize.slices_to_canvas","title":"<code>torchmil.visualize.slices_to_canvas(slices_list, slice_size)</code>","text":"<p>Given a list of images of CT scan slices, return a canvas with all the slices.</p> <p>Parameters:</p> <ul> <li> <code>slices_list</code>               (<code>list[ndarray]</code>)           \u2013            <p>List of images of CT scan slices. Each image is a numpy array with shape <code>(slice_size, slice_size, 3)</code>.</p> </li> <li> <code>slice_size</code>               (<code>int</code>)           \u2013            <p>Size of the slices.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with all the slices. It has shape <code>(slice_size, bag_len*slice_size, 3)</code>.</p> </li> </ul>"},{"location":"api/visualize/vis_ctscan/#torchmil.visualize.draw_slices_contour","title":"<code>torchmil.visualize.draw_slices_contour(canvas, slice_size, contour_prop=0.05)</code>","text":"<p>Given a canvas with CT scan slices already drawn, draw a contour around each slice.</p> <p>Parameters:</p> <ul> <li> <code>canvas</code>               (<code>ndarray</code>)           \u2013            <p>Canvas with all the slices. It has shape <code>(slice_size, bag_len*slice_size, 3)</code>.</p> </li> <li> <code>slice_size</code>               (<code>int</code>)           \u2013            <p>Size of the slices.</p> </li> <li> <code>contour_prop</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>Proportion of the slice size that the contour will cover.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with the contours drawn. It has shape <code>(slice_size, bag_len*slice_size, 3)</code>.</p> </li> </ul>"},{"location":"api/visualize/vis_ctscan/#torchmil.visualize.draw_heatmap_ctscan","title":"<code>torchmil.visualize.draw_heatmap_ctscan(canvas, values, slice_size, alpha=0.5, max_color=np.array([0.8392156862745098, 0.15294117647058825, 0.1568627450980392]), min_color=np.array([0.17254901960784313, 0.6274509803921569, 0.17254901960784313]))</code>","text":"<p>Given a canvas with CT scan slices already drawn, draw a heatmap on top of the slices. This heatmap is defined by <code>values</code>, which should be normalized between 0 and 1.</p> <p>Parameters:</p> <ul> <li> <code>canvas</code>               (<code>ndarray</code>)           \u2013            <p>Canvas with all the slices. It has shape <code>(slice_size, bag_len*slice_size, 3)</code>.</p> </li> <li> <code>values</code>               (<code>ndarray</code>)           \u2013            <p>List of values to draw the heatmap. Each value should be normalized between 0 and 1.</p> </li> <li> <code>slice_size</code>               (<code>int</code>)           \u2013            <p>Size of the slices.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Alpha value for blending the heatmap with the canvas.</p> </li> <li> <code>max_color</code>               (<code>ndarray</code>, default:                   <code>array([0.8392156862745098, 0.15294117647058825, 0.1568627450980392])</code> )           \u2013            <p>Color for the maximum value in the heatmap.</p> </li> <li> <code>min_color</code>               (<code>ndarray</code>, default:                   <code>array([0.17254901960784313, 0.6274509803921569, 0.17254901960784313])</code> )           \u2013            <p>Color for the minimum value in the heatmap.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with the heatmap drawn. It has shape <code>(slice_size, bag_len*slice_size, 3)</code>.</p> </li> </ul>"},{"location":"api/visualize/vis_wsi/","title":"Visualizing WSIs","text":""},{"location":"api/visualize/vis_wsi/#torchmil.visualize.patches_to_canvas","title":"<code>torchmil.visualize.patches_to_canvas(patches_list, row_array, column_array, patch_size)</code>","text":"<p>Given a list of WSI patches and their corresponding row and column indices, return a canvas with all the patches.</p> <p>Parameters:</p> <ul> <li> <code>patches_list</code>               (<code>list</code>)           \u2013            <p>List of WSI patches. Each patch is a numpy array with shape <code>(patch_size, patch_size, 3)</code>.</p> </li> <li> <code>row_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the row indices of the patches.</p> </li> <li> <code>column_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the column indices of the patches.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Size of the patches.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with all the patches. It has shape <code>(max_row*patch_size, max_column*patch_size, 3)</code>.</p> </li> </ul>"},{"location":"api/visualize/vis_wsi/#torchmil.visualize.draw_patches_contour","title":"<code>torchmil.visualize.draw_patches_contour(canvas, row_array, column_array, patch_size, contour_prop=0.05)</code>","text":"<p>Given a canvas with WSI patches already drawn, draw a contour around each patch.</p> <p>Parameters:</p> <ul> <li> <code>canvas</code>               (<code>ndarray</code>)           \u2013            <p>Canvas with all the patches. It has shape <code>(max_row*patch_size, max_column*patch_size, 3)</code>.</p> </li> <li> <code>row_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the row indices of the patches.</p> </li> <li> <code>column_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the column indices of the patches.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Size of the patches.</p> </li> <li> <code>contour_prop</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>Proportion of the patch size that the contour will cover.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with the contours drawn. It has shape <code>(max_row*patch_size, max_column*patch_size, 3)</code>.</p> </li> </ul>"},{"location":"api/visualize/vis_wsi/#torchmil.visualize.draw_heatmap_wsi","title":"<code>torchmil.visualize.draw_heatmap_wsi(canvas, values, patch_size, row_array, col_array, alpha=0.5, max_color=np.array([0.8392156862745098, 0.15294117647058825, 0.1568627450980392]), min_color=np.array([0.17254901960784313, 0.6274509803921569, 0.17254901960784313]))</code>","text":"<p>Given a canvas with WSI patches already drawn, draw a heatmap on top of the patches. This heatmap is defined by <code>values</code>, which should be normalized between 0 and 1.</p> <p>Parameters:</p> <ul> <li> <code>canvas</code>               (<code>ndarray</code>)           \u2013            <p>Canvas with all the patches. It has shape <code>(max_row*patch_size, max_column*patch_size, 3)</code>.</p> </li> <li> <code>values</code>               (<code>ndarray</code>)           \u2013            <p>Array with the values of the heatmap.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Size of the patches.</p> </li> <li> <code>row_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the row indices of the patches.</p> </li> <li> <code>col_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the column indices of the patches.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Alpha value of the heatmap.</p> </li> <li> <code>max_color</code>               (<code>ndarray</code>, default:                   <code>array([0.8392156862745098, 0.15294117647058825, 0.1568627450980392])</code> )           \u2013            <p>Color of the highest value of the heatmap.</p> </li> <li> <code>min_color</code>               (<code>ndarray</code>, default:                   <code>array([0.17254901960784313, 0.6274509803921569, 0.17254901960784313])</code> )           \u2013            <p>Color of the lowest value of the heatmap.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with the heatmap drawn. It has shape <code>(max_row*patch_size, max_column*patch_size, 3)</code>.</p> </li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Here you can find a collection of examples that demonstrate how to use torchmil in practice.</p> <ul> <li>Representing data in torchmil: we explain how data is represented in torchmil. We cover bags, instances, graphs, mini-batching, and more.</li> <li>Datasets in torchmil: we explain how datasets are implemented in torchmil. We cover processed datasets vs non-processed datasets, and how to create your own dataset.</li> <li>Training your first MIL model: we show how to train a simple attention-based MIL model with a toy dataset.</li> <li>WSI classification in torchmil: we show how to train an attention-based MIL model with WSI data.</li> <li>CT scan classification in torchmil: we show how to train a transformer-based MIL model with CT scan data.</li> </ul>"},{"location":"examples/ctscan_classification/","title":"CT scan classification","text":"<pre><code>import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n# torchmil's facilities\nfrom torchmil.visualize.vis_ctscan import slices_to_canvas, draw_slices_contour\n\nCSV_PATH = '/data/datasets/RSNA_ICH/bags_train.csv' \nIMG_PATH = '/data/datasets/RSNA_ICH/original/'\n\n# Randomly select a bag\ndf = pd.read_csv(CSV_PATH)\nbag_names = df['bag_name'].unique()\nbag_name = bag_names[np.random.randint(0, len(bag_names))] # Randomly select a bag\n\nbag_df = df[df['bag_name'] == bag_name].sort_values('order')\ninst_names = bag_df['instance_name'].values\ninst_labels = bag_df['instance_label'].values\ninst_names_list = [inst_name.split('.')[0] for inst_name in inst_names]\ninst_imgs = [np.load(IMG_PATH + inst_name + '.npy') for inst_name in inst_names_list]\nprint(\"This scan has {} slices\".format(len(inst_imgs)))\n\n# Using torchmil's functions\ncanvas = slices_to_canvas(inst_imgs, 512) \ncanvas_contours = draw_slices_contour(canvas, slice_size = 512, contour_prop = 0.05)\n\nfig, ax = plt.subplots(figsize=(30, 20))\nax.imshow((canvas_contours * 255).astype(np.uint8))\nax.set_xticks([])\nax.set_yticks([])\nplt.show()\n</code></pre> <pre>\n<code>This scan has 43 slices\n</code>\n</pre> <p>In practice, training a MIL model directly on the slices is computationally intractable. Due to this limitation, MIL models usually operate on pre-computed features extracted from each of the instances. Although torchmil allows to define models that receive the original slices as input, in this tutorial we will use the pre-computed. We have processed the RSNA dataset to be used for MIL binary classification problems. It can be downloaded from here.</p> <p>We now make use of <code>torchmil.datasets.RSNAMILDataset</code> to create an object that serves as a <code>torch.utils.data.Dataset</code> dataset and contains RSNA. You only need to provide the <code>root</code> path to the processed dataset, and the desired <code>features</code> and <code>partition</code> to load. See how simple is to instance the train dataset:</p> <pre><code>from torchmil.datasets import RSNAMILDataset\nfrom sklearn.model_selection import train_test_split\n\ndataset = RSNAMILDataset(\n    root='/data/datasets/RSNA_ICH/MIL_processed/',\n    features='resnet50',\n    partition='train',\n    load_at_init=True\n)\n\n# Split the dataset into train and validation sets\nbag_labels = dataset.get_bag_labels()\nidx = list(range(len(bag_labels)))\nval_prop = 0.2\nidx_train, idx_val = train_test_split(\n    idx, test_size=val_prop, random_state=1234, stratify=bag_labels\n)\ntrain_dataset = dataset.subset(idx_train)\nval_dataset = dataset.subset(idx_val)\n\ntest_dataset = RSNAMILDataset(\n    root='/data/datasets/RSNA_ICH/MIL_processed/',\n    features='resnet50',\n    partition='test',\n    load_at_init=True\n)\n</code></pre> <p>In torchmil, each bag is a <code>TensorDict</code>. The different keys correspond to different elements of the bag. In this case, each bag has a feature matrix <code>X</code>, the bag label <code>Y</code>, the instance coordinates <code>coords</code>, and the instance labels <code>y_inst</code>. Recall that the instance labels cannot be used during training, they are available only for evaluation purposes.</p> <pre><code>bag = train_dataset[0]\nprint(bag)\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([32, 2048]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n        coords: Tensor(shape=torch.Size([32, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([32]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)\n</code>\n</pre> <pre><code>from torchmil.data import collate_fn\n\nbatch_size = 64\n\n# Create dataloaders\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n\nit = iter(train_dataloader)\nbatch = next(it)\ndata_shape = (batch['X'].shape[-1], )\nprint(\"Batch: \", batch)\n</code></pre> <pre>\n<code>Batch:  TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([64, 52, 2048]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([64]), device=cpu, dtype=torch.int64, is_shared=False),\n        coords: Tensor(shape=torch.Size([64, 52, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n        mask: Tensor(shape=torch.Size([64, 52]), device=cpu, dtype=torch.uint8, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([64, 52]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)\n</code>\n</pre> <p>Each batch is again a <code>TensorDict</code> with an additional key <code>mask</code> that indicates which instances are real and which are padding. As we can see, the bags are padded to the maximum size of the bags in the batch with zeros. The mask tensor indicates which elements are real instances and which are padding. The function <code>collate_fn</code> also pads other tensors, such as the adjacency matrix or the instance coordinates. </p> <pre><code>from torchmil.models import TransformerABMIL\nmodel = TransformerABMIL(in_shape = data_shape)\n</code></pre> <p>See? It can not be easier! Now, let's train the model. torchmil offers an easy-to-use trainer class located in <code>torchmil.utils.trainer.Trainer</code> that provides a generic training for any MIL model. Also, it will show the evolution of the losses and the desired metrics during the epochs.</p> <p>Note</p> <p>This <code>Trainer</code> gives the flexibility to log the results using any wrapped <code>logger</code>, use annealing for the loss functions via the <code>annealing_scheduler_dict</code> dictionary, or to set a learning rate scheduler using the parameter <code>lr_scheduler</code>. Also, you can follow multiple metrics during the training thanks to the parameter <code>metrics_dict</code> and the integration with the torchmetrics package.</p> <p>For now, let us just keep it simple and perform a simple training using the <code>torch.optim.Adam</code> optimizer and training the model for 10 epochs. First, we instance the trainer.</p> <pre><code>from torchmil.utils.trainer import Trainer\nimport torchmetrics\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    metrics_dict={\n        'auroc': torchmetrics.AUROC(task='binary').to(device),\n        'acc': torchmetrics.Accuracy(task='binary').to(device)\n    },\n    obj_metric='BCEWithLogitsLoss',\n    obj_metric_mode='min',\n    device='cuda',\n    verbose=False\n)\n\ntrainer.train(\n    max_epochs=10,\n    train_dataloader=train_dataloader,\n    val_dataloader=val_dataloader\n)\n</code></pre> <pre>\n<code>[Epoch 1] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:01&lt;00:00,  9.23it/s, train/loss=0.667, train/BCEWithLogitsLoss=0.667, train/auroc=0.604, train/acc=0.596]\n[Epoch 1] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 27.33it/s, val/loss=0.586, val/BCEWithLogitsLoss=0.586, val/auroc=0.782, val/acc=0.735]\n[Epoch 2] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 20.66it/s, train/loss=0.553, train/BCEWithLogitsLoss=0.553, train/auroc=0.787, train/acc=0.709]\n[Epoch 2] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 26.74it/s, val/loss=0.655, val/BCEWithLogitsLoss=0.655, val/auroc=0.818, val/acc=0.7]\n[Epoch 3] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 21.13it/s, train/loss=0.502, train/BCEWithLogitsLoss=0.502, train/auroc=0.827, train/acc=0.75]\n[Epoch 3] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 19.09it/s, val/loss=0.535, val/BCEWithLogitsLoss=0.535, val/auroc=0.841, val/acc=0.775]\n[Epoch 4] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 20.54it/s, train/loss=0.429, train/BCEWithLogitsLoss=0.429, train/auroc=0.886, train/acc=0.801]\n[Epoch 4] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 26.81it/s, val/loss=0.538, val/BCEWithLogitsLoss=0.538, val/auroc=0.857, val/acc=0.765]\n[Epoch 5] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 20.55it/s, train/loss=0.402, train/BCEWithLogitsLoss=0.402, train/auroc=0.896, train/acc=0.824]\n[Epoch 5] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 26.81it/s, val/loss=0.642, val/BCEWithLogitsLoss=0.642, val/auroc=0.866, val/acc=0.76]\n[Epoch 6] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 20.76it/s, train/loss=0.349, train/BCEWithLogitsLoss=0.349, train/auroc=0.92, train/acc=0.851]\n[Epoch 6] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 26.43it/s, val/loss=0.567, val/BCEWithLogitsLoss=0.567, val/auroc=0.871, val/acc=0.8]\n[Epoch 7] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 20.95it/s, train/loss=0.307, train/BCEWithLogitsLoss=0.307, train/auroc=0.944, train/acc=0.88]\n[Epoch 7] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 26.75it/s, val/loss=0.544, val/BCEWithLogitsLoss=0.544, val/auroc=0.87, val/acc=0.79]\n[Epoch 8] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 20.93it/s, train/loss=0.28, train/BCEWithLogitsLoss=0.28, train/auroc=0.95, train/acc=0.894]\n[Epoch 8] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 27.06it/s, val/loss=0.526, val/BCEWithLogitsLoss=0.526, val/auroc=0.878, val/acc=0.8]\n[Epoch 9] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 20.28it/s, train/loss=0.249, train/BCEWithLogitsLoss=0.249, train/auroc=0.961, train/acc=0.901]\n[Epoch 9] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 27.12it/s, val/loss=0.506, val/BCEWithLogitsLoss=0.506, val/auroc=0.879, val/acc=0.8]\n[Epoch 10] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 20.24it/s, train/loss=0.205, train/BCEWithLogitsLoss=0.205, train/auroc=0.976, train/acc=0.923]\n[Epoch 10] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 26.81it/s, val/loss=0.723, val/BCEWithLogitsLoss=0.723, val/auroc=0.851, val/acc=0.79]\n</code>\n</pre> <p>The loss decreases as the model learns to predict the bag labels. The accuracy increases as the model learns to predict the correct bag labels. This is a good sign that the model is learning! </p> <p>Let's evaluate the model. We are going to compute the accuracy and f1-score on the test set. The accuracy is the proportion of correctly classified bags, while the f1-score is the harmonic mean of precision and recall. The f1-score is a good metric for imbalanced datasets. Typically, in MIL datasets, there are more negative bags than positive bags.</p> <pre><code>from sklearn.metrics import accuracy_score, f1_score\n\ninst_pred_list = []\ny_inst_list = []\nY_pred_list = []\nY_list = []\n\nmodel.eval()\n\nfor batch in test_dataloader:\n    batch = batch.to(device)\n\n    # predict bag label using our model\n    out = model(batch['X'], batch['mask'])\n    Y_pred = (out &amp;gt; 0).float()\n\n    Y_pred_list.append(Y_pred)\n    Y_list.append(batch['Y'])\n\nY_pred = torch.cat(Y_pred_list).cpu().numpy()\nY = torch.cat(Y_list).cpu().numpy()\n\nprint(f\"test/bag/acc: {accuracy_score(Y_pred, Y)}\")\nprint(f\"test/bag/f1: {f1_score(Y_pred, Y)}\")\n</code></pre> <pre>\n<code>test/bag/acc: 0.8133333333333334\ntest/bag/f1: 0.7878787878787878\n</code>\n</pre> <p>Good! Our model is working well. The accuracy and f1-score are high. And we got this result in less than two minutes on GPU and very few lines of code thanks to torchmil!</p>"},{"location":"examples/ctscan_classification/#training-a-mil-model-for-ct-intracranial-hemorrage-detection","title":"Training a MIL model for CT Intracranial Hemorrage Detection","text":"<p>The intracraneal hemorrage (ICH) is a serious life-threatening emergency caused by blood-leackage inside the brain. The presence of ICH is confirmed by radiologists by using a Computed Tomography (CT) scan, which consists of a significant number of slices, each representing a section of the head at a given height.</p> <p>Training a model to detect ICH in CT scans is a challenging task, as it requires that a team of radiologists manually label each CT scan, indicating the presence of ICH in each slice. This is a time-consuming and expensive process, and it is not always feasible to obtain such detailed annotations for large datasets.</p> <p>An alternative approach is to use Multiple Instance Learning (MIL), which allows us to train a model using weak labels. In this case, we can use the presence of ICH in the CT scan as a weak label, without requiring detailed annotations for each slice. In the following, we explain how to train a simple Multiple Instance Learning (MIL) model to detect ICH using the torchmil library.</p> <p>ICH detection as a MIL problem</p> <p>We treat a CT scan as a bag of instances, where each instance is a slice of the CT scan. </p> <p>The labels of the slices are \\(\\mathbf{y} = \\left[ y_1, \\ldots, y_N \\right]^\\top \\in \\{0, 1\\}^N\\). A slice will be given a positive label (\\(y_i = 1\\)) if it contains ICH, and a negative label (\\(y_i = 0\\)) if it does not. The labels of the slices are not available at training time, as they are usually obtained by a team of radiologists who manually annotate the CT scans. </p> <p>In this case, we have access to the bag labels \\(Y \\in \\{0, 1\\}\\), which indicate whether the CT scan contains ICH or not. The relation between the instance labels and the bag label is as follows:</p> \\[ Y = \\max \\left\\{ y_1, \\ldots, y_N \\right\\} \\] <p>This means that if at least one slice in the CT scan has hemorrhage (i.e., \\(y_i = 1\\)), then the bag is labeled as positive (\\(Y = 1\\)). Otherwise, the bag is labeled as negative (\\(Y = 0\\)). This is a typical setting for MIL, where we have access to weak labels (the bag labels) but not to the instance labels.</p>"},{"location":"examples/ctscan_classification/#the-data","title":"The data","text":"<p>For this tutorial, we will use the RSNA dataset, which can be found in Kaggle. The dataset is composed of a set of CT scans, each containing a number of slices. Each slice is a 2D image, and the CT scan is a 3D volume. As part of torchmil, we have published a version of this dataset adapted for MIL, see the Huggingface repository.</p> <p>Let us first visualize the data using the <code>torchmil.visualize.vis_ctscan</code> module. In the following, we first load the slices of a bag and then use the function <code>slices_to_canvas</code> to create a canvas with all the images and we draw a small contour on each of the slices in the canvas using <code>draw_slices_contour</code>.</p>"},{"location":"examples/ctscan_classification/#mini-batching-of-bags","title":"Mini-batching of bags","text":"<p>Tipically, the bags in a MIL dataset have different size. This can be a problem when creating mini-batches. To solve this, we use the function <code>collate_fn</code> from the torchmil.data module. This function creates a mini-batch of bags by padding the bags with zeros to the size of the largest bag in the batch. The function also returns a mask tensor that indicates which instances are real and which are padding.</p> <p>Why not use <code>torch.nested</code>?</p> <p><code>torch.nested</code> offer a more flexible method for handling bags of varying sizes. However, since the PyTorch API for nested tensors is still in the prototype stage, torchmil currently relies on the padding approach.</p> <p>Let's create the dataloaders and visualize the shape of a mini-batch. Since the RSNA dataset does not have many instances per bag, we can use a <code>batch_size</code> of 64 for the train and validation sets.</p>"},{"location":"examples/ctscan_classification/#training-a-model-in-rsna","title":"Training a model in RSNA","text":"<p>We have shown how to load the RSNA dataset for the binary classification task. Now, let us train a MIL model in this dataset! For this example, we will use torchmil implementation of a TransformerABMIL, a version of ABMIL where a Transformer encoder is applied to refine the instances before the Attention Pool. To highlight how simple is to instance a model in torchmil, we will leave all the parameters by default except for the <code>in_shape</code>, which reflects the data shape. Feel free to check the documentation of Transformer ABMIL to observe the different parameters that this model can be passed.</p>"},{"location":"examples/ctscan_classification/#evaluating-the-model","title":"Evaluating the model","text":""},{"location":"examples/data_representation/","title":"Representing bags in torchmil","text":"<p>In the following, we explain how torchmil represents bags and how to use them in your code. </p> <p>This notebook contains:</p> <ul> <li>A brief introduction to bags in Multiple Instance Learning (MIL). </li> <li>How to represent bags in torchmil.</li> <li>A first look at the <code>ToyDataset</code> from the torchmil.datasets module.</li> <li>How mini-batching is handled in torchmil.</li> <li>Differences between the sequential and spatial representations of bags.</li> </ul> <pre><code>import torch\nfrom torchvision import datasets, transforms\n\n# Load MNIST dataset\nmnist = datasets.MNIST('/tmp/', train=True, download=True, transform=transforms.ToTensor())\n\n# Extract features and labels\ndata = mnist.data.view(-1, 28*28) / 255\nlabels = mnist.targets\n</code></pre> <p>Let's create a bag of 10 instances. The label of each instance will be the digit it represents, and the label of the bag will be the maximum digit in the bag.</p> <pre><code>from tensordict import TensorDict\n\n# Select 10 random indices\nindices = torch.randperm(data.size(0))[:10]\n\nbag = TensorDict({\n    'X': data[indices],\n    'y_inst': labels[indices],\n    'Y': labels[indices].max()\n})\nbag\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([10, 784]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)</code>\n</pre> <p>Now, let's create a MIL dataset using the <code>ToyDataset</code> class from torchmil.datasets.</p> <p>We will create a binary dataset, where the digits \\(4\\) and \\(5\\) are the positive instances (their label is \\(1\\)), and the rest are the negative instances (their label is \\(0\\)). Thus, the label of the bag is \\(1\\) if it contains at least one \\(4\\) or \\(5\\), and \\(0\\) otherwise.</p> <pre><code>from torchmil.datasets import ToyDataset\n\n# Define positive labels\nobj_labels = [4, 5] # Digits 4 and 5 are considered positive\n\n# Create MIL dataset\ntoy_dataset = ToyDataset(data, labels, num_bags=100, obj_labels=obj_labels, bag_size=10)\n\n# Retrieve a bag\nbag = toy_dataset[0]\nbag\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([10, 784]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)</code>\n</pre> <p>Let's visualize the bags</p> <pre><code>import matplotlib.pyplot as plt\n\ndef plot_bag(bag):\n    bag_size = len(bag['X'])\n    fig, axes = plt.subplots(1, bag_size, figsize=(bag_size, 1.8))\n    for i in range(bag_size):\n        ax = axes[i]\n        ax.imshow(bag['X'][i].view(28, 28), cmap='gray')\n        ax.set_title(f\"label: {bag['y_inst'][i].item()}\")        \n        ax.axis('off')\n    fig.suptitle(f'Bag label: {bag[\"Y\"].item()}')\n    plt.show()\n\nfor i in range(3):\n    bag = toy_dataset[i]\n    plot_bag(bag)\n</code></pre> <pre><code>from torchmil.datasets import ToyDataset\n\n# Define positive labels\nobj_labels = [4, 5] # Digits 4 and 5 are considered positive\n\n# Create MIL dataset\ntoy_dataset = ToyDataset(data, labels, num_bags=100, obj_labels=obj_labels, bag_size=(4, 10))\n</code></pre> <p>We retrieve four bags from the dataset and collate them into a batch. A batch is just a <code>TensorDict</code> object containing the padded tensors and the mask tensor.</p> <pre><code>from torchmil.data import collate_fn\n\nbag_list = [toy_dataset[i] for i in range(4)]\nbatch = collate_fn(bag_list)\nbatch\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([4, 9, 784]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False),\n        mask: Tensor(shape=torch.Size([4, 9]), device=cpu, dtype=torch.uint8, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([4, 9]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)</code>\n</pre> <p>Let's plot the bags in the batch and the mask tensor.</p> <pre><code>def plot_batch(batch):\n    batch_size = len(batch['X'])\n    bag_size = len(batch['X'][0])\n    fig, axes = plt.subplots(batch_size, bag_size, figsize=(bag_size, 1.5*batch_size))\n    for i in range(batch_size):\n        for j in range(bag_size):\n            ax = axes[i, j]\n            ax.imshow(batch['X'][i][j].view(28, 28), cmap='gray')\n            ax.set_title(f\"label: {batch['y_inst'][i][j].item()}\\nmask: {batch['mask'][i][j].item()}\")        \n            ax.axis('off')\n        fig.suptitle(f'Bag labels: {batch[\"Y\"].tolist()}')\n\nplot_batch(batch)\n</code></pre> <p>As we can see, the bags are padded to the maximum size of the bags in the batch with zeros. The mask tensor indicates which elements are real instances and which are padding. Additionally, the function <code>collate_fn</code> also pads other tensors, such as the adjacency matrix or the instance coordinates. </p> <pre><code># Select 10 random indices\nindices = torch.randperm(data.size(0))[:5]\n\nbag = TensorDict({\n    'X': data[indices],\n    'y_inst': labels[indices],\n    'Y': labels[indices].max()\n})\n\n# Create the canvas\nheight = 3*28\nwidth = 5*28\ncanvas = torch.zeros(height, width)\n\n# Randomly place the digits on the canvas\ntorch.manual_seed(5) # set seed\ncoords_list = []\nfor n in range(5):\n    i = torch.randint(0, height-28, (1,)).item()\n    j = torch.randint(0, width-28, (1,)).item()\n    canvas[i:i+28, j:j+28] = bag['X'][n].view(28, 28)\n    coords_list.append((i, j))\n\n# Convert to tensor\ncoords = torch.tensor(coords_list)\n\n# Display the canvas\nplt.imshow(canvas, cmap='gray')\nplt.title('Original canvas')\nplt.axis('off')\nplt.show()\n</code></pre> <p>Now, the digits in our bag have a spatial structure given by their coordinates. Let's compute the spatial representation of the bag using the coordinates.</p> <pre><code>from torchmil.data import seq_to_spatial, spatial_to_seq\nX = bag['X'].unsqueeze(0) # add batch dimension for seq_to_spatial and spatial_to_seq\ncoords = coords.unsqueeze(0) # add batch dimension for seq_to_spatial and spatial_to_seq\nX_esp = seq_to_spatial(X, coords) # remove batch dimension\nX_seq = spatial_to_seq(X_esp, coords) # remove batch dimension\n\n# Remove batch dimension\ncoords = coords.squeeze(0)\nX = X.squeeze(0)\nX_seq = X_seq.squeeze(0)\nX_esp = X_esp.squeeze(0)\n\nprint('X shape:', X.shape)\nprint('X_seq shape:', X_seq.shape)\nprint('X_esp shape:', X_esp.shape)\nprint('X and X_seq are equal:', torch.allclose(X, X_seq))\n</code></pre> <pre>\n<code>X shape: torch.Size([5, 784])\nX_seq shape: torch.Size([5, 784])\nX_esp shape: torch.Size([44, 111, 784])\nX and X_seq are equal: True\n</code>\n</pre> <p><code>X_esp</code> is the spatial representation of the bag. It is equivalent to the canvas with the digits placed in the corresponding coordinates. Let's reconstruct the canvas using it. </p> <pre><code>canvas_rec = torch.zeros(height, width)\nfor n in range(5):\n    i, j = coords[n]\n    canvas_rec[i:i+28, j:j+28] = X_esp[i,j,:].view(28, 28)\n\nplt.imshow(canvas_rec, cmap='gray')\nplt.title('Reconstructed canvas')\nplt.axis('off')\nplt.show()\n</code></pre> <p>Finally, the sequential representation, which is already stored in <code>bag</code>, can be coupled with the coordinates. Using these coordinates, we can compute an adjacency matrix. </p> <pre><code>coords = coords.squeeze(0).type(torch.float32) # convert to float32\nbag['coords'] = coords # add to bag\n\n# Create the adjacency matrix. Each entry (i, j) is given by an RBF kernel evaluated at coordinates i and j with a length scale of 28\nadj = torch.zeros(5, 5)\nfor i in range(5):\n    for j in range(5):\n        if i != j:\n            adj[i, j] = torch.exp(- torch.norm(coords[i] - coords[j]) / 28 )\nbag['adj'] = adj # add to bag\n\n# Plot the canvas with the adjacency matrix overlayed\nplt.imshow(canvas, cmap='gray')\nfor i in range(5):\n    for j in range(5):\n        plt.plot([coords[i, 1]+14, coords[j, 1]+14], [coords[i, 0]+14, coords[j, 0]+14], 'r', alpha=adj[i, j].item())\nplt.axis('off')\nplt.show()\n</code></pre> <p>The adjacency matrix indicates the distance between the digits in the bag. </p>"},{"location":"examples/data_representation/#representing-bags-in-torchmil","title":"Representing bags in torchmil","text":""},{"location":"examples/data_representation/#what-is-a-bag","title":"What is a bag?","text":"<p>In Multiple Instance Learning (MIL), a bag is a collection of instances. Usually, both instances and bags have labels. However, it is assumed that the labels of the instances in a bag are not available at training time. Instead, we only have access to:</p> <ul> <li>The label of the bag, </li> <li>Some kind of relation between the instance labels and the bag label.</li> </ul> <p>Additionally, a bag can have some structure, such as a graph representing the relationships between the instances in the bag, or the coordinates of the instances in some space. All these cases can be handled with torchmil.</p> <p>Example: MIL binary classification</p> <p>In this case, the bags have the form \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), where each \\(\\mathbf{x}_n \\in \\mathbb{R}^D\\) is an instance.  The labels of the instances are \\(\\mathbf{y} = \\left[ y_1, \\ldots, y_N \\right]^\\top \\in \\{0, 1\\}^N\\), but we do not have access to them at training time (they may be accessible at test time). The label of the bag is \\(Y \\in \\{0, 1\\}\\), and the relation between the instance labels and the bag label is as follows:</p> \\[ Y = \\max \\left\\{ y_1, \\ldots, y_N \\right\\} \\] <p>This example is the most common in MIL, but there are many other possibilities. </p>"},{"location":"examples/data_representation/#how-bags-are-represented-in-torchmil","title":"How bags are represented in torchmil?","text":"<p>In torchmil, bags are represented as a <code>TensorDict</code> object, which stores any kind of information about the bag. In most cases, a bag will contain at least the following keys:</p> <ul> <li><code>bag['X']</code>: a tensor of shape <code>(bag_size, ...)</code> containing the instances in the bag. Usually, this tensor is called bag feature matrix, since these instances are feature vectors extracted from the raw representation of the instances. Therefore, in most cases it has shape <code>(bag_size, feature_dim)</code>. </li> <li><code>bag['Y']</code>: a tensor containing the label of the bag. In the simplest case, this tensor is a scalar, but it can be a tensor of any shape (e.g., in multi-class MIL).</li> </ul> <p>Additionally, a bag may contain other keys. The most common ones in torchmil are:</p> <ul> <li><code>bag['y_inst']</code>: a tensor of shape <code>(bag_size, ...)</code> containing the labels of the instances in the bag. In the pure MIL setting, this tensor is only used for evaluation purposes since the label of the instances are not known. However, some methods may require some sort of supervision at the instance level.</li> <li><code>bag['adj']</code>: a tensor of shape <code>(bag_size, bag_size)</code> containing the adjacency matrix of the bag. This matrix is used to represent the relationships between the instances in the bag. The methods implemented in torchmil.models allow this matrix to be a sparse tensor.</li> <li><code>bag['coords']</code>: a tensor of shape <code>(bag_size, coords_dim)</code> containing the coordinates of the instances in the bag. This tensor is used to represent the absolute position of the instances in the bag.</li> </ul>"},{"location":"examples/data_representation/#example-mnist","title":"Example: MNIST","text":"<p>Creating a bag is as simple as creating a <code>TensorDict</code> object. Let's use the MNIST dataset to illustrate how bags are represented in torchmil.</p>"},{"location":"examples/data_representation/#mini-batches-in-torchmil-masks","title":"Mini-batches in torchmil: masks","text":"<p>Mini-batches enable the training of deep learning models with huge amounts of data. In torchmil, mini-batches are handled by the <code>collate_fn</code> function of torchmil.data, which is used to collate a list of bags into a batch.</p> <p>In MIL, each bag can be of different size. To solve this, in torchmil, the tensors in the bags are padded to the maximum size of the bags in the batch. A mask tensor is used to indicate which elements of the padded tensors are real instances and which are padding. This mask tensor is used to adjust the behavior of the models to ignore the padding elements (e.g., in the attention mechanism).</p> <p>Why not use <code>torch.nested</code>?</p> <p><code>torch.nested</code> offer a more flexible method for handling bags of varying sizes. However, since the PyTorch API for nested tensors is still in the prototype stage, torchmil currently relies on the padding approach.</p> <p>We illustrate this behaviour in the following example. We use again the MNIST dataset, but this time we create a dataset with bags of different sizes.</p>"},{"location":"examples/data_representation/#sequential-representation-vs-spatial-representation","title":"Sequential representation vs spatial representation","text":"<p>In torchmil, bags can be represented in two ways: sequential and spatial. </p> <p>In the sequential representation <code>bag['X']</code> is a tensor of shape <code>(bag_size, dim)</code>. This representation is the most common in MIL.  When the bag has some spatial structure, the sequential representation can be coupled with a graph using an adjacency matrix or with the coordinates of the instances. These are stored as <code>bag['adj']</code> (of shape <code>(bag_size, bag_size)</code>) and <code>bag['coords']</code> (of shape <code>(bag_size, coords_dim)</code>), respectively.</p> <p>Alternatively, the spatial representation can be used. In this case, <code>bag['X']</code> is a tensor of shape <code>(coord1, ..., coordN, dim)</code>, where <code>N=coords_dim</code> is the number of dimensions of the space.</p> <p>In torchmil, you can convert from one representation to the other using the functions <code>torchmil.utils.seq_to_spatial</code> and <code>torchmil.utils.spatial_to_seq</code> from the torchmil.data module. These functions need the coordinates of the instances in the bag, stored as <code>bag['coords']</code>.</p> <p>Example: Whole Slide Images</p> <p>Due to their large resolution, Whole Slide Images (WSIs) are usually represented as bags of patches. Each patch is an image, from which a feature vector of is typically extracted. The spatial representation of a WSI has shape <code>(height, width, feat_dim)</code>, while the sequential representation has shape <code>(bag_size, feat_dim)</code>. The coordinates corresponds to the coordinates of the patches in the WSI. </p> <p>SETMIL is an example of a model that uses the spatial representation of a WSI. </p> <p>Let's illustrate this with an example. Again, using MNIST, we will create a bag of 10 instances, and randomly place them in a canvas.</p>"},{"location":"examples/datasets/","title":"Datasets in torchmil","text":""},{"location":"examples/datasets/#datasets","title":"Datasets","text":"<p>Coming soon...</p>"},{"location":"examples/training_your_first_mil_model/","title":"Training your first MIL model","text":"<pre><code>import torch\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n</code></pre> <pre><code>from torchvision import datasets, transforms\nfrom torchmil.datasets import ToyDataset\n\n# Load MNIST dataset\nmnist = datasets.MNIST('/tmp/', train=True, download=True, transform=transforms.ToTensor())\n\n# Extract features and labels\ndata = mnist.data.view(-1, 28*28) / 255.0\nlabels = mnist.targets\n\n# Define positive labels\nobj_labels = [0, 1] \n\n# Define bag size\nbag_size = (7, 12) # Randomly sample between 8 and 12 instances per bag\n\n# Create MIL dataset\ntrain_dataset = ToyDataset(data, labels, num_bags=10000, obj_labels=obj_labels, bag_size=bag_size, seed=0)\ntest_dataset = ToyDataset(data, labels, num_bags=2000, obj_labels=obj_labels, bag_size=bag_size, seed=2)\n\n# Print one bag\nprint(train_dataset[0])\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([11, 784]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)\n</code>\n</pre> <p>In torchmil, each bag is a <code>TensorDict</code>. The different keys correspond to different elements of the bag. In this case, each bag has a feature matrix <code>X</code>, the bag label <code>Y</code>, and the instance labels <code>y_inst</code>. Recall that the instance labels cannot be used during training, they are available only for evaluation purposes.</p> <pre><code>from torchmil.data import collate_fn\n\n# Create dataloaders\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nfrom tensordict import TensorDict\n\ndef plot_batch(batch, max_bags=5):\n    batch_size = min(len(batch['X']), max_bags)\n    bag_size = len(batch['X'][0])\n    fig, axes = plt.subplots(batch_size, bag_size, figsize=(bag_size, 1.5*batch_size))\n    for i in range(batch_size):\n        for j in range(bag_size):\n            ax = axes[i, j]\n            ax.imshow(batch['X'][i][j].view(28, 28), cmap='gray')\n            title_str = f\"label: {batch['y_inst'][i][j].item()}\\nmask: {batch['mask'][i][j].item()}\"\n            ax.set_title(title_str)        \n            ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nbatch = next(iter(train_dataloader))\nprint(batch)\nplot_batch(batch, max_bags=4)\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([32, 11, 784]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([32]), device=cpu, dtype=torch.int64, is_shared=False),\n        mask: Tensor(shape=torch.Size([32, 11]), device=cpu, dtype=torch.uint8, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([32, 11]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)\n</code>\n</pre> <p>Each batch is again a <code>TensorDict</code> with an additional key <code>mask</code> that indicates which instances are real and which are padding. As we can see, the bags are padded to the maximum size of the bags in the batch with zeros. The mask tensor indicates which elements are real instances and which are padding. The function <code>collate_fn</code> also pads other tensors, such as the adjacency matrix or the instance coordinates. </p> <pre><code>from torchmil.nn import masked_softmax\n\nclass ABMIL(torch.nn.Module):\n    def __init__(self, emb_dim, att_dim):\n        super().__init__()\n\n        # Feature extractor\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(28*28, 512),\n            torch.nn.ReLU(),\n            torch.nn.Linear(512, emb_dim),\n        )\n\n        self.fc1 = torch.nn.Linear(emb_dim, att_dim)\n        self.fc2 = torch.nn.Linear(att_dim, 1)\n\n        self.classifier = torch.nn.Linear(emb_dim, 1)       \n\n\n    def forward(self, X, mask, return_att=False):\n        X = self.mlp(X) # (batch_size, bag_size, emb_dim)\n        H = torch.tanh(self.fc1(X)) # (batch_size, bag_size, att_dim)\n        att = torch.sigmoid(self.fc2(H)) # (batch_size, bag_size, 1)\n        att_s = masked_softmax(att, mask) # (batch_size, bag_size, 1)\n        # att_s = torch.nn.functional.softmax(att, dim=1)\n        X = torch.bmm(att_s.transpose(1, 2), X).squeeze(1) # (batch_size, emb_dim)\n        y = self.classifier(X).squeeze(1) # (batch_size,)\n        if return_att:\n            return y, att_s\n        else:\n            return y\n\nmodel = ABMIL(emb_dim=256, att_dim=128)\nprint(model)\n</code></pre> <pre>\n<code>ABMIL(\n  (mlp): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=256, bias=True)\n  )\n  (fc1): Linear(in_features=256, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=1, bias=True)\n  (classifier): Linear(in_features=256, out_features=1, bias=True)\n)\n</code>\n</pre> <p>Great! Now, let's train the model. We are going to use the <code>torch.optim.Adam</code> optimizer and the <code>torch.nn.BCEWithLogitsLoss</code> loss function. We will train the model for 20 epochs.</p> <p>Thanks to the torchmil library, training a MIL model is as simple as training a standard PyTorch model. The training loop is straightforward, similar to the standard PyTorch training loop. We iterate over the training dataloader, compute the loss, and update the model parameters. We also track the loss and accuracy.</p> <pre><code>model = ABMIL(emb_dim=256, att_dim=128)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n\ndef train(dataloader, epoch):\n    model.train()\n\n    sum_loss = 0.0\n    sum_correct = 0.0\n    for batch in dataloader:\n        batch = batch.to(device)\n        out = model(batch['X'], batch['mask'])\n        loss = criterion(out, batch['Y'].float())\n        loss.backward()        \n        optimizer.step()\n        optimizer.zero_grad()\n\n        sum_loss += loss.item()\n        pred = (out &amp;gt; 0).float()\n        sum_correct += (pred == batch['Y']).sum().item()\n        sum_loss += loss.item()\n\n    print(f\"[Epoch {epoch}] Train, train/loss: {sum_loss / len(dataloader)}, 'train/bag/acc': {sum_correct / len(dataloader.dataset)}\")\n\ndef val(dataloader, epoch):\n    model.eval()\n\n    sum_loss = 0.0\n    sum_correct = 0.0\n    for batch in dataloader:\n        batch = batch.to(device)\n        out = model(batch['X'], batch['mask'])\n        loss = criterion(out, batch['Y'].float())\n\n        sum_loss += loss.item()\n        pred = (out &amp;gt; 0).float()\n        sum_correct += (pred == batch['Y']).sum().item()\n        sum_loss += loss.item()\n\n    print(f\"[Epoch {epoch}] Validation, val/loss: {sum_loss / len(dataloader)}, 'val/bag/acc': {sum_correct / len(dataloader.dataset)}\")\n\nmodel = model.to(device)\nfor epoch in range(20):\n    train(train_dataloader, epoch+1)\n    val(test_dataloader, epoch+1)\n</code></pre> <pre>\n<code>[Epoch 1] Train, train/loss: 0.3859690427304076, 'train/bag/acc': 0.9286\n[Epoch 1] Validation, val/loss: 0.1880752561939141, 'val/bag/acc': 0.9695\n[Epoch 2] Train, train/loss: 0.16442154182311589, 'train/bag/acc': 0.9739\n[Epoch 2] Validation, val/loss: 0.12515571917451562, 'val/bag/acc': 0.9845\n[Epoch 3] Train, train/loss: 0.14584087032063034, 'train/bag/acc': 0.9771\n[Epoch 3] Validation, val/loss: 0.1147860995538178, 'val/bag/acc': 0.9825\n[Epoch 4] Train, train/loss: 0.13143371639493556, 'train/bag/acc': 0.9804\n[Epoch 4] Validation, val/loss: 0.11624959915403336, 'val/bag/acc': 0.985\n[Epoch 5] Train, train/loss: 0.12332545817731477, 'train/bag/acc': 0.9827\n[Epoch 5] Validation, val/loss: 0.10535499797246996, 'val/bag/acc': 0.985\n[Epoch 6] Train, train/loss: 0.12535623266263463, 'train/bag/acc': 0.9815\n[Epoch 6] Validation, val/loss: 0.10129267238967475, 'val/bag/acc': 0.987\n[Epoch 7] Train, train/loss: 0.11201968839534888, 'train/bag/acc': 0.984\n[Epoch 7] Validation, val/loss: 0.11095998269165792, 'val/bag/acc': 0.9835\n[Epoch 8] Train, train/loss: 0.10963183950213269, 'train/bag/acc': 0.9831\n[Epoch 8] Validation, val/loss: 0.11002697482971209, 'val/bag/acc': 0.983\n[Epoch 9] Train, train/loss: 0.11524660762000721, 'train/bag/acc': 0.9836\n[Epoch 9] Validation, val/loss: 0.13182692171355326, 'val/bag/acc': 0.9785\n[Epoch 10] Train, train/loss: 0.10611518676773594, 'train/bag/acc': 0.9836\n[Epoch 10] Validation, val/loss: 0.12102683454692836, 'val/bag/acc': 0.981\n[Epoch 11] Train, train/loss: 0.09992601268793852, 'train/bag/acc': 0.9857\n[Epoch 11] Validation, val/loss: 0.09442376957408019, 'val/bag/acc': 0.986\n[Epoch 12] Train, train/loss: 0.09881719468331066, 'train/bag/acc': 0.9853\n[Epoch 12] Validation, val/loss: 0.11326536555064931, 'val/bag/acc': 0.983\n[Epoch 13] Train, train/loss: 0.09219979361360209, 'train/bag/acc': 0.9859\n[Epoch 13] Validation, val/loss: 0.10536213541432979, 'val/bag/acc': 0.984\n[Epoch 14] Train, train/loss: 0.0933219926556745, 'train/bag/acc': 0.9872\n[Epoch 14] Validation, val/loss: 0.12082792839242353, 'val/bag/acc': 0.9805\n[Epoch 15] Train, train/loss: 0.08617063817350915, 'train/bag/acc': 0.9869\n[Epoch 15] Validation, val/loss: 0.10966313540166805, 'val/bag/acc': 0.984\n[Epoch 16] Train, train/loss: 0.08572717891714443, 'train/bag/acc': 0.9888\n[Epoch 16] Validation, val/loss: 0.0979384019497841, 'val/bag/acc': 0.9855\n[Epoch 17] Train, train/loss: 0.08640271301170031, 'train/bag/acc': 0.9869\n[Epoch 17] Validation, val/loss: 0.09659811220915308, 'val/bag/acc': 0.987\n[Epoch 18] Train, train/loss: 0.08080236079876486, 'train/bag/acc': 0.9875\n[Epoch 18] Validation, val/loss: 0.09953816693335299, 'val/bag/acc': 0.986\n[Epoch 19] Train, train/loss: 0.07695601370182638, 'train/bag/acc': 0.9889\n[Epoch 19] Validation, val/loss: 0.11378709812988601, 'val/bag/acc': 0.983\n[Epoch 20] Train, train/loss: 0.07340383955973763, 'train/bag/acc': 0.9883\n[Epoch 20] Validation, val/loss: 0.10783347497058529, 'val/bag/acc': 0.9865\n</code>\n</pre> <p>The loss decreases as the model learns to predict the bag labels. The accuracy increases as the model learns to predict the correct bag labels. This is a good sign that the model is learning!</p> <p>Let's evaluate the model. We are going to compute the accuracy and f1-score on the test set. The accuracy is the proportion of correctly classified bags, while the f1-score is the harmonic mean of precision and recall. The f1-score is a good metric for imbalanced datasets. Typically, in MIL datasets, there are many more negative instances than positive instances. In this case, the f1-score will be very useful.</p> <p>To compute predictions at the instance level, we are going to use the attention values. They indicate the importance given by the model to each instance in the bag. As explained in the ABMIL paper, in positive bags, the model should give more importance to the positive instances, so the attention values should be higher for positive instances. </p> <p>Instance-level predictions have certain caveats due to padding. When performing operations such as normalizing across the bag, it\u2019s crucial to handle padded instances carefully, as they can affect predictions. Additionally, padded instances should be excluded when computing metrics to ensure accuracy.</p> <p>First, we define some auxiliary functions. Take a look at how we handle padded instances in <code>normalize</code>.</p> <pre><code>def accuracy(pred, y):\n    return (pred == y).sum().item() / len(y)\n\ndef f1_score(pred, y):\n    tp = ((pred == 1) &amp;amp; (y == 1)).sum().item()\n    fp = ((pred == 1) &amp;amp; (y == 0)).sum().item()\n    fn = ((pred == 0) &amp;amp; (y == 1)).sum().item()\n    precision = tp / (tp + fp + 1e-6)\n    recall = tp / (tp + fn + 1e-6)\n    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n    return f1\n\ndef normalize(att, mask = None):\n    if mask is None:\n        mask = torch.ones_like(att)\n    else:\n        mask = mask.unsqueeze(-1)\n\n    # exclude masked values from computing min and max\n    att_tmp = att.clone()\n    att_tmp[mask == 0] = float('inf')\n    att_min = att_tmp.min(dim=1, keepdim=True).values\n    att_tmp[mask == 0] = float('-inf')\n    att_max = att_tmp.max(dim=1, keepdim=True).values\n    att_min = att_min.expand_as(att)\n    att_max = att_max.expand_as(att)\n\n    # padded instances are set to min value, so that after normalization are set to 0 \n    att = torch.where(mask == 0, att_min, att)\n\n    # normalize\n    att_norm = (att - att_min) / (att_max - att_min + 1e-10)\n    return att_norm\n</code></pre> <p>Now, we compute the predictions at both the bag and instance level. Take a look at how we handle the padded instances!</p> <pre><code>inst_pred_list = []\ny_inst_list = []\nY_pred_list = []\nY_list = []\n\nfor batch in test_dataloader:\n    batch = batch.to(device)\n\n    # predict bag label and attention\n    out, att = model(batch['X'], batch['mask'], return_att=True)\n    Y_pred = (out &amp;gt; 0).float()\n\n    # normalize attention\n    att_norm = normalize(att, batch['mask'])\n\n    # remove attention corresponding to padded instances\n    att_norm = att_norm.view(-1)[batch['mask'].view(-1) == 1]\n    inst_pred = (att_norm &amp;gt; 0.5).float()\n\n    # remove labels corresponding to padded instances\n    y_inst = batch['y_inst'].view(-1)[batch['mask'].view(-1) == 1]\n\n    inst_pred_list.append(inst_pred)\n    y_inst_list.append(y_inst)\n    Y_pred_list.append(Y_pred)\n    Y_list.append(batch['Y'])\n\n\ninst_pred = torch.cat(inst_pred_list)\ny_inst = torch.cat(y_inst_list)\nY_pred = torch.cat(Y_pred_list)\nY = torch.cat(Y_list)\n\nprint(f\"test/bag/acc: {accuracy(Y_pred, Y)}\")\nprint(f\"test/bag/f1: {f1_score(Y_pred, Y)}\")\nprint(f\"test/inst/acc: {accuracy(inst_pred, y_inst)}\")\nprint(f\"test/inst/f1: {f1_score(inst_pred, y_inst)}\")\n</code></pre> <pre>\n<code>test/bag/acc: 0.9865\ntest/bag/f1: 0.9865062456410512\ntest/inst/acc: 0.9869223096531087\ntest/inst/f1: 0.9446524078781653\n</code>\n</pre> <p>Good! Our model is working well. The accuracy and f1-score are high. This means that the attention values are correctly indicating the importance of the instances in the bag. Let's visualize the attention values for some bags.</p> <p>Finally, we are going to take a look at the attention maps, which show the importance of each instance in the bag. The attention maps are computed by normalizing the attention values across the bag. Also, we highlight the instances predicted as positive in the positive bags. </p> <pre><code>import numpy as np\n\ndef plot_batch_att(batch, att, max_bags=5):\n    batch_size = min(len(batch['X']), max_bags)\n    bag_size = len(batch['X'][0])\n    fig, axes = plt.subplots(batch_size, bag_size, figsize=(bag_size, 1.5*batch_size))\n    for i in range(batch_size):\n        for j in range(bag_size):\n            ax = axes[i, j]\n            ax.imshow(batch['X'][i][j].view(28, 28), cmap='gray')\n            # if the bag is positive, overlay red mask on the instance predicted as positive\n            if batch['Y'][i] == 1:\n                att_bag = normalize(att[i].unsqueeze(0), batch['mask'][i].unsqueeze(0)).squeeze(0)\n                pred = (att_bag &amp;gt; 0.5).float()\n                overlay_red = np.full((28, 28, 4), [1., 0., 0., 0.4*pred[j].item()])\n                ax.imshow(overlay_red)\n            title_str = f\"label: {batch['y_inst'][i][j].item()}\\natt: {att[i][j].item():.2}\"\n            ax.set_title(title_str)\n            ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nbatch = next(iter(test_dataloader))\nbatch = batch.to(device)\n_, att = model(batch['X'], batch['mask'], return_att=True)\nbatch = batch.detach().cpu()\natt = att.detach().cpu()\nplot_batch_att(batch, att, max_bags=10)\n</code></pre> <p>As we can see, the model assigns more importance to the positive instances. This is a good sign! This means that the model itself learned to predict the positive instances correctly, even though it was not trained with instance-level labels. This is the power of Multiple Instance Learning!</p>"},{"location":"examples/training_your_first_mil_model/#training-your-first-mil-model","title":"Training your first MIL model","text":"<p>In the following, we explain how to train a simple Multiple Instance Learning (MIL) model using the torchmil library.</p>"},{"location":"examples/training_your_first_mil_model/#the-dataset","title":"The dataset","text":"<p>MIL binary classification</p> <p>In this case, the bags have the form \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), where each \\(\\mathbf{x}_n \\in \\mathbb{R}^D\\) is an instance.  The labels of the instances are \\(\\mathbf{y} = \\left[ y_1, \\ldots, y_N \\right]^\\top \\in \\{0, 1\\}^N\\), but we do not have access to them at training time (they may be accessible at test time). The label of the bag is \\(Y \\in \\{0, 1\\}\\), and the relation between the instance labels and the bag label is as follows:</p> \\[ Y = \\max \\left\\{ y_1, \\ldots, y_N \\right\\} \\] <p>This example is the most common in MIL, but there are many other possibilities. </p> <p>We will create a synthetic MIL dataset from the MNIST dataset using the <code>ToyDataset</code> from the torchmil.datasets module. The <code>ToyDataset</code> generates an MIL dataset from a labeled dataset by specifying the desired bag size and the objective labels, which determine positive instances. Subsequently, <code>ToyDataset</code> constructs bags of the specified size and labels them based on these objective labels. For further details, refer to here for more details.</p> <p>In this example, we are going to set <code>obj_labels=[2, 5, 8]</code>. This means that a bag is positive if it contains at least one instance with label 5 or 8.</p>"},{"location":"examples/training_your_first_mil_model/#mini-batching-of-bags","title":"Mini-batching of bags","text":"<p>Tipically, the bags in a MIL dataset have different size. This can be a problem when creating mini-batches. To solve this, we use the function <code>collate_fn</code> from the torchmil.data module. This function creates a mini-batch of bags by padding the bags with zeros to the size of the largest bag in the batch. The function also returns a mask tensor that indicates which instances are real and which are padding.</p> <p>Why not use <code>torch.nested</code>?</p> <p><code>torch.nested</code> offer a more flexible method for handling bags of varying sizes. However, since the PyTorch API for nested tensors is still in the prototype stage, torchmil currently relies on the padding approach.</p> <p>Let's create the dataloaders and visualize some bags from a mini-batch.</p>"},{"location":"examples/training_your_first_mil_model/#the-model","title":"The model","text":"<p>In this example, we are going to use the ABMIL model.  Although it is readily available in the torchmil.models module, we are going to implement a simple variation from scratch to show how to create a custom MIL model.</p>"},{"location":"examples/training_your_first_mil_model/#training-the-model","title":"Training the model","text":""},{"location":"examples/training_your_first_mil_model/#evaluating-the-model","title":"Evaluating the model","text":""},{"location":"examples/training_your_first_mil_model/#inspecting-the-predictions","title":"Inspecting the predictions","text":""},{"location":"examples/wsi_classification/","title":"WSI classification","text":"<pre><code>import tifffile\nimport zarr\nimport cv2\nimport numpy as np\n\ndef read_wsi_patches(wsi_path, coords_path, size=512, resize_size=10):\n\n    store = tifffile.imread(wsi_path, aszarr=True)\n    wsi = zarr.open(store, mode='r')[0]\n\n    inst_coords = np.load(coords_path, allow_pickle=True)\n    bag_len = len(inst_coords)\n    patches_list = []\n    row_list = []\n    column_list = []\n    for i in range(bag_len):\n        coord = inst_coords[i]\n        x, y = coord      \n\n        patch = wsi[y:y+size, x:x+size]\n        patch = cv2.resize(patch, (resize_size, resize_size))\n        patches_list.append(patch)\n\n        row = int(y / size)\n        column = int(x / size)\n        row_list.append(row)\n        column_list.append(column)\n\n    row_array = np.array(row_list)\n    column_array = np.array(column_list)\n\n    row_array = row_array - row_array.min()\n    column_array = column_array - column_array.min()\n\n    return patches_list, row_array, column_array\n</code></pre> <p>Let's load the patches from a WSI using this function:</p> <pre><code>import os\n\nimg_dir = \"/data/datasets/CAMELYON16/original/images/\"\ncoord_dir = \"/data/datasets/CAMELYON16/patches_512_preset/coords/\"\n\nwsi_name = \"test_016\"\n\nwsi_path = os.path.join(img_dir, wsi_name + \".tif\")\ncoords_path = os.path.join(coord_dir, wsi_name + \".npy\")\n\npatches_list, row_array, column_array = read_wsi_patches(wsi_path, coords_path, size=512, resize_size=10)\n</code></pre> <p>Now that we have the patches, we can visualize the WSI using the <code>patches_to_canvas</code> from the <code>torchmil.visualize</code> module:</p> <pre><code>from torchmil.visualize import patches_to_canvas\n\ncanvas = patches_to_canvas(patches_list, row_array, column_array, 10)\nimport matplotlib.pyplot as plt\n\nplt.imshow(canvas)\nplt.axis('off')\nplt.show()\n</code></pre> <pre><code>import torch\nfrom torchmil.datasets import CAMELYON16MILDataset\nfrom sklearn.model_selection import train_test_split\n\nroot = '/home/fran/data/datasets/CAMELYON16/'\nfeatures = 'UNI'\npatch_size = 512\n\ndataset = CAMELYON16MILDataset(\n    root=root,\n    features=features,\n    patch_size=patch_size,\n    partition='train',\n    load_at_init=True\n)\n\n# Split the dataset into train and validation sets\nbag_labels = dataset.get_bag_labels()\nidx = list(range(len(bag_labels)))\nval_prop = 0.2\nidx_train, idx_val = train_test_split(\n    idx, test_size=val_prop, random_state=1234, stratify=bag_labels)\ntrain_dataset = dataset.subset(idx_train)\nval_dataset = dataset.subset(idx_val)\n\n# Load the test dataset\ntest_dataset = CAMELYON16MILDataset(\n    root=root,\n    features=features,\n    patch_size=patch_size,\n    partition='test',\n    load_at_init=True\n)\n\n# Print one bag\nbag = train_dataset[0]\nprint(\"Bag type:\", type(bag))\nfor key in bag.keys():\n    print(key, bag[key].shape)     \n</code></pre> <pre>\n<code>Bag type: &lt;class 'tensordict._td.TensorDict'&gt;\nX torch.Size([8555, 1024])\nY torch.Size([])\ny_inst torch.Size([8555])\ncoords torch.Size([8555, 2])\nadj torch.Size([8555, 8555])\n</code>\n</pre> <p>As you can see, in torchmil each bag is a <code>TensorDict</code>, and the different keys correspond to different elements of the bag. In this case, each bag has a feature matrix <code>X</code>, the bag label <code>Y</code>, the instance labels <code>y_inst</code>, the coordinates of the patches <code>coords</code> and an adjacency matrix <code>adj</code>. The adjacency matrix is used to define the spatial relationships between instances in the bag. Also, a patch is considered positive if at least half of its pixels are cancerous. Recall that the instance labels cannot be used during training, they are available only for evaluation purposes.</p> <pre><code>from torchmil.data import collate_fn\n\nbatch_size = 1\n\n# Create dataloaders\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n\nit = iter(train_dataloader)\nbatch = next(it)\nprint(\"Batch type:\", type(batch))\nfor key in batch.keys():\n    print(key, batch[key].shape)     \n</code></pre> <pre>\n<code>Batch type: &lt;class 'tensordict._td.TensorDict'&gt;\nX torch.Size([1, 1450, 1024])\nY torch.Size([1])\ny_inst torch.Size([1, 1450])\ncoords torch.Size([1, 1450, 2])\nadj torch.Size([1, 1450, 1450])\nmask torch.Size([1, 1450])\n</code>\n</pre> <p>Each batch is again a <code>TensorDict</code> with an additional key <code>mask</code> that indicates which instances are real and which are padding. As we can see, the bags are padded to the maximum size of the bags in the batch with zeros. The mask tensor indicates which elements are real instances and which are padding. The function <code>collate_fn</code> also pads other tensors, such as the adjacency matrix or the instance coordinates. </p> <pre><code>from torchmil.models import SmABMIL\ndata_shape = (1024,)\nmodel = SmABMIL(in_shape = data_shape)\n</code></pre> <p>See? It can not be easier! Now, let's train the model. torchmil offers an easy-to-use trainer class located in <code>torchmil.utils.trainer.Trainer</code> that provides a generic training for any MIL model. Also, it will show the evolution of the losses and the desired metrics during the epochs.</p> <p>Note</p> <p>This <code>Trainer</code> gives the flexibility to log the results using any wrapped <code>logger</code>, use annealing for the loss functions via the <code>annealing_scheduler_dict</code> dictionary, or to set a learning rate scheduler using the parameter <code>lr_scheduler</code>. Also, you can follow multiple metrics during the training thanks to the parameter <code>metrics_dict</code> and the integration with the torchmetrics package.</p> <p>For now, let us just keep it simple and perform a simple training using the <code>torch.optim.Adam</code> optimizer. When using the features from the UNI model, torchmil models obtain very good results in just a few epochs. We will train the model for only 10 epochs. First, we instance the trainer and then we train the model.</p> <pre><code>from torchmil.utils.trainer import Trainer\nimport torchmetrics\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    metrics_dict={\n        'acc': torchmetrics.Accuracy(\n            task='binary').to(device)\n    },\n    obj_metric='acc',\n    device=device,\n    disable_pbar=False,\n    verbose=False\n)\n</code></pre> <pre><code>EPOCHS = 10\ntrainer.train(\n    max_epochs=EPOCHS,\n    train_dataloader=train_dataloader,\n    val_dataloader=val_dataloader\n)\n</code></pre> <pre>\n<code>[Epoch 1] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 216/216 [00:05&lt;00:00, 41.54it/s, train/loss=0.677, train/BCEWithLogitsLoss=0.677, train/acc=0.588]\n[Epoch 1] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:00&lt;00:00, 62.29it/s, val/loss=0.642, val/BCEWithLogitsLoss=0.642, val/acc=0.593]\n[Epoch 2] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 216/216 [00:05&lt;00:00, 41.03it/s, train/loss=0.545, train/BCEWithLogitsLoss=0.545, train/acc=0.713]\n[Epoch 2] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:00&lt;00:00, 58.76it/s, val/loss=0.475, val/BCEWithLogitsLoss=0.475, val/acc=0.741]\n[Epoch 3] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 216/216 [00:05&lt;00:00, 40.86it/s, train/loss=0.309, train/BCEWithLogitsLoss=0.309, train/acc=0.917]\n[Epoch 3] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:00&lt;00:00, 60.27it/s, val/loss=0.199, val/BCEWithLogitsLoss=0.199, val/acc=0.963]\n[Epoch 4] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 216/216 [00:05&lt;00:00, 41.39it/s, train/loss=0.156, train/BCEWithLogitsLoss=0.156, train/acc=0.958]\n[Epoch 4] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:00&lt;00:00, 60.81it/s, val/loss=0.106, val/BCEWithLogitsLoss=0.106, val/acc=1]\n[Epoch 5] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 216/216 [00:05&lt;00:00, 37.42it/s, train/loss=0.0833, train/BCEWithLogitsLoss=0.0833, train/acc=0.995]\n[Epoch 5] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:00&lt;00:00, 60.59it/s, val/loss=0.0758, val/BCEWithLogitsLoss=0.0758, val/acc=1]\n[Epoch 6] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 216/216 [00:05&lt;00:00, 38.96it/s, train/loss=0.0535, train/BCEWithLogitsLoss=0.0535, train/acc=0.995]\n[Epoch 6] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:00&lt;00:00, 61.92it/s, val/loss=0.052, val/BCEWithLogitsLoss=0.052, val/acc=1]\n[Epoch 7] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 216/216 [00:05&lt;00:00, 42.25it/s, train/loss=0.032, train/BCEWithLogitsLoss=0.032, train/acc=1]\n[Epoch 7] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:00&lt;00:00, 63.13it/s, val/loss=0.0423, val/BCEWithLogitsLoss=0.0423, val/acc=1]\n[Epoch 8] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 216/216 [00:05&lt;00:00, 41.72it/s, train/loss=0.023, train/BCEWithLogitsLoss=0.023, train/acc=1]\n[Epoch 8] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:01&lt;00:00, 42.45it/s, val/loss=0.036, val/BCEWithLogitsLoss=0.036, val/acc=1]\n[Epoch 9] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 216/216 [00:05&lt;00:00, 42.46it/s, train/loss=0.0174, train/BCEWithLogitsLoss=0.0174, train/acc=1]\n[Epoch 9] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:00&lt;00:00, 59.15it/s, val/loss=0.0319, val/BCEWithLogitsLoss=0.0319, val/acc=1]\n[Epoch 10] Train : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 216/216 [00:05&lt;00:00, 42.37it/s, train/loss=0.0136, train/BCEWithLogitsLoss=0.0136, train/acc=1]\n[Epoch 10] Validation : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:00&lt;00:00, 63.78it/s, val/loss=0.0289, val/BCEWithLogitsLoss=0.0289, val/acc=1]\n</code>\n</pre> <p>Let's evaluate the model. We are going to compute the accuracy and f1-score on the test set. The accuracy is the proportion of correctly classified bags, while the f1-score is the harmonic mean of precision and recall. The f1-score is a good metric for imbalanced datasets. Typically, in MIL datasets, there are more negative bags than positive bags.</p> <pre><code>from sklearn.metrics import accuracy_score, f1_score\n\ninst_pred_list = []\ny_inst_list = []\nY_pred_list = []\nY_list = []\n\nmodel = model.to(device)\nmodel.eval()\n\nfor batch in test_dataloader:\n    batch = batch.to(device)\n\n    X = batch['X'].to(device)\n    mask = batch['mask'].to(device)\n    adj = batch['adj'].to(device)\n    Y = batch['Y']\n\n    # predict bag label using our model\n    out = model(X, adj, mask)\n    Y_pred = (out &amp;gt; 0).float()\n\n    Y_pred_list.append(Y_pred)\n    Y_list.append(Y)\n\nY_pred = torch.cat(Y_pred_list).cpu().numpy()\nY = torch.cat(Y_list).cpu().numpy()\n\nprint(f\"test/bag/acc: {accuracy_score(Y_pred, Y)}\")\nprint(f\"test/bag/f1: {f1_score(Y_pred, Y)}\")\n</code></pre> <pre>\n<code>test/bag/acc: 0.9457364341085271\ntest/bag/f1: 0.9247311827956989\n</code>\n</pre> <p>Excellent! Our model has reached a very high accuracy and f1-score in only 10 epochs! This shows how simple is to obtain very good results in one of the most famous WSI classification datasets, such as CAMELYON16, thanks to torchmil!</p> <pre><code>wsi_name = \"test_016\"\n\nbag = test_dataset._build_bag(wsi_name)\nX = bag['X'].unsqueeze(0).to(device)\nadj = bag['adj'].unsqueeze(0).to(device)\n\nY_pred, y_inst_pred = model.predict(X, adj, return_inst_pred=True)\n\ny_inst_pred = y_inst_pred.squeeze(0).detach().cpu().numpy()\ny_inst = bag['y_inst'].cpu().numpy()\n</code></pre> <p>Finally, we can visualize these predictions on the WSI using the function <code>draw_heatmap_wsi</code> from the <code>torchmil.visualize</code> module. This function takes the WSI, the instance scores, and the coordinates of the patches to draw a heatmap on the WSI. The heatmap shows the predicted scores for each instance, with red indicating high scores and green indicating low scores. You can also custimize these colors. </p> <pre><code>from torchmil.visualize import draw_heatmap_wsi\n\n# normalize the predictions\ny_inst_pred = (y_inst_pred - y_inst_pred.min()) / (y_inst_pred.max() - y_inst_pred.min())\n\ncanvas_labels = draw_heatmap_wsi(\n    canvas,\n    y_inst,\n    10, \n    row_array,\n    column_array,\n)\n\ncanvas_pred = draw_heatmap_wsi(\n    canvas,\n    y_inst_pred,\n    10, \n    row_array,\n    column_array\n)\n\n# plot canvas side by side\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(canvas_labels)\nax[0].axis('off')\nax[0].set_title('Ground Truth')\nax[1].imshow(canvas_pred)\nax[1].axis('off')\nax[1].set_title('Predictions')\nplt.show()\n</code></pre> <p>The heatmap shows the predicted scores for each instance, with red indicating high scores and green indicating low scores. Our model is able to detect the metastases in the WSI! Note that the model has learned to detect these regions without any instance-level labels, only using the bag-level labels. This is a clear example of the power of MIL in WSI classification tasks.</p>"},{"location":"examples/wsi_classification/#training-a-mil-model-for-cancer-detection-in-whole-slide-images","title":"Training a MIL model for cancer detection in Whole Slide Images","text":"<p>Detecting cancerous tissue in Whole Slide Images (WSIs) is a critical task in pathology. Traditionally, this involves a pathologist manually examining the entire slide - a process that is not only time-consuming but also subject to variability in human judgment. With recent advancements in machine learning, however, it is now possible to develop models that can assist or even automate parts of this diagnostic workflow.</p> <p>The common fully supervised approach would be as follows: the WSI is divided into smaller patches, each of which is labeled as either cancerous or non-cancerous. While effective, this method is heavily reliant on large amounts of annotated data. Since each WSI contains thousands of patches, labeling every patch requires significant effort from expert pathologists, making the process expensive and often impractical at scale.</p> <p>To address this limitation, Multiple Instance Learning (MIL) offer a more feasible alternative. Instead of labeling individual patches, each WSI is given a single label\u2014cancerous or non-cancerous\u2014without detailed annotations at the patch level. This drastically reduces the annotation burden, but also introduces new challenges in model training. In this tutorial, we explore the MIL framework to tackle this problem, enabling us to train an effective cancer detection model with only slide-level labels.</p>"},{"location":"examples/wsi_classification/#the-data","title":"The data","text":"<p>In this tutorial, we will use the CAMELYON16 dataset, a public dataset for the detection of breast cancer metastasis. The original version of this dataset can be found here. </p> <p>We have extracted patches from each WSI using the CLAM tool. Let's take a look at some of them. First, we define an auxiliary function to extract the patches from a <code>.tif</code> file. The coordinates of the patches\u2014also provided by CLAM\u2014are used to locate and crop the relevant regions from the slide.</p>"},{"location":"examples/wsi_classification/#creating-the-dataset","title":"Creating the dataset","text":"<p>Let's create a dataset for the CAMELYON16 dataset. Since training a MIL model directly on the patch images is computationally intractable, we are going to use pre-computed features. This is a common practice for MIL, since data rarely fits in memory. We have processed the CAMELYON16 dataset to be used for MIL binary classification problems, see the Hugging Face Hub. We offer some choices of features, including the ones extracted using the Barlow Twins method and the foundation model UNI.</p> <p>Breast cancer detection as a MIL problem</p> <p>We model each Whole Slide Image (WSI) as a bag of instances, where each instance corresponds to a patch extracted from the WSI.</p> <p>Each patch has an associated (but unknown) label denoted by \\(\\mathbf{y} = \\left[ y_1, \\ldots, y_N \\right]^\\top \\in {0, 1}^N\\), where \\(y_i = 1\\) if the patch contains evidence of breast cancer, and \\(y_i = 0\\) otherwise. Importantly, these instance-level labels are not available during training.</p> <p>Instead, we have access only to bag-level labels, denoted by \\(Y \\in {0, 1}\\), which indicate whether the entire WSI contains cancerous tissue. The relationship between the instance labels and the bag label follows a standard Multiple Instance Learning (MIL) assumption:</p> \\[ Y = \\max \\left\\{ y_1, \\ldots, y_N \\right\\} \\] <p>In other words, a WSI is labeled as positive (\\(Y = 1\\)) if at least one of its patches is cancerous (\\(y_i = 1\\)), and negative (\\(Y = 0\\)) if none of the patches contain cancer. This formulation allows us to train models using only weak supervision, where only bag-level labels are available, while the finer-grained instance labels remain latent.</p> <p>The <code>torchmil</code> library provides a convenient way to handle datasets specifically designed for MIL tasks. In this case, we will be using the <code>torchmil.datasets.CAMELYON16MILDataset</code> class, which is tailored for the CAMELYON16 dataset that we have processed. </p> <p>You only need to provide the <code>root</code> path of the processed dataset, the <code>patch_size</code>, the desired feature extractor (<code>features</code>) and the <code>partition</code> to load (train/test). See how simple is to instance the dataset, using the UNI features and patches of size \\(512 \\times 512\\):</p>"},{"location":"examples/wsi_classification/#mini-batching-of-bags","title":"Mini-batching of bags","text":"<p>Tipically, the bags in a MIL dataset have different size. This can be a problem when creating mini-batches. To solve this, we use the function <code>collate_fn</code> from the torchmil.data module. This function creates a mini-batch of bags by padding the bags with zeros to the size of the largest bag in the batch. The function also returns a mask tensor that indicates which instances are real and which are padding.</p> <p>Why not use <code>torch.nested</code>?</p> <p><code>torch.nested</code> offer a more flexible method for handling bags of varying sizes. However, since the PyTorch API for nested tensors is still in the prototype stage, torchmil currently relies on the padding approach.</p> <p>Let's create the dataloaders and visualize the shapes of a mini-batch. When using a patch size of \\(512 \\times 512\\), some of the bags in CAMELYON16 produce more than \\(20.000\\) instances. Because of this, we need to use a small <code>batch_size</code> to be able to fit it in standard GPUs.</p>"},{"location":"examples/wsi_classification/#training-a-model-in-camelyon16","title":"Training a model in CAMELYON16","text":"<p>We have shown how to load the CAMELYON16 dataset for the binary classification task. Now, let us train a MIL model in this dataset! </p> <p>For this example, we will use torchmil implementation of SmABMIL, a version of ABMIL where the Sm operator incorporates local interactions among the instances. To highlight how simple is to instance a model in torchmil, we will leave all the parameters by default except for the <code>in_shape</code>, which reflects the data shape. Feel free to check the documentation of SmABMIL to observe the different parameters that this model can be passed.</p>"},{"location":"examples/wsi_classification/#evaluating-the-model","title":"Evaluating the model","text":""},{"location":"examples/wsi_classification/#localizing-the-metastases","title":"Localizing the metastases","text":"<p>Now that we have trained a model, we can use it to localize the metastases in the WSI. We will use the <code>predict</code> method of the model to obtain the instance-level predictions. The <code>predict</code> method returns a tensor with the predicted scores for each instance in the bag. We can then threshold these scores to obtain the predicted instance labels. In the case of TransformerABMIL, these scores correspond to the attention weights of the instances.</p>"}]}