{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"torchmil","text":"<p>torchmil is a PyTorch-based library for deep Multiple Instance Learning (MIL).  It provides a simple, flexible, and extensible framework for working with MIL models and data. </p> <p>It includes:</p> <ul> <li>A collection of popular MIL models. </li> <li>Different PyTorch modules frequently used in MIL models.</li> <li>Handy tools to deal with MIL data.</li> <li>A collection of popular MIL datasets.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install torchmil\n</code></pre>"},{"location":"#quick-start","title":"Quick start","text":"<p>You can load a MIL dataset and train a MIL model in just a few lines of code:</p> <pre><code>from torchmil.datasets import Camelyon16MIL\nfrom torchmil.models import ABMIL\nfrom torchmil.utils import Trainer\nfrom torchmil.data import collate_fn\nfrom torch.utils.data import DataLoader\n\n# Load the Camelyon16 dataset\ndataset = Camelyon16MIL(root='data', features='UNI')\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n\n# Instantiate the ABMIL model and optimizer\nmodel = ABMIL(in_shape=(2048,), criterion=torch.nn.BCEWithLogitsLoss()) # each model has its own criterion\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# Instantiate the Trainer\ntrainer = Trainer(model, optimizer, device='cuda')\n\n# Train the model\ntrainer.train(dataloader, epochs=10)\n\n# Save the model\ntorch.save(model.state_dict(), 'model.pth')\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<p>Head to the get started guide to learn more about torchmil. Also, you can take a look at the examples to see how to use torchmil in practice. To see the full list of available models, datasets, and modules, check the API reference.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this library useful, please consider citing it:</p> <pre><code>@misc{torchmil,\n  author = {},\n  title = {torchmil: A PyTorch-based library for deep Multiple Instance Learning},\n  year = {},\n  publisher = {},\n  journal = {}\n}\n</code></pre>"},{"location":"getting_started/","title":"Getting started","text":""},{"location":"api/","title":"API reference","text":""},{"location":"api/#data-torchmildata","title":"Data: torchmil.data","text":"<ul> <li>Introduction</li> <li>Collate function</li> <li>Spatial and sequential representation</li> </ul>"},{"location":"api/#datasets-torchmildatasets","title":"Datasets: torchmil.datasets","text":"<ul> <li>Introduction</li> <li>Processed MIL Dataset</li> <li>Toy Dataset</li> <li>CT Scan Dataset</li> <li>WSI Dataset</li> <li>Binary classification Dataset</li> <li>Camelyon16 MIL Dataset</li> <li>PANDA MIL Dataset</li> <li>RSNA MIL Dataset</li> <li>False Frequency Dataset</li> <li>Multi-Concept Standard Dataset</li> <li>Single Concept Standard Dataset</li> </ul>"},{"location":"api/#modules-torchmilnn","title":"Modules: torchmil.nn","text":"<ul> <li>Introduction</li> <li>Attention<ul> <li>Attention Pool</li> <li>Sm Attention Pool</li> <li>Probabilistic Smooth Attention Pool</li> <li>Multihead Self-Attention</li> <li>Multihead Self-Attention with image Relative Positional Encoding (iRPE)</li> <li>Nystr\u00f6m Attention</li> <li>Multihead Cross-Attention</li> </ul> </li> <li>Transformers<ul> <li>Transformer base class</li> <li>Conventional Transformer</li> <li>Sm Transformer</li> <li>Nystr\u00f6m Transformer</li> <li>Transformer with image Relative Positional Encoding (iRPE)</li> <li>Tokens-to-Token module</li> </ul> </li> <li>Smooth operator (Sm)</li> <li>Mean pool</li> <li>Max pool</li> </ul>"},{"location":"api/#models-torchmilmodels","title":"Models: torchmil.models","text":"<ul> <li>Introduction</li> <li>General MIL model</li> <li>ABMIL</li> <li>CAMIL</li> <li>CLAM</li> <li>DeepGraphSurv</li> <li>DFTDMIL</li> <li>DSMIL</li> <li>GTP</li> <li>IIBMIL</li> <li>PatchGCN</li> <li>ProbSmoothABMIL</li> <li>SmABMIL</li> <li>SmTransformerABMIL</li> <li>TransformerABMIL</li> <li>TransformerProbSmoothABMIL</li> <li>TransMIL</li> <li>SETMIL</li> <li>IIBMIL</li> </ul>"},{"location":"api/#visualize-torchmilvisualize","title":"Visualize: torchmil.visualize","text":"<ul> <li>Introduction</li> <li>Visualizing CT scans</li> <li>Visualizing WSI</li> </ul>"},{"location":"api/#utils-torchmilutils","title":"Utils: torchmil.utils","text":"<ul> <li>Introduction</li> <li>Graph utils</li> <li>Trainer</li> </ul>"},{"location":"api/data/","title":"torchmil.data","text":"<p>Note</p> <p>The way data is represented in torchmil is inspired by PyTorch Geometric.</p> <p>See this notebook for an explanation with examples of the data representation in torchmil.</p>"},{"location":"api/data/#bags-in-torchmil","title":"Bags in torchmil","text":"<p>In Multiple Instance Learning (MIL), a bag is a collection of instances.  In torchmil, a bag is represented as a TensorDict. In most cases (e.g., the datasets provided in torchmil), a bag  will contain at least two keys:</p> <ul> <li><code>bag['X']</code>: a tensor of shape <code>(bag_size, ...)</code> containing the instances in the bag. Usually, this tensor is called bag feature matrix, since these instances are feature vectors extracted from the raw representation of the instances, and therefore it has shape <code>(bag_size, feature_dim)</code>. </li> <li><code>bag['Y']</code>: a tensor containing the label of the bag. In the simplest case, this tensor is a scalar, but it can be a tensor of any shape (e.g., in multi-class MIL).</li> </ul> <p>Additionally, a bag may contain other keys. The most common ones in torchmil are:</p> <ul> <li><code>bag['y_inst']</code>: a tensor of shape <code>(bag_size, ...)</code> containing the labels of the instances in the bag. In the pure MIL setting, this tensor is only used for evaluation purposes since the label of the instances are not known. However, some methods may require some sort of supervision at the instance level.</li> <li><code>bag['adj']</code>: a tensor of shape <code>(bag_size, bag_size)</code> containing the adjacency matrix of the bag. This matrix is used to represent the relationships between the instances in the bag. The methods implemented in torchmil.models allow this matrix to be a sparse tensor.</li> <li><code>bag['coords']</code>: a tensor of shape <code>(bag_size, coords_dim)</code> containing the coordinates of the instances in the bag. This tensor is used to represent the absolute position of the instances in the bag.</li> </ul> <p>The data representation in torchmil is designed to be flexible and to allow the user to add any additional information to the bags. The user can define new keys in the bags and use them in the models implemented in torchmil.</p>"},{"location":"api/data/#more-information","title":"More information","text":"<ul> <li>Batches in torchmil</li> <li>Spatial and sequential representation</li> </ul>"},{"location":"api/data/collate/","title":"Collate","text":"<p>Note</p> <p>See this notebook for an explanation with examples of how batching is performed in torchmil.</p>"},{"location":"api/data/collate/#batches-in-torchmil","title":"Batches in torchmil","text":"<p>Batching is crucial for training deep learning models. However, in MIL, each bag can be of different size. To solve this, in torchmil, the tensors in the bags are padded to the maximum size of the bags in the batch. A mask tensor is used to indicate which elements of the padded tensors are real instances and which are padding. This mask tensor is used to adjust the behavior of the models to ignore the padding elements (e.g., in the attention mechanism). </p> <p>Why not use <code>torch.nested</code>?</p> <p><code>torch.nested</code> offer a more flexible method for handling bags of varying sizes. However, since the PyTorch API for nested tensors is still in the prototype stage, torchmil currently relies on the padding approach.</p> <p>The function <code>torchmil.data.collate_fn</code> is used to collate a list of bags into a batch. This function can be used as the <code>collate_fn</code> argument of the PyTorch <code>DataLoader</code>. The function <code>torchmil.data.pad_tensors</code> is used to pad the tensors in the bags.</p>"},{"location":"api/data/collate/#torchmil.data.collate_fn","title":"<code>torchmil.data.collate_fn(batch_list, sparse=True)</code>","text":"<p>Collate function for MIL datasets. Given a list of bags (represented as dictionaries) it pads the tensors in the bag to the same shape. Then, it returns a dictionary representing the batch. The keys in the dictionary are the keys in the bag dictionaries. Additionally, the returned dictionary contains a mask for the padded tensors. This mask is 1 where the tensor is not padded and 0 where the tensor is padded.</p> <p>Parameters:</p> <ul> <li> <code>batch_list</code>               (<code>list[dict[str, Tensor]]</code>)           \u2013            <p>List of dictionaries. Each dictionary represents a bag and should contain the same keys. The values can be:</p> <ul> <li>Tensors of shape <code>(bag_size, ...)</code>. In this case, the tensors are padded to the same shape.</li> <li>Sparse tensors in COO format. In this case, the resulting sparse tensor has shape <code>(batch_size, max_bag_size, max_bag_size)</code>, where <code>max_bag_size</code> is the maximum bag size in the batch. If <code>sparse=False</code>, the sparse tensor is converted to a dense tensor.</li> </ul> </li> <li> <code>sparse</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, the sparse tensors are returned as sparse tensors. If False, the sparse tensors are converted to dense tensors.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>batch_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary with the same keys as the bag dictionaries. The values are tensors of shape <code>(batch_size, max_bag_size, ...)</code> or sparse tensors of shape <code>(batch_size, max_bag_size, max_bag_size)</code>. Additionally, the dictionary contains a mask of shape <code>(batch_size, max_bag_size)</code>.</p> </li> </ul>"},{"location":"api/data/collate/#torchmil.data.pad_tensors","title":"<code>torchmil.data.pad_tensors(tensor_list, padding_value=0)</code>","text":"<p>Pads a list of tensors to the same shape and returns a mask.</p> <p>Parameters:</p> <ul> <li> <code>tensor_list</code>               (<code>list[Tensor]</code>)           \u2013            <p>List of tensors, each of shape <code>(bag_size, ...)</code>.</p> </li> <li> <code>padding_value</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Value to pad with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>padded_tensor</code> (              <code>Tensor</code> )          \u2013            <p>Padded tensor of shape <code>(batch_size, max_bag_size, ...)</code>.</p> </li> <li> <code>mask</code> (              <code>Tensor</code> )          \u2013            <p>Mask of shape <code>(batch_size, max_bag_size)</code>.</p> </li> </ul>"},{"location":"api/data/representation/","title":"Representation","text":"<p>Note</p> <p>See this notebook for an explanation with examples of the different types of representations in torchmil.</p>"},{"location":"api/data/representation/#spatial-and-sequential-representation","title":"Spatial and sequential representation","text":"<p>In torchmil, bags can be represented in two ways: sequential and spatial. </p> <p>In the sequential representation <code>bag['X']</code> is a tensor of shape <code>(bag_size, dim)</code>. This representation is the most common in MIL. </p> <p>When the bag has some spatial structure, the sequential representation can be coupled with a graph using an adjacency matrix or with the coordinates of the instances. These are stored as <code>bag['adj']</code> (of shape <code>(bag_size, bag_size)</code>) and <code>bag['coords']</code> (of shape <code>(bag_size, coords_dim)</code>), respectively.</p> <p>Alternatively, the spatial representation can be used.  In this case, <code>bag['X']</code> is a tensor of shape <code>(coord1, ..., coordN, dim)</code>, where <code>N=coords_dim</code> is the number of dimensions of the space.</p> <p>In torchmil, you can convert from one representation to the other using the functions <code>torchmil.utils.seq_to_spatial</code> and <code>torchmil.utils.spatial_to_seq</code> from the torchmil.data module. These functions need the coordinates of the instances in the bag, stored as <code>bag['coords']</code>.</p> <p>Example: Whole Slide Images</p> <p>Due to their large resolution, Whole Slide Images (WSIs) are usually represented as bags of patches. Each patch is an image, from which a feature vector of is typically extracted. The spatial representation of a WSI has shape <code>(height, width, feat_dim)</code>, while the sequential representation has shape <code>(bag_size, feat_dim)</code>. The coordinates corresponds to the coordinates of the patches in the WSI. </p> <p>SETMIL is an example of a model that uses the spatial representation of a WSI. </p>"},{"location":"api/data/representation/#torchmil.data.seq_to_spatial","title":"<code>torchmil.data.seq_to_spatial(X, coords)</code>","text":"<p>Computes the spatial representation of a bag given the sequential representation and the coordinates.</p> <p>Given the input tensor <code>X</code> of shape <code>(batch_size, bag_size, dim)</code> and the coordinates <code>coords</code> of shape <code>(batch_size, bag_size, n)</code>,  this function returns the spatial representation <code>X_enc</code> of shape <code>(batch_size, coord1, coord2, ..., coordn, dim)</code>.</p> <p>This representation is characterized by the fact that the coordinates are used to index the elements of spatial representation: <code>X_enc[batch, i1, i2, ..., in, :] = X[batch, idx, :]</code> where <code>(i1, i2, ..., in) = coords[batch, idx]</code>.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Sequential representation of shape <code>(batch_size, bag_size, dim)</code>.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>Coordinates of shape <code>(batch_size, bag_size, n)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>X_esp</code> (              <code>Tensor</code> )          \u2013            <p>Spatial representation of shape <code>(batch_size, coord1, coord2, ..., coordn, dim)</code>.</p> </li> </ul>"},{"location":"api/data/representation/#torchmil.data.spatial_to_seq","title":"<code>torchmil.data.spatial_to_seq(X_esp, coords)</code>","text":"<p>Computes the sequential representation of a bag given the spatial representation and the coordinates.</p> <p>Given the spatial tensor <code>X_esp</code> of shape <code>(batch_size, coord1, coord2, ..., coordn, dim)</code> and the coordinates <code>coords</code> of shape <code>(batch_size, bag_size, n)</code>,  this function returns the sequential representation <code>X</code> of shape <code>(batch_size, bag_size, dim)</code>.</p> <p>This representation is characterized by the fact that the coordinates are used to index the elements of spatial representation: <code>X_seq[batch, idx, :] = X_esp[batch, i1, i2, ..., in, :]</code> where <code>(i1, i2, ..., in) = coords[batch, idx]</code>.</p> <p>Parameters:</p> <ul> <li> <code>X_esp</code>               (<code>Tensor</code>)           \u2013            <p>Spatial representation of shape <code>(batch_size, coord1, coord2, ..., coordn, dim)</code>.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>Coordinates of shape <code>(batch_size, bag_size, n)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>X_seq</code> (              <code>Tensor</code> )          \u2013            <p>Sequential representation of shape <code>(batch_size, bag_size, dim)</code>.</p> </li> </ul>"},{"location":"api/datasets/","title":"torchmil.datasets","text":""},{"location":"api/datasets/#available-datasets","title":"Available datasets","text":"<ul> <li>Processed MIL Dataset</li> <li>Toy Dataset</li> <li>CT Scan Dataset</li> <li>WSI Dataset</li> <li>Binary classification Dataset</li> <li>Camelyon16 MIL Dataset</li> <li>PANDA MIL Dataset</li> <li>RSNA MIL Dataset</li> <li>False Frequency Dataset</li> <li>Multi-Concept Standard Dataset</li> <li>Single Concept Standard Dataset</li> </ul>"},{"location":"api/datasets/binary_classification_dataset/","title":"Binary classification dataset","text":""},{"location":"api/datasets/binary_classification_dataset/#torchmil.datasets.BinaryClassificationDataset","title":"<code>torchmil.datasets.BinaryClassificationDataset</code>","text":"<p>               Bases: <code>ProcessedMILDataset</code></p> <p>Dataset for binary classification MIL problems. See <code>torchmil.datasets.ProcessedMILDataset</code> for more information.</p> <p>For a given bag with bag label \\(Y\\) and instance labels \\(\\left\\{ y_1, \\ldots, y_N \\right \\}\\), this dataset assumes that </p> \\[\\begin{gather}     Y \\in \\left\\{ 0, 1 \\right\\}, \\quad y_n \\in \\left\\{ 0, 1 \\right\\}, \\quad \\forall n \\in \\left\\{ 1, \\ldots, N \\right\\},\\\\     Y = \\max \\left\\{ y_1, \\ldots, y_N \\right\\}. \\end{gather}\\] <p>When the instance labels are not provided, they are set to -1. If the instance labels are provided, but they are not consistent with the bag label, a warning is issued and the instance labels are set to the bag label.</p>"},{"location":"api/datasets/binary_classification_dataset/#torchmil.datasets.BinaryClassificationDataset.__init__","title":"<code>__init__(features_path, labels_path, inst_labels_path=None, coords_path=None, bag_names=None, dist_thr=1.5, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":""},{"location":"api/datasets/binary_classification_dataset/#torchmil.datasets.BinaryClassificationDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag. </li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/camelyon16mil_dataset/","title":"CAMELYON16 dataset","text":""},{"location":"api/datasets/camelyon16mil_dataset/#torchmil.datasets.CAMELYON16MILDataset","title":"<code>torchmil.datasets.CAMELYON16MILDataset</code>","text":"<p>               Bases: <code>BinaryClassificationDataset</code>, <code>WSIDataset</code></p> <p>CAMELYON16 dataset for Multiple Instance Learning (MIL). </p> <p>The original CAMELYON16 dataset has been processed to be used for MIL binary classification problems. It can be downloaded from here.</p> <p>A patch is considered positive if at least 50% of its pixels are annotated as tumor. The WSI label is positive if at least one patch is positive.</p> <p>The following directory structure is expected:</p> <pre><code>```\nroot\n\u251c\u2500\u2500 patches_{patch_size}\n\u2502   \u251c\u2500\u2500 features\n\u2502   \u2502   \u251c\u2500\u2500 features_{features}\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 wsi1.npy\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 wsi2.npy\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 labels\n\u2502   \u2502   \u251c\u2500\u2500 wsi1.npy\n\u2502   \u2502   \u251c\u2500\u2500 wsi2.npy\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 patch_labels\n\u2502   \u2502   \u251c\u2500\u2500 wsi1.npy\n\u2502   \u2502   \u251c\u2500\u2500 wsi2.npy\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 coords\n\u2502   \u2502   \u251c\u2500\u2500 wsi1.npy\n\u2502   \u2502   \u251c\u2500\u2500 wsi2.npy\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 splits.csv\n```\n</code></pre>"},{"location":"api/datasets/camelyon16mil_dataset/#torchmil.datasets.CAMELYON16MILDataset.__init__","title":"<code>__init__(root, features='UNI', partition='train', patch_size=512, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>root</code>               (<code>str</code>)           \u2013            <p>Path to the root directory of the dataset.</p> </li> <li> <code>features</code>               (<code>str</code>, default:                   <code>'UNI'</code> )           \u2013            <p>Type of features to use. Must be one of ['UNI', 'resnet50_bt'].</p> </li> <li> <code>partition</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Partition of the dataset. Must be one of ['train', 'test'].</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Size of the patches. Currently, only 512 is supported.</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the patches features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/camelyon16mil_dataset/#torchmil.datasets.CAMELYON16MILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag. </li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/ctscan_dataset/","title":"CTScan dataset","text":""},{"location":"api/datasets/ctscan_dataset/#torchmil.datasets.CTScanDataset","title":"<code>torchmil.datasets.CTScanDataset</code>","text":"<p>               Bases: <code>ProcessedMILDataset</code></p> <p>This class represents a dataset of Computed Tomography (CT) scans for Multiple Instance Learning (MIL).</p> <p>MIL and CT scans. Computed Tomography (CT) scans are medical imaging techniques that use X-rays to obtain detailed images of the body. Usually, a CT scan is a 3D volume and is composed of a sequence of slices. Each slice is a 2D image that represents a cross-section of the body. In the context of MIL, a CT scan is considered a bag, and the slices are considered instances.</p> <p>Directory structure. It is assumed that the bags have been processed and saved as numpy files.  For more information on the processing of the bags, refer to the <code>ProcessedMILDataset</code> class. This dataset expects the following directory structure:</p> <pre><code>features_path\n\u251c\u2500\u2500 ctscan1.npy\n\u251c\u2500\u2500 ctscan2.npy\n\u2514\u2500\u2500 ...\nlabels_path\n\u251c\u2500\u2500 ctscan1.npy\n\u251c\u2500\u2500 ctscan2.npy\n\u2514\u2500\u2500 ...\ninst_labels_path\n\u251c\u2500\u2500 ctscan1.npy\n\u251c\u2500\u2500 ctscan2.npy\n\u2514\u2500\u2500 ...\n</code></pre> <p>Order of the slices and the adjacency matrix. This dataset assumes that the slices of the CT scans are ordered. An adjacency matrix \\(\\mathbf{A} = \\left[ A_{ij} \\right]\\) is built using this information:</p> \\[\\begin{equation} A_{ij} = \\begin{cases} d_{ij}, &amp; \\text{if } \\lvert i - j \\rvert = 1, \\\\ 0, &amp; \\text{otherwise},     \\end{cases} \\quad d_{ij} = \\begin{cases} 1, &amp; \\text{if } \\text{adj_with_dist=False}, \\\\ \\exp\\left( -\\frac{\\left\\| \\mathbf{x}_i - \\mathbf{x}_j \\right\\|}{d} \\right), &amp; \\text{if } \\text{adj_with_dist=True}. \\end{cases} \\end{equation}\\] <p>where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(\\mathbf{x}_j \\in \\mathbb{R}^d\\) are the features of instances \\(i\\) and \\(j\\), respectively.</p>"},{"location":"api/datasets/ctscan_dataset/#torchmil.datasets.CTScanDataset.__init__","title":"<code>__init__(features_path, labels_path, slice_labels_path=None, ctscan_names=None, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>features_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing the matrices of the CT scans</p> </li> <li> <code>labels_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing the labels of the CT scans.</p> </li> <li> <code>slice_labels_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the labels of the slices.</p> </li> <li> <code>ctscan_names</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of the names of the CT scans to load. If None, all CT scans in the <code>features_path</code> directory are loaded.</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the slices features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/ctscan_dataset/#torchmil.datasets.CTScanDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag. </li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/false_frequency_dataset/","title":"False Frequency dataset","text":""},{"location":"api/datasets/false_frequency_dataset/#torchmil.datasets.FalseFrequencyMILDataset","title":"<code>torchmil.datasets.FalseFrequencyMILDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>False Frequency MIL Dataset. Implementation from Algorithm 3 in Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests.</p>"},{"location":"api/datasets/false_frequency_dataset/#torchmil.datasets.FalseFrequencyMILDataset.__init__","title":"<code>__init__(D, num_bags, B, pos_class_prob=0.5, train=True, seed=0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>D</code>               (<code>int</code>)           \u2013            <p>Dimensionality of the data.</p> </li> <li> <code>num_bags</code>               (<code>int</code>)           \u2013            <p>Number of bags in the dataset.</p> </li> <li> <code>B</code>               (<code>int</code>)           \u2013            <p>Number of negative instances in each bag.</p> </li> <li> <code>pos_class_prob</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Probability of a bag being positive.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Seed for the random number generator.</p> </li> </ul>"},{"location":"api/datasets/false_frequency_dataset/#torchmil.datasets.FalseFrequencyMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Bag features of shape <code>(bag_size, feat_dim)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag.</li> </ul> </li> </ul>"},{"location":"api/datasets/mc_standard_dataset/","title":"Multi-Concept Standard dataset","text":""},{"location":"api/datasets/mc_standard_dataset/#torchmil.datasets.MCStandardMILDataset","title":"<code>torchmil.datasets.MCStandardMILDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Multi-Concept Standard MIL Dataset. Implementation from Algorithm 2 in Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests.</p>"},{"location":"api/datasets/mc_standard_dataset/#torchmil.datasets.MCStandardMILDataset.__init__","title":"<code>__init__(D, num_bags, B, pos_class_prob=0.5, train=True, seed=0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>D</code>               (<code>int</code>)           \u2013            <p>Dimensionality of the data.</p> </li> <li> <code>num_bags</code>               (<code>int</code>)           \u2013            <p>Number of bags in the dataset.</p> </li> <li> <code>B</code>               (<code>int</code>)           \u2013            <p>Number of negative instances in each bag.</p> </li> <li> <code>pos_class_prob</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Probability of a bag being positive.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Seed for the random number generator.</p> </li> </ul>"},{"location":"api/datasets/mc_standard_dataset/#torchmil.datasets.MCStandardMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Bag features of shape <code>(bag_size, feat_dim)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag.</li> </ul> </li> </ul>"},{"location":"api/datasets/pandamil_dataset/","title":"PANDA dataset","text":""},{"location":"api/datasets/pandamil_dataset/#torchmil.datasets.PANDAMILDataset","title":"<code>torchmil.datasets.PANDAMILDataset</code>","text":"<p>               Bases: <code>BinaryClassificationDataset</code>, <code>WSIDataset</code></p> <p>Prostate cANcer graDe Assessment (PANDA) dataset for Multiple Instance Learning (MIL). </p> <p>The original PANDA dataset has been processed to be used for MIL binary classification problems. It can be downloaded from here.</p> <p>A patch is considered positive if at least 50% of its pixels are annotated as tumor. The WSI label is positive if at least one patch is positive.</p> <p>The following directory structure is expected:</p> <pre><code>```\nroot\n\u251c\u2500\u2500 patches_{patch_size}\n\u2502   \u251c\u2500\u2500 features\n\u2502   \u2502   \u251c\u2500\u2500 features_{features}\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 wsi1.npy\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 wsi2.npy\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 labels\n\u2502   \u2502   \u251c\u2500\u2500 wsi1.npy\n\u2502   \u2502   \u251c\u2500\u2500 wsi2.npy\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 patch_labels\n\u2502   \u2502   \u251c\u2500\u2500 wsi1.npy\n\u2502   \u2502   \u251c\u2500\u2500 wsi2.npy\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 coords\n\u2502   \u2502   \u251c\u2500\u2500 wsi1.npy\n\u2502   \u2502   \u251c\u2500\u2500 wsi2.npy\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 splits.csv\n```\n</code></pre>"},{"location":"api/datasets/pandamil_dataset/#torchmil.datasets.PANDAMILDataset.__init__","title":"<code>__init__(root, features='UNI', partition='train', patch_size=512, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>root</code>               (<code>str</code>)           \u2013            <p>Path to the root directory of the dataset.</p> </li> <li> <code>features</code>               (<code>str</code>, default:                   <code>'UNI'</code> )           \u2013            <p>Type of features to use. Must be one of ['UNI', 'resnet50_bt'].</p> </li> <li> <code>partition</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Partition of the dataset. Must be one of ['train', 'test'].</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Size of the patches. Currently, only 512 is supported.</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the patches features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/pandamil_dataset/#torchmil.datasets.PANDAMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag. </li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/processed_mil_dataset/","title":"Processed MIL dataset","text":""},{"location":"api/datasets/processed_mil_dataset/#torchmil.datasets.ProcessedMILDataset","title":"<code>torchmil.datasets.ProcessedMILDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>This class represents a general MIL dataset where the bags have been processed.</p> <p>MIL processing and directory structure. It is assumed that the bags have been processed and saved as numpy files.  A feature file should yield an array of shape <code>(bag_size, ...)</code>, where <code>...</code> represents the shape of the features. A label file should yield an array of shape arbitrary shape, e.g., <code>(1,)</code> for binary classification. An instance label file should yield an array of shape <code>(bag_size, ...)</code>, where <code>...</code> represents the shape of the instance labels. A coordinates file should yield an array of shape <code>(bag_size, coords_dim)</code>, where <code>coords_dim</code> is the dimension of the coordinates.</p> <p>This dataset expects the following directory structure:</p> <pre><code>features_path\n\u251c\u2500\u2500 bag1.npy\n\u251c\u2500\u2500 bag2.npy\n\u2514\u2500\u2500 ...\nlabels_path\n\u251c\u2500\u2500 bag1.npy\n\u251c\u2500\u2500 bag2.npy\n\u2514\u2500\u2500 ...\ninst_labels_path\n\u251c\u2500\u2500 bag1.npy\n\u251c\u2500\u2500 bag2.npy\n\u2514\u2500\u2500 ...\ncoords_path\n\u251c\u2500\u2500 bag1.npy\n\u251c\u2500\u2500 bag2.npy\n\u2514\u2500\u2500 ...\n</code></pre> <p>Adjacency matrix. If the coordinates of the instances are available, the adjacency matrix will be built using the Euclidean distance between the coordinates. Formally, the adjacency matrix \\(\\mathbf{A} = \\left[ A_{ij} \\right]\\) is defined as:</p> \\[\\begin{equation} A_{ij} = \\begin{cases} d_{ij}, &amp; \\text{if } \\left\\| \\mathbf{c}_i - \\mathbf{c}_j \\right\\| \\leq \\text{dist_thr}, \\\\ 0, &amp; \\text{otherwise},  \\end{cases} \\quad d_{ij} = \\begin{cases} 1, &amp; \\text{if } \\text{adj_with_dist=False}, \\\\ \\exp\\left( -\\frac{\\left\\| \\mathbf{x}_i - \\mathbf{x}_j \\right\\|}{d} \\right), &amp; \\text{if } \\text{adj_with_dist=True}. \\end{cases} \\end{equation}\\] <p>where \\(\\mathbf{c}_i\\) and \\(\\mathbf{c}_j\\) are the coordinates of the instances \\(i\\) and \\(j\\), respectively, \\(\\text{dist_thr}\\) is a threshold distance, and \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(\\mathbf{x}_j \\in \\mathbb{R}^d\\) are the features of instances \\(i\\) and \\(j\\), respectively.</p>"},{"location":"api/datasets/processed_mil_dataset/#torchmil.datasets.ProcessedMILDataset.__init__","title":"<code>__init__(features_path, labels_path, inst_labels_path=None, coords_path=None, bag_names=None, dist_thr=1.5, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>features_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing the features.</p> </li> <li> <code>labels_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing the bag labels.</p> </li> <li> <code>inst_labels_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the instance labels.</p> </li> <li> <code>coords_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the coordinates.</p> </li> <li> <code>bag_names</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of bag names to load. If None, all bags are loaded.</p> </li> <li> <code>dist_thr</code>               (<code>float</code>, default:                   <code>1.5</code> )           \u2013            <p>Distance threshold for building the adjacency matrix.</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the instance features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/processed_mil_dataset/#torchmil.datasets.ProcessedMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag. </li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/rsnamil_dataset/","title":"RSNA dataset","text":""},{"location":"api/datasets/rsnamil_dataset/#torchmil.datasets.RSNAMILDataset","title":"<code>torchmil.datasets.RSNAMILDataset</code>","text":"<p>               Bases: <code>BinaryClassificationDataset</code>, <code>CTScanDataset</code></p> <p>RSNA Intracranial Hemorrhage Detection dataset for Multiple Instance Learning (MIL). </p> <p>The original dataset has been processed to be used for MIL binary classification problems. It can be downloaded from here.</p> <p>A slice is considered positive if it shows evidence of hemorrhage. The CT scan label is positive if at least one slice is positive.</p> <p>The following directory structure is expected:</p> <pre><code>```\nroot\n\u251c\u2500\u2500 features\n\u2502   \u251c\u2500\u2500 features_{features}\n\u2502   \u2502   \u251c\u2500\u2500 ctscan_name1.npy\n\u2502   \u2502   \u251c\u2500\u2500 ctscan_name2.npy\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 labels\n\u2502   \u251c\u2500\u2500 ctscan_name1.npy\n\u2502   \u251c\u2500\u2500 ctscan_name2.npy\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 slice_labels\n\u2502   \u251c\u2500\u2500 ctscan_name1.npy\n\u2502   \u251c\u2500\u2500 ctscan_name2.npy\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 splits.csv\n```\n</code></pre>"},{"location":"api/datasets/rsnamil_dataset/#torchmil.datasets.RSNAMILDataset.__init__","title":"<code>__init__(root, features='resnet50', partition='train', adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>root</code>               (<code>str</code>)           \u2013            <p>Path to the root directory of the dataset.</p> </li> <li> <code>features</code>               (<code>str</code>, default:                   <code>'resnet50'</code> )           \u2013            <p>Type of features to use. Must be one of ['resnet18', 'resnet50', 'vit_b_32']</p> </li> <li> <code>partition</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Partition of the dataset. Must be one of ['train', 'test'].</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the patches features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.       </p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/rsnamil_dataset/#torchmil.datasets.RSNAMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag. </li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/datasets/sc_standard_dataset/","title":"Single Concept Standard dataset","text":""},{"location":"api/datasets/sc_standard_dataset/#torchmil.datasets.SCStandardMILDataset","title":"<code>torchmil.datasets.SCStandardMILDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Single-Concept Standard MIL Dataset. Implementation from Algorithm 1 in Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests.</p>"},{"location":"api/datasets/sc_standard_dataset/#torchmil.datasets.SCStandardMILDataset.__init__","title":"<code>__init__(D, num_bags, B, pos_class_prob=0.5, train=True, seed=0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>D</code>               (<code>int</code>)           \u2013            <p>Dimensionality of the data.</p> </li> <li> <code>num_bags</code>               (<code>int</code>)           \u2013            <p>Number of bags in the dataset.</p> </li> <li> <code>B</code>               (<code>int</code>)           \u2013            <p>Number of negative instances in each bag.</p> </li> <li> <code>pos_class_prob</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Probability of a bag being positive.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Seed for the random number generator.</p> </li> </ul>"},{"location":"api/datasets/sc_standard_dataset/#torchmil.datasets.SCStandardMILDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Bag features of shape <code>(bag_size, feat_dim)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag.</li> </ul> </li> </ul>"},{"location":"api/datasets/toy_dataset/","title":"Toy dataset","text":""},{"location":"api/datasets/toy_dataset/#torchmil.datasets.ToyDataset","title":"<code>torchmil.datasets.ToyDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>This class represents a synthetic dataset for Multiple Instance Learning (MIL) tasks. It generates synthetic bags of instances from a given dataset, where each bag is labeled based on the presence or absence of specific \"positive\" instances.  This class is particularly useful for simulating MIL scenarios, where the goal is to learn from bags of instances rather than individual data points.</p> <p>Bag generation. The dataset generates bags by sampling instances from the input  <code>(data, labels)</code> pair. A bag is labeled as positive if it contains at least one instance from a predefined set of positive labels (<code>obj_labels</code>). The probability of generating a positive bag can be controlled via the argument <code>pos_class_prob</code>. The size of each bag can be specified using the argument <code>bag_size</code>. Additionally, each bag includes instance-level labels, indicating whether individual instances belong to the positive class.</p> <p>Each bag is returned as a dictionary (TensorDict) with the following keys:</p> <ul> <li>X: The bag's feature matrix of shape <code>(bag_size, num_features)</code>.</li> <li>Y: The bag's label (1 for positive, 0 for negative).</li> <li>y_inst: The instance-level labels within the bag.</li> </ul> <p>MNIST example.  We can create a MIL dataset from the original MNIST as follows:</p> <pre><code>import torch\nfrom torchvision import datasets, transforms\n\n# Load MNIST dataset\nmnist_train = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n\n# Extract features and labels\ndata = mnist_train.data.numpy().reshape(-1, 28*28) / 255\nlabels = mnist_train.targets.numpy()\n\n# Define positive labels\nobj_labels = [1, 2] # Digits 1 and 2 are considered positive\n\n# Create MIL dataset\ntoy_dataset = ToyDataset(data, labels, num_bags=1000, obj_labels=obj_labels, bag_size=10)\n\n# Retrieve a bag\nbag = toy_dataset[0]\nX, Y, y_inst = bag['X'], bag['Y'], bag['y_inst']    \n</code></pre>"},{"location":"api/datasets/toy_dataset/#torchmil.datasets.ToyDataset.__init__","title":"<code>__init__(data, labels, num_bags, obj_labels, bag_size, pos_class_prob=0.5, seed=0)</code>","text":"<p>ToyMIL dataset class constructor.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>Data matrix of shape <code>(num_instances, num_features)</code>.</p> </li> <li> <code>labels</code>               (<code>ndarray</code>)           \u2013            <p>Labels vector of shape <code>(num_instances,)</code>.</p> </li> <li> <code>num_bags</code>               (<code>int</code>)           \u2013            <p>Number of bags to generate.</p> </li> <li> <code>obj_labels</code>               (<code>list[int]</code>)           \u2013            <p>List of labels to consider as positive.</p> </li> <li> <code>bag_size</code>               (<code>Union[int, tuple[int, int]]</code>)           \u2013            <p>Number of instances per bag. If a tuple <code>(min_size, max_size)</code> is provided, the bag size is sampled uniformly from this range.</p> </li> <li> <code>pos_class_prob</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Probability of generating a positive bag.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Random seed.</p> </li> </ul>"},{"location":"api/datasets/toy_dataset/#torchmil.datasets.ToyDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Bag features of shape <code>(bag_size, feat_dim)</code>.</li> <li>Y: Label of the bag.</li> <li>y_inst: Instance labels of the bag.</li> </ul> </li> </ul>"},{"location":"api/datasets/wsi_dataset/","title":"WSI dataset","text":""},{"location":"api/datasets/wsi_dataset/#torchmil.datasets.WSIDataset","title":"<code>torchmil.datasets.WSIDataset</code>","text":"<p>               Bases: <code>ProcessedMILDataset</code></p> <p>This class represents a dataset of Whole Slide Images (WSI) for Multiple Instance Learning (MIL).</p> <p>MIL and WSIs. Whole Slide Images (WSIs) are high-resolution images of tissue samples used in digital pathology. Due to their large size, WSIs are usually divided into smaller patches. In the context of MIL, a WSI is considered a bag, and the patches are considered instances.</p> <p>Patch and feature extraction. Different tools are available to obtain the previous directory structure from a set of WSIs. For example, given a set of WSIs in the original format (e.g., .tif extension), patches can be extracted using tools such as CLAM. This tool outputs the coordinates of the patches, which can be used to extract the patches from the WSIs. Then, a feature vector can be extracted from each patch using a pretrained model. </p> <p>Binary MIL for WSIs. In binary classification, the label \\(Y\\) of the WSI usually represents the presence (\\(Y=1\\)) or absence (\\(Y=0\\)) of a certain characteristic in the WSI.  This characteristic can be present in one or more patches of the WSI, but the exact location of the characteristic is unknown. This translates into a patch having an unknown label \\(y_n\\), which is positive (\\(y_n=1\\)) if it contains the characteristic, and negative (\\(y_n=0\\)) otherwise.  Consequently, \\(Y = \\max\\left\\{ y_1, y_2, \\ldots, y_N \\right\\}\\), where \\(N\\) is the number of patches in the WSI. This means that the WSI is positive (contains the characteristic) if at least one of its patches is positive (contains the characteristic). In the case that the WSI has been annotated at the patch level, the instance labels \\(y_n\\) can be used solely for evaluation purposes. See <code>torchmil.datasets.BinaryClassificationDataset</code> for more information.</p> <p>Directory structure. It is assumed that the bags have been processed and saved as numpy files.  For more information on the processing of the bags, refer to the <code>ProcessedMILDataset</code> class. This dataset expects the following directory structure:</p> <pre><code>features_path\n\u251c\u2500\u2500 wsi1.npy\n\u251c\u2500\u2500 wsi2.npy\n\u2514\u2500\u2500 ...\nlabels_path\n\u251c\u2500\u2500 wsi1.npy\n\u251c\u2500\u2500 wsi2.npy\n\u2514\u2500\u2500 ...\ninst_labels_path\n\u251c\u2500\u2500 wsi1.npy\n\u251c\u2500\u2500 wsi2.npy\n\u2514\u2500\u2500 ...\ncoords_path\n\u251c\u2500\u2500 wsi1.npy\n\u251c\u2500\u2500 wsi2.npy\n\u2514\u2500\u2500 ...\n</code></pre> <p>Adjacency matrix. If the coordinates of the patches are available, an adjacency matrix representing the spatial relationships between the patches is built.</p> \\[\\begin{equation} A_{ij} = \\begin{cases} d_{ij}, &amp; \\text{if } \\left\\| \\mathbf{c}_i - \\mathbf{c}_j \\right\\| \\leq \\text{dist_thr}, \\\\ 0, &amp; \\text{otherwise},  \\end{cases} \\quad d_{ij} = \\begin{cases} 1, &amp; \\text{if } \\text{adj_with_dist=False}, \\\\ \\exp\\left( -\\frac{\\left\\| \\mathbf{x}_i - \\mathbf{x}_j \\right\\|}{d} \\right), &amp; \\text{if } \\text{adj_with_dist=True}. \\end{cases} \\end{equation}\\] <p>where \\(\\mathbf{c}_i\\) and \\(\\mathbf{c}_j\\) are the coordinates of the patches \\(i\\) and \\(j\\), respectively, \\(\\text{dist_thr}\\) is a threshold distance, and \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(\\mathbf{x}_j \\in \\mathbb{R}^d\\) are the features of patches \\(i\\) and \\(j\\), respectively. If no <code>dist_thr</code> is provided, it is set to \\(\\sqrt{2} \\times \\text{patch_size}\\).</p>"},{"location":"api/datasets/wsi_dataset/#torchmil.datasets.WSIDataset.__init__","title":"<code>__init__(features_path, labels_path, patch_labels_path=None, coords_path=None, wsi_names=None, patch_size=512, dist_thr=None, adj_with_dist=False, norm_adj=True, load_at_init=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>features_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing the feature matrices of the WSIs.</p> </li> <li> <code>labels_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing the labels of the WSIs.</p> </li> <li> <code>patch_labels_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the labels of the patches.</p> </li> <li> <code>coords_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory containing the coordinates of the patches.</p> </li> <li> <code>wsi_names</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of the names of the WSIs to load. If None, all the WSIs in the <code>features_path</code> directory are loaded.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Size of the patches.</p> </li> <li> <code>dist_thr</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Distance threshold for building the adjacency matrix. If None, it is set to <code>sqrt(2) * patch_size</code>.</p> </li> <li> <code>adj_with_dist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the adjacency matrix is built using the Euclidean distance between the patches features. If False, the adjacency matrix is binary.</p> </li> <li> <code>norm_adj</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalize the adjacency matrix.</p> </li> <li> <code>load_at_init</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, load the bags at initialization. If False, load the bags on demand.</p> </li> </ul>"},{"location":"api/datasets/wsi_dataset/#torchmil.datasets.WSIDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>Index of the bag to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bag_dict</code> (              <code>TensorDict</code> )          \u2013            <p>Dictionary containing the following keys:</p> <ul> <li>X: Features of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>Y: Label of the bag. </li> <li>y_inst: Instance labels of the bag, of shape <code>(bag_size, ...)</code>.</li> <li>adj: Adjacency matrix of the bag. It is a sparse COO tensor of shape <code>(bag_size, bag_size)</code>. If <code>norm_adj=True</code>, the adjacency matrix is normalized.</li> <li>coords: Coordinates of the bag, of shape <code>(bag_size, coords_dim)</code>.</li> </ul> </li> </ul>"},{"location":"api/models/","title":"torchmil.models","text":""},{"location":"api/models/#available-models","title":"Available models","text":"<ul> <li>General MIL model</li> <li>ABMIL</li> <li>CAMIL</li> <li>CLAM</li> <li>DFTDMIL</li> <li>DSMIL</li> <li>GTP</li> <li>ProbSmoothABMIL</li> <li>SmABMIL</li> <li>SmTransformerABMIL</li> <li>TransformerABMIL</li> <li>TransformerProbSmoothABMIL</li> <li>TransMIL</li> <li>SETMIL</li> <li>IIBMIL</li> </ul>"},{"location":"api/models/abmil/","title":"ABMIL","text":""},{"location":"api/models/abmil/#torchmil.models.ABMIL","title":"<code>torchmil.models.ABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Attention-based Multiple Instance Learning (ABMIL) model, proposed in the paper Attention-based Multiple Instance Learning.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, it aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) using the attention-based pooling, </p> \\[ \\mathbf{z}, \\mathbf{f} = \\operatorname{AttentionPool}(\\mathbf{X}). \\] <p>where \\(\\mathbf{f} \\in \\mathbb{R}^{N}\\) are the attention values. See AttentionPool for more details on the attention-based pooling. The bag representation \\(\\mathbf{z}\\) is then fed into a classifier (one linear layer) to predict the bag label.</p>"},{"location":"api/models/abmil/#torchmil.models.ABMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=128, att_act='tanh', gated=False, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>att_act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for attention. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>gated</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use gated attention in the attention pooling.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/abmil/#torchmil.models.ABMIL.forward","title":"<code>forward(X, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/abmil/#torchmil.models.ABMIL.compute_loss","title":"<code>compute_loss(Y, X, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/abmil/#torchmil.models.ABMIL.predict","title":"<code>predict(X, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/camil/","title":"CAMIL","text":""},{"location":"api/models/camil/#torchmil.models.CAMIL","title":"<code>torchmil.models.CAMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Context-Aware Multiple Instance Learning (CAMIL) model, presented in the paper CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, a global bag representation is computed using a NystromTransformerLayer layer, </p> \\[ \\mathbf{T} = \\operatorname{NystromTransformerLayer}(\\mathbf{X})\\] <p>Next, a local bag representation is computed using the CAMILSelfAttention layer,</p> \\[ \\mathbf{L} = \\operatorname{CAMILSelfAttention}(\\mathbf{T}) \\] <p>Finally, the local and global information is fused as</p> \\[ \\mathbf{M} = \\operatorname{sigmoid}(\\mathbf{L}) \\odot \\mathbf{L} + (1 - \\operatorname{sigmoid}(\\mathbf{L})) \\odot \\mathbf{T},\\] <p>where \\(\\odot\\) denotes element-wise multiplication and \\(\\operatorname{sigmoid}\\) is the sigmoid function.</p> <p>Lastly, the final bag representation is computed using the CAMILAttentionPool, modification of the Gatted Attention Pool mechanism. The bag representation is then fed into a linear classifier to predict the bag label.</p>"},{"location":"api/models/camil/#torchmil.models.CAMIL.__init__","title":"<code>__init__(in_shape, nystrom_att_dim=512, pool_att_dim=128, gated_pool=False, n_heads=4, n_landmarks=None, pinv_iterations=6, residual=True, dropout=0.0, use_mlp=False, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>pool_att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension for the attention pooling layer.</p> </li> <li> <code>gated_pool</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use gated attention pooling.</p> </li> <li> <code>nystrom_att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension for the Nystrom Transformer layer.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of attention heads in the Nystrom Transformer layer.</p> </li> <li> <code>n_landmarks</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of landmarks in the Nystrom Transformer layer.</p> </li> <li> <code>pinv_iterations</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of iterations for computing the pseudo-inverse in the Nystrom Transformer layer.</p> </li> <li> <code>residual</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, use residual connections in the Nystrom Transformer layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate of the Nystrom Transformer Layer.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use MLP in the Nystrom Transformer layer.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/camil/#torchmil.models.CAMIL.forward","title":"<code>forward(X, adj, mask, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/camil/#torchmil.models.CAMIL.compute_loss","title":"<code>compute_loss(Y, X, adj, mask)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/camil/#torchmil.models.CAMIL.predict","title":"<code>predict(X, adj, mask, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/camil/#torchmil.models.camil.CAMILSelfAttention","title":"<code>torchmil.models.camil.CAMILSelfAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Self-attention layer used in CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images. This layer computes the self-attention values using the local information of the bag. The local information is captured using an adjacency matrix, which measures the similarity between the embeddings of instances in the bag.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), and an adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), this layer computes</p> \\[ \\mathbf{l}_i = \\frac{\\exp\\left(\\sum_{j=1}^N a_{ij} \\mathbf{q}_i^\\top \\mathbf{k}_j \\right)}{\\sum_{k=1}^N \\exp \\left(\\sum_{j=1}^N a_{kj} \\mathbf{q}_k^\\top \\mathbf{k}_j \\right)} \\mathbf{v}_i,\\] <p>where \\(\\mathbf{q}_i = \\mathbf{W_q}\\mathbf{x}_i\\), \\(\\mathbf{k}_i = \\mathbf{W_k}\\mathbf{x}_i\\), and \\(\\mathbf{v}_i = \\mathbf{W_v}\\mathbf{x}_i\\) are the query, key, and value vectors, respectively. Finally, it returns \\(\\mathbf{L} = \\left[ \\mathbf{l}_1, \\ldots, \\mathbf{l}_N \\right]^\\top\\).</p>"},{"location":"api/models/camil/#torchmil.models.camil.CAMILSelfAttention.forward","title":"<code>forward(X, adj)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag of features of shape (batch_size, bag_size, in_dim)</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>L</code> (              <code>Tensor</code> )          \u2013            <p>Self-attention vectors with shape (batch_size, bag_size, in_dim)</p> </li> </ul>"},{"location":"api/models/camil/#torchmil.models.camil.CAMILAttentionPool","title":"<code>torchmil.models.camil.CAMILAttentionPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Attention pooling layer as described in CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images.</p> <p>Given a bag of features \\(\\mathbf{T} = \\left[ \\mathbf{t}_1, \\ldots, \\mathbf{t}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\) and \\(\\mathbf{M} = \\left[ \\mathbf{m}_1, \\ldots, \\mathbf{m}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this layer computes the final bag representation \\(\\mathbf{z}\\) as</p> \\[\\begin{gather} \\mathbf{f} = \\mathbf{w}^\\top \\tanh(\\mathbf{T} \\mathbf{W} ) \\odot \\operatorname{sigmoid}(\\mathbf{T} \\mathbf{U}), \\\\ \\mathbf{s} = \\text{softmax}(\\mathbf{f}), \\\\ \\mathbf{z} = \\mathbf{M}^\\top \\mathbf{s}, \\end{gather}\\] <p>where \\(\\mathbf{W}, \\mathbf{U}\\) and \\(\\mathbf{w}\\) are learnable parameters. Note the difference with conventional AttentionPool layer, where the attention values and bag representation are computed from the same set of features.</p>"},{"location":"api/models/camil/#torchmil.models.camil.CAMILAttentionPool.forward","title":"<code>forward(T, M, mask, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>T</code>               (<code>Tensor</code>)           \u2013            <p>(batch_size, bag_size, in_dim)</p> </li> <li> <code>M</code>               (<code>Tensor</code>)           \u2013            <p>(batch_size, bag_size, in_dim)</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>(batch_size, bag_size)</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values in addition to <code>z</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>Tensor</code> )          \u2013            <p>(batch_size, in_dim)</p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>(batch_size, bag_size) if `return_att</p> </li> </ul>"},{"location":"api/models/clam/","title":"CLAM","text":""},{"location":"api/models/clam/#torchmil.models.CLAM_SB","title":"<code>torchmil.models.CLAM_SB</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Clustering-constrained Attention Multiple Instance Learning (CLAM), proposed in the paper Data Efficient and Weakly Supervised Computational Pathology on Whole Slide Images.</p> <p>Overview. The forward pass of CLAM is identical to the forward pass of ABMIL. The difference lies in the instance-level regularization, which we describe below.</p> <p>Instance-level regularization. CLAM uses a binary clustering objective during training.  For this, in the binary MIL setting, two clustering classifiers are considered: \\(c_0 \\colon \\mathbb{R}^D \\to \\mathbb{R}\\) and \\(c_1 \\colon \\mathbb{R}^D \\to \\mathbb{R}\\). To supervise this objective, the attention values computes by the attention pooling are used to generate pseudo labels. </p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with label \\(Y\\) and attention values \\(\\mathbf{f} = \\left[ f_1, \\ldots, f_N \\right]^\\top \\in \\mathbb{R}^{N}\\), the instance-level regularization is performed as follows:</p> <ol> <li> <p>The \\(k\\) instances with the highest attention values are selected as in-the-class instances. The \\(k\\) instances with the lowest attention values are selected as out-of-the-class instances,</p> \\[\\begin{gather}     D_{\\text{in}} = \\left\\{ \\mathbf{x}_i \\mid f_i \\in \\text{TopK}(\\mathbf{f}, k) \\right\\}, \\\\     D_{\\text{out}} = \\left\\{ \\mathbf{x}_i \\mid f_i \\in \\text{BottomK}(\\mathbf{f}, k) \\right\\}. \\end{gather}\\] </li> <li> <p>The instances in \\(D_{\\text{in}}\\) are assigned a pseudo label of 1 for \\(c_Y\\), and a pseudo label of 0 for \\(c_{1-Y}\\).  The instances in \\(D_{\\text{out}}\\) are assigned a pseudo label of 0 for \\(c_Y\\). The pseudo labels are used to train the clustering classifiers, </p> \\[\\begin{gather}     \\ell_{\\text{in}} = \\frac{1}{2K} \\left( \\sum_{\\mathbf{x} \\in D_{\\text{in}}}\\ell_{\\text{inst}}(c_Y(\\mathbf{x}), 1) + \\sum_{\\mathbf{x} \\in D_{\\text{out}}}\\ell_{\\text{inst}}(c_{Y}(\\mathbf{x}), 0) \\right), \\\\     \\ell_{\\text{out}} = \\frac{1}{K} \\sum_{\\mathbf{x} \\in D_{\\text{in}}}\\ell_{\\text{inst}}(c_{1-Y}(\\mathbf{x}), 0), \\end{gather}\\] </li> </ol> <p>where \\(\\ell_{\\text{inst}}\\) is the instance-level loss function (the default is SmoothTop1SVM) and \\(Y\\) is the true bag label.  The total instance-level loss is \\(\\ell_{\\text{in}} + \\ell_{\\text{out}}\\), which is added to the bag-level loss to train the model.</p>"},{"location":"api/models/clam/#torchmil.models.CLAM_SB.__init__","title":"<code>__init__(in_shape=None, att_dim=128, att_act='tanh', k_sample=10, gated=False, inst_loss_name='SmoothTop1SVM', feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>att_act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for attention. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>k_sample</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of instances to sample.</p> </li> <li> <code>gated</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use gated attention in the attention pooling.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/clam/#torchmil.models.CLAM_SB.forward","title":"<code>forward(X, mask=None, return_att=False, return_emb=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_emb</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns embeddings in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> <li> <code>emb</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_emb=True</code>. Embeddings of shape (batch_size, bag_size, feat_dim).</p> </li> </ul>"},{"location":"api/models/clam/#torchmil.models.CLAM_SB.compute_loss","title":"<code>compute_loss(Y, X, mask)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/clam/#torchmil.models.CLAM_SB.predict","title":"<code>predict(X, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Predicted bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>Predicted instance labels of shape <code>(batch_size, bag_size)</code>. Only returned when <code>return_inst_pred=True</code>.</p> </li> </ul>"},{"location":"api/models/deepgraphsurv/","title":"DeepGraphSurv","text":""},{"location":"api/models/deepgraphsurv/#torchmil.models.DeepGraphSurv","title":"<code>torchmil.models.DeepGraphSurv</code>","text":"<p>               Bases: <code>Module</code></p> <p>DeepGraphSurv model, as proposed in Graph CNN for Survival Analysis on Whole Slide Pathological Images.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\),  the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, the representation branch transforms the instance features using a Graph Convolutional Network (GCN), and the  attention branch computes the attention values \\(\\mathbf{f}\\) using another GCN,</p> \\[\\begin{gather} \\mathbf{H} = \\operatorname{GCN}_{\\text{rep}}(\\mathbf{X}, \\mathbf{A}) \\in \\mathbb{R}^{N \\times \\texttt{hidden_dim}}, \\\\ \\mathbf{f} = \\operatorname{GCN}_{\\text{att}}(\\mathbf{X}, \\mathbf{A}) \\in \\mathbb{R}^{N \\times 1}. \\end{gather}\\] <p>These GCNs are implemented using the DeepGCN layer (see DeepGCNLayer) with GCNConv, LayerNorm, and ReLU activation (see GCNConv).</p> <p>Writing \\(\\mathbf{H} = \\left[ \\mathbf{h}_1, \\ldots, \\mathbf{h}_N \\right]^\\top\\), the attention values are used to compute the bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{\\texttt{hidden_dim}}\\) as </p> \\[\\begin{equation} \\mathbf{z} = \\mathbf{H}^\\top \\operatorname{Softmax}(\\mathbf{f}) = \\sum_{n=1}^N s_n \\mathbf{h}_n, \\end{equation}\\] <p>where \\(s_n\\) is the normalized attention score for the \\(n\\)-th instance.  The bag representation \\(\\mathbf{z}\\) is then fed into a classifier (one linear layer) to predict the bag label.</p>"},{"location":"api/models/deepgraphsurv/#torchmil.models.DeepGraphSurv.__init__","title":"<code>__init__(in_shape=None, n_layers_rep=4, n_layers_att=2, hidden_dim=None, att_dim=128, dropout=0.0, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>n_layers_rep</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of GCN layers in the representation branch.</p> </li> <li> <code>n_layers_att</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Number of GCN layers in the attention branch.</p> </li> <li> <code>hidden_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Hidden dimension. If not provided, it will be set to the feature dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function.</p> </li> </ul>"},{"location":"api/models/deepgraphsurv/#torchmil.models.DeepGraphSurv.forward","title":"<code>forward(X, adj, mask, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/deepgraphsurv/#torchmil.models.DeepGraphSurv.compute_loss","title":"<code>compute_loss(Y, X, adj, mask)</code>","text":"<p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss</p> </li> </ul>"},{"location":"api/models/deepgraphsurv/#torchmil.models.DeepGraphSurv.predict","title":"<code>predict(X, adj, mask, return_inst_pred=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns instance predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/dsmil/","title":"DSMIL","text":""},{"location":"api/models/dsmil/#torchmil.models.DSMIL","title":"<code>torchmil.models.DSMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Dual-stream Multiple Instance Learning (DSMIL) model, proposed in the paper Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning.</p> <p>Overview. Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, two streams are used. The first stream uses an instance classifier \\(c \\ \\colon \\mathbb{R}^D \\to \\mathbb{R}\\) (implemented as a linear layer) and retrieves the instance with the highest logit score,</p> \\[ m = \\arg \\max \\{ c(\\mathbf{x}_1), \\ldots, c(\\mathbf{x}_N) \\}. \\] <p>Then, the second stream computes the bag representation \\(\\mathbf{z} \\in \\mathbb{R}^D\\) as</p> \\[ \\mathbf{z} = \\frac{ \\exp \\left( \\mathbf{q}_i^\\top \\mathbf{q}_m \\right)}{\\sum_{k=1}^N \\exp \\left( \\mathbf{q}_k^\\top \\mathbf{q}_m \\right)} \\mathbf{v}_i, \\] <p>where \\(\\mathbf{q}_i = \\mathbf{W}_q \\mathbf{x}_i\\) and \\(\\mathbf{v}_i = \\mathbf{W}_v \\mathbf{x}_i\\). This is similar to self-attention with the difference that query-key matching is performed only with the critical instance.</p> <p>Finally, the bag representation is used to predict the bag label using a bag classifier implemented as a linear layer.</p> <p>Loss function. By default, the model is trained end-to-end using the followind per-bag loss:</p> \\[ \\ell = \\ell_{\\text{BCE}}(Y, \\hat{Y}) + \\ell_{\\text{BCE}}(Y, c(\\mathbf{x}_m)), \\] <p>where \\(\\ell_{\\text{BCE}}\\) is the Binary Cross-Entropy loss, \\(Y\\) is the true bag label, \\(\\hat{Y}\\) is the predicted bag label, and \\(c(\\mathbf{x}_m)\\) is the predicted label of the critical instance.</p>"},{"location":"api/models/dsmil/#torchmil.models.DSMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=128, n_classes=1, nonlinear_q=False, nonlinear_v=False, dropout=0.0, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_classes</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of classes.</p> </li> <li> <code>nonlinear_q</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply nonlinearity to the query.</p> </li> <li> <code>nonlinear_v</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply nonlinearity to the value.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/dsmil/#torchmil.models.DSMIL.forward","title":"<code>forward(X, mask, return_att=False, return_inst_pred=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns instance label logits in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> <li> <code>y_pred</code> (              <code>tuple[Tensor, Tensor]</code> )          \u2013            <p>Only returned when <code>return_inst_pred=True</code>. Instance label logits of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/dsmil/#torchmil.models.DSMIL.compute_loss","title":"<code>compute_loss(Y, X, mask)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/dsmil/#torchmil.models.DSMIL.predict","title":"<code>predict(X, mask, return_inst_pred=False)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/dtfdmil/","title":"DFTDMIL","text":""},{"location":"api/models/dtfdmil/#torchmil.models.DTFDMIL","title":"<code>torchmil.models.DTFDMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Double-Tier Feature Distillation Multiple Instance Learning (DFTD-MIL) model, proposed in the paper DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification.</p> <p>Overview. Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, the instances in a bag are randomly grouped in \\(M\\) pseudo-bags \\(\\{\\mathbf{X}_1, \\cdots, \\mathbf{X}_M\\}\\) with approximately the same number of instances. Each pseudo-bag is assigned its parent's bag label \\(Y_m = Y\\). Then, the model has two prediction tiers:</p> <p>In Tier 1, the model uses the attention pool (see AttentionPool for details) and a classifier, jointly noted as \\(T_1\\) to predict the label of each pseudo-bag, </p> \\[ \\widehat{Y}_m = T_1(\\mathbf{X}_m).\\] <p>The loss associated to this tier is the binary cross entropy computed using the pseudo-bag labels \\(Y_m\\) and the predicted label \\(\\widehat{Y}_m\\).</p> <p>In Tier 2, Grad-CAM (see Grad-CAM for details) is used to compute the probability of each instance. Based on that probability, a feature vector \\(\\mathbf{z}^m\\) is distilled for the \\(m\\)-th pseudo-bag. Then, the model uses another attention pool and a classifier, jointly noted as \\(T_2\\) to predict the final label of the bag,</p> \\[ \\widehat{Y} = T_2\\left( \\left[ \\mathbf{z}_1, \\ldots, \\mathbf{z}_M \\right]^\\top  \\right).\\] <p>The loss associated to this tier is the binary cross entropy computed using the bag labels \\(Y\\) and the predicted label \\(\\widehat{Y}\\).</p> <p>Loss function. By default, the model is trained end-to-end using the followind per-bag loss:</p> \\[ \\ell = \\ell_{\\text{BCE}}(Y, \\widehat{Y}) + \\frac{1}{M} \\sum_{m=1}^{M} \\ell_{\\text{BCE}}(Y_m, \\widehat{Y}_m),\\] <p>where \\(\\ell_{\\text{BCE}}\\) is the binary cross entropy loss.</p>"},{"location":"api/models/dtfdmil/#torchmil.models.DTFDMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=128, n_groups=8, distill_mode='maxmin', feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_groups</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of groups to split the bag instances.</p> </li> <li> <code>distill_mode</code>               (<code>str</code>, default:                   <code>'maxmin'</code> )           \u2013            <p>Distillation mode. Possible values: 'maxmin', 'max', 'afs'.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/dtfdmil/#torchmil.models.DTFDMIL.forward","title":"<code>forward(X, mask=None, return_pseudo_pred=False, return_inst_cam=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_pseudo_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns pseudo label logits in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_inst_cam</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns instance-level CAM values in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>inst_cam</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_inst_cam=True</code>. Instance-level CAM values of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/dtfdmil/#torchmil.models.DTFDMIL.compute_loss","title":"<code>compute_loss(Y, X, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/dtfdmil/#torchmil.models.DTFDMIL.predict","title":"<code>predict(X, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/gtp/","title":"GTP","text":""},{"location":"api/models/gtp/#torchmil.models.GTP","title":"<code>torchmil.models.GTP</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Method proposed in the paper GTP: Graph-Transformer for Whole Slide Image Classification.</p> <p>Forward pass. Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>The bags are processed using a Graph Convolutional Network (GCN) to extract high-level instance embeddings.  This GCN leverages a graph \\(\\mathbf{A}\\) constructed from the bag, where nodes correspond to patches, and edges are determined based on spatial adjacency:</p> \\[ \\mathbf{H} = \\text{GCN}(\\mathbf{X}, \\mathbf{A}) \\in \\mathbb{R}^{N \\times D}.\\] <p>To reduce the number of nodes while preserving structural relationships, a min-cut pooling operation is applied:</p> \\[ \\mathbf{X}', \\mathbf{A}' = \\text{MinCutPool}(\\mathbf{H}, \\mathbf{A}).\\] <p>The pooled graph is then passed through a Transformer encoder, where a class token is introduced:</p> \\[ \\mathbf{Z} = \\text{Transformer}([\\text{CLS}; \\mathbf{X}']) \\in \\mathbb{R}^{(N' + 1) \\times D}.\\] <p>Finally, the class token representation is used for classification:</p> \\[ \\mathbf{z} = \\mathbf{Z}_{0}, \\quad Y_{\\text{pred}} = \\text{Classifier}(\\mathbf{z}).\\] <p>Optionally, GraphCAM can be used to generate class activation maps highlighting the most relevant regions for the classification decision.</p> <p>Loss function. By default, the model is trained end-to-end using the followind per-bag loss:</p> \\[ \\ell = \\ell_{\\text{BCE}}(Y_{\\text{pred}}, Y) + \\ell_{\\text{MinCut}}(\\mathbf{X}, \\mathbf{A}) + \\ell_{\\text{Ortho}}(\\mathbf{X}, \\mathbf{A}),\\] <p>where \\(\\ell_{\\text{BCE}}\\) is the Binary Cross-Entropy loss, \\(\\ell_{\\text{MinCut}}\\) is the MinCut loss, and \\(\\ell_{\\text{Ortho}}\\) is the Orthogonality loss, computed during the min-cut pooling operation, see Dense MinCut Pooling.</p>"},{"location":"api/models/gtp/#torchmil.models.GTP.__init__","title":"<code>__init__(in_shape, att_dim=512, n_clusters=100, n_layers=1, n_heads=8, use_mlp=True, dropout=0.0, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension for transformer encoder.</p> </li> <li> <code>n_clusters</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of clusters in mincut pooling.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in transformer encoder.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads in transformer encoder.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use MLP in transformer encoder.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in transformer encoder.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/gtp/#torchmil.models.GTP.forward","title":"<code>forward(X, adj, mask=None, return_cam=False, return_loss=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_cam</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the class activation map in addition to <code>Y_logits_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>cam</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_cam=True</code>. Class activation map of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/gtp/#torchmil.models.GTP.compute_loss","title":"<code>compute_loss(Y, X, adj, mask)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/gtp/#torchmil.models.GTP.predict","title":"<code>predict(X, adj, mask, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/iibmil/","title":"IIBMIL","text":""},{"location":"api/models/iibmil/#torchmil.models.IIBMIL","title":"<code>torchmil.models.IIBMIL</code>","text":"<p>               Bases: <code>Module</code></p> <p>Integrated Instance-Level and Bag-Level Multiple Instance Learning (IIB-MIL) model, proposed in the paper IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, a TransformerEncoder is applied to transform the instance features using context information.  Subsequently, the model uses bag-level and instance-level supervision:</p> <p>Bag-level supervision: The instances are aggregated into a class token using \\(\\texttt{n_queries}\\) queries embeddings and the IIBMILDecoder. A linear layer is then applied to predict the bag label.</p> <p>Instance-level supervision: Consists of four steps.</p> <ol> <li>Using an instance classifier, obtain the probability of instance \\(i\\) belonging to class \\(c\\), denoted as \\(p_{i,c}\\).</li> <li>The prototype \\(\\mathbf{p}_{c,t} \\in \\mathbf{R}^{D}\\) of class \\(c\\) at time \\(t\\) is updated using a momentum update rule based on the set of instances with the top \\(k\\) highest probabilities of belonging to class \\(c\\). Writing \\(\\mathbf{P}_t = \\left[ \\mathbf{p}_{1,t}, \\ldots, \\mathbf{p}_{C,t}  \\right]^\\top \\in \\mathbb{R}^{C \\times D}\\), the prototype label \\(z_{i}\\) of each instance is obtained as \\(z_{i} = \\text{argmax}_{c} \\ \\mathbf{P} \\mathbf{x}_i\\).</li> <li>Compute instance-level soft labels using the prototype labels and a momentum update.</li> <li>Compute the instance-level cross-entropy loss using the soft labels and the instance classifier.</li> </ol>"},{"location":"api/models/iibmil/#torchmil.models.IIBMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=256, n_layers_encoder=1, n_layers_decoder=1, n_heads=4, n_queries=5, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_layers_encoder</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in the transformer encoder.</p> </li> <li> <code>n_layers_decoder</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in the transformer decoder.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of attention heads.</p> </li> <li> <code>n_queries</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of queries.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/iibmil/#torchmil.models.IIBMIL.forward","title":"<code>forward(X, mask, return_inst_pred=False, return_X_enc=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_X_enc</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns instance embeddings in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_inst_pred=True</code>. Instance label logits of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>X_enc</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_X_enc=True</code>. Instance embeddings of shape <code>(batch_size, bag_size, att_dim)</code>.</p> </li> </ul>"},{"location":"api/models/iibmil/#torchmil.models.IIBMIL.compute_loss","title":"<code>compute_loss(Y, X, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/iibmil/#torchmil.models.IIBMIL.predict","title":"<code>predict(X, mask=None, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/mil_model/","title":"General MIL model","text":""},{"location":"api/models/mil_model/#torchmil.models.MILModel","title":"<code>torchmil.models.MILModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for Multiple Instance Learning (MIL) models in torchmil.</p> <p>Subclasses should implement the following methods:</p> <ul> <li><code>forward</code>: Forward pass of the model. Accepts bag features (and optionally other arguments) and returns the bag label prediction (and optionally other outputs).</li> <li><code>compute_loss</code>: Compute inner losses of the model. Accepts bag features (and optionally other arguments) and returns the output of the forward method a dictionary of pairs (loss_name, loss_value). By default, the model has no inner losses, so this dictionary is empty.</li> <li><code>predict</code>: Predict bag and (optionally) instance labels. Accepts bag features (and optionally other arguments) and returns label predictions (and optionally instance label predictions).</li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the module.</p>"},{"location":"api/models/mil_model/#torchmil.models.MILModel.forward","title":"<code>forward(X, *args, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label prediction of shape <code>(batch_size,)</code>.</p> </li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModel.compute_loss","title":"<code>compute_loss(Y, X, *args, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label prediction of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss values.</p> </li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModel.predict","title":"<code>predict(X, return_inst_pred=False, *args, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label prediction of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModelWrapper","title":"<code>torchmil.models.MILModelWrapper</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>A wrapper class for MIL models in torchmil. It allows to use all models that inherit from <code>MILModel</code> using a common interface:</p> <pre><code>model_A = ... # forward accepts arguments 'X', 'adj'\nmodel_B = ... # forward accepts arguments 'X''\nmodel_A_w = MILModelWrapper(model_A)\nmodel_B_w = MILModelWrapper(model_B)\n\nbag = TensorDict({'X': ..., 'adj': ..., ...})\nY_pred_A = model_A_w(bag) # calls model_A.forward(X=bag['X'], adj=bag['adj'])\nY_pred_B = model_B_w(bag) # calls model_B.forward(X=bag['X'])\n</code></pre>"},{"location":"api/models/mil_model/#torchmil.models.MILModelWrapper.__init__","title":"<code>__init__(model)</code>","text":""},{"location":"api/models/mil_model/#torchmil.models.MILModelWrapper.forward","title":"<code>forward(bag, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>bag</code>               (<code>TensorDict</code>)           \u2013            <p>Dictionary containing one key for each argument accepted by the model's <code>forward</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>Any</code> )          \u2013            <p>Output of the model's <code>forward</code> method.</p> </li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModelWrapper.compute_loss","title":"<code>compute_loss(bag, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>bag</code>               (<code>TensorDict</code>)           \u2013            <p>Dictionary containing one key for each argument accepted by the model's <code>forward</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>tuple[Any, dict]</code> )          \u2013            <p>Output of the model's <code>compute_loss</code> method.</p> </li> </ul>"},{"location":"api/models/mil_model/#torchmil.models.MILModelWrapper.predict","title":"<code>predict(bag, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>bag</code>               (<code>TensorDict</code>)           \u2013            <p>Dictionary containing one key for each argument accepted by the model's <code>forward</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>Any</code> )          \u2013            <p>Output of the model's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/models/patch_gcn/","title":"PatchGCN","text":""},{"location":"api/models/patch_gcn/#torchmil.models.PatchGCN","title":"<code>torchmil.models.PatchGCN</code>","text":"<p>               Bases: <code>Module</code></p> <p>PatchGCN model, as proposed in Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction using Patch-based Graph Convolutional Networks.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\),  the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, a Graph Convolutional Network (GCN) and a Multi-Layer Perceptron (MLP) are used to transform the instance features, </p> \\[\\begin{gather} \\mathbf{H} = \\operatorname{GCN}(\\mathbf{X}, \\mathbf{A}) \\in \\mathbb{R}^{N \\times \\texttt{out_gcn_dim}}, \\\\ \\mathbf{H} = \\operatorname{MLP}(\\mathbf{H}) \\in \\mathbb{R}^{N \\times \\texttt{hidden_dim}}, \\end{gather}\\] <p>where \\(\\texttt{out_gcn_dim} = \\texttt{hidden_dim} \\cdot \\texttt{n_gcn_layers}\\).  These GCNs are implemented using the DeepGCN layer (see DeepGCNLayer) with GCNConv, LayerNorm, and ReLU activation (see GCNConv),  along with residual connections and dense connections.</p> <p>Then, attention values \\(\\mathbf{f} \\in \\mathbb{R}^{N \\times 1}\\) and the bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{\\texttt{hidden_dim}}\\) are computed using the attention pooling mechanism (see Attention Pooling),</p> \\[\\begin{equation} \\mathbf{z}, \\mathbf{f} = \\operatorname{AttentionPool}(\\mathbf{H}). \\end{equation}\\] <p>Finally, the bag representation \\(\\mathbf{z}\\) is fed into a classifier (one linear layer) to predict the bag label.</p>"},{"location":"api/models/patch_gcn/#torchmil.models.PatchGCN.__init__","title":"<code>__init__(in_shape=None, n_gcn_layers=4, mlp_depth=1, hidden_dim=None, att_dim=128, dropout=0.0, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>n_gcn_layers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of GCN layers.</p> </li> <li> <code>mlp_depth</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in the MLP (applied after the GCN).</p> </li> <li> <code>hidden_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Hidden dimension. If not provided, it will be set to the feature dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function.</p> </li> </ul>"},{"location":"api/models/patch_gcn/#torchmil.models.PatchGCN.forward","title":"<code>forward(X, adj, mask, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/patch_gcn/#torchmil.models.PatchGCN.compute_loss","title":"<code>compute_loss(Y, X, adj, mask)</code>","text":"<p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss</p> </li> </ul>"},{"location":"api/models/patch_gcn/#torchmil.models.PatchGCN.predict","title":"<code>predict(X, adj, mask, return_inst_pred=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns instance predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/prob_smooth_abmil/","title":"ProbSmoothABMIL","text":""},{"location":"api/models/prob_smooth_abmil/#torchmil.models.ProbSmoothABMIL","title":"<code>torchmil.models.ProbSmoothABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Attention-based Multiple Instance Learning (ABMIL) model with Probabilistic Smooth Attention Pooling. Proposed in Probabilistic Smooth Attention for Deep Multiple Instance Learning in Medical Imaging and Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection</p> <p>Overview. This model extends the ABMIL model by incorporating a probabilistic pooling mechanism.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Subsequently, it aggregates the instance features into a bag representation using a probabilistic attention-based pooling mechanism, as detailed in ProbSmoothAttentionPool.</p> <p>Specifically, it computes a mean vector \\(\\mathbf{\\mu}_{\\mathbf{f}} \\in \\mathbb{R}^N\\) and a variance vector \\(\\mathbf{\\sigma}_{\\mathbf{f}^2} \\in \\mathbb{R}^N\\) that define the attention distribution \\(q(\\mathbf{f} \\mid \\mathbf{X}) = \\mathcal{N}\\left(\\mathbf{f} \\mid \\mathbf{\\mu}_{\\mathbf{f}}, \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2) \\right)\\),</p> \\[ \\mathbf{\\mu}_{\\mathbf{f}}, \\mathbf{\\sigma}_{\\mathbf{f}} = \\operatorname{ProbSmoothAttentionPool}(\\mathbf{X}). \\] <p>If <code>covar_mode='zero'</code>, the variance vector \\(\\mathbf{\\sigma}_{\\mathbf{f}}^2\\) is set to zero, resulting in a deterministic attention distribution.</p> <p>Then, \\(m\\) attention vectors \\(\\widehat{\\mathbf{F}} = \\left[ \\widehat{\\mathbf{f}}^{(1)}, \\ldots, \\widehat{\\mathbf{f}}^{(m)} \\right]^\\top \\in \\mathbb{R}^{m \\times N}\\) are sampled from the attention distribution. The bag representation \\(\\widehat{\\mathbf{z}} \\in \\mathbb{R}^{m \\times D}\\) is then computed as:</p> \\[ \\widehat{\\mathbf{z}} = \\operatorname{Softmax}(\\widehat{\\mathbf{F}}) \\mathbf{X}. \\] <p>The bag representation \\(\\widehat{\\mathbf{z}}\\) is fed into a classifier, implemented as a linear layer, to produce bag label predictions \\(Y_{\\text{pred}} \\in \\mathbb{R}^{m}\\).</p> <p>Notably, the attention distribution naturally induces a distribution over the bag label predictions. This model thus generates multiple predictions for each bag, corresponding to different samples from this distribution.</p> <p>Regularization. The probabilistic pooling mechanism introduces a regularization term to the loss function that encourages smoothness in the attention values. Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), the regularization term corresponds to</p> \\[     \\ell_{\\text{KL}}(\\mathbf{X}, \\mathbf{A}) =          \\begin{cases}             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} \\quad &amp; \\text{if } \\texttt{covar_mode='zero'}, \\\\             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} + \\operatorname{Tr}(\\mathbf{L} \\mathbf{\\Sigma}_{\\mathbf{f}}) - \\frac{1}{2}\\log \\det( \\mathbf{\\Sigma}_{\\mathbf{f}} ) + \\operatorname{const} \\quad &amp; \\text{if } \\texttt{covar_mode='diag'}, \\\\         \\end{cases} \\] <p>where \\(\\operatorname{const}\\) is a constant term that does not depend on the parameters, \\(\\mathbf{\\Sigma}_{\\mathbf{f}} = \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2)\\), \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) is the graph Laplacian matrix, and \\(\\mathbf{D}\\) is the degree matrix of \\(\\mathbf{A}\\). This term is then averaged for all bags in the batch and added to the loss function.</p>"},{"location":"api/models/prob_smooth_abmil/#torchmil.models.ProbSmoothABMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=128, covar_mode='diag', n_samples_train=1000, n_samples_test=5000, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>covar_mode</code>               (<code>str</code>, default:                   <code>'diag'</code> )           \u2013            <p>Covariance mode for the Gaussian prior. Possible values: 'diag', 'full'.</p> </li> <li> <code>n_samples_train</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Number of samples for training.</p> </li> <li> <code>n_samples_test</code>               (<code>int</code>, default:                   <code>5000</code> )           \u2013            <p>Number of samples for testing.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/prob_smooth_abmil/#torchmil.models.ProbSmoothABMIL.forward","title":"<code>forward(X, adj=None, mask=None, return_att=False, return_samples=False, return_kl_div=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>. Only required when <code>return_kl_div=True</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_samples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and <code>return_att=True</code>, the attention values returned are samples from the attention distribution.</p> </li> <li> <code>return_kl_div</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the KL divergence between the attention distribution and the prior distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size, n_samples)</code> if <code>return_samples=True</code>, else <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape <code>(batch_size, bag_size, n_samples)</code> if <code>return_samples=True</code>, else <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>kl_div</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_kl_div=True</code>. KL divergence between the attention distribution and the prior distribution of shape <code>()</code>.</p> </li> </ul>"},{"location":"api/models/prob_smooth_abmil/#torchmil.models.ProbSmoothABMIL.compute_loss","title":"<code>compute_loss(Y, X, adj, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value and the KL divergence between the attention distribution and the prior distribution.</p> </li> </ul>"},{"location":"api/models/prob_smooth_abmil/#torchmil.models.ProbSmoothABMIL.predict","title":"<code>predict(X, mask=None, return_inst_pred=True, return_samples=False)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, returns the attention values as instance labels predictions, in addition to bag label predictions.</p> </li> <li> <code>return_samples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and <code>return_inst_pred=True</code>, the instance label predictions returned are samples from the instance label distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_inst_pred=True</code>. Attention values (before normalization) of shape <code>(batch_size, bag_size)</code> if <code>return_samples=False</code>, else <code>(batch_size, bag_size, n_samples)</code>.</p> </li> </ul>"},{"location":"api/models/setmil/","title":"SETMIL","text":""},{"location":"api/models/setmil/#torchmil.models.SETMIL","title":"<code>torchmil.models.SETMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>SETMIL: Spatial Encoding Transformer-Based Multiple Instance Learning for Pathological Image Analysis (SETMIL) model, proposed in the paper SETMIL: Spatial Encoding Transformer-Based Multiple Instance Learning for Pathological Image Analysis.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, the Pyramid Multi-Scale Fusion (PMF) module enriches the representation with multi-scale context information. The PMF module consists of three T2T modules with different kernel sizes, \\(k = 3, 5, 7\\), concatenated along the feature dimension,</p> \\[\\operatorname{PMF}\\left( \\mathbf{X} \\right) = \\text{Concat}(\\text{T2T}_{k=3}(\\mathbf{X}), \\text{T2T}_{k=5}(\\mathbf{X}), \\text{T2T}_{k=7}(\\mathbf{X})).\\] <p>See T2T and T2TLayer for further information.</p> <p>Then, the model applies a Spatial Encoding Transformer (SET), which consists of a stack of transformer layers with image Relative Positional Encoding (iRPE). See iRPETransformer for further information.</p> <p>Finally, using the class token computed by the SET module, the model predicts the bag label \\(\\hat{Y}\\) using a linear layer.</p>"},{"location":"api/models/setmil/#torchmil.models.SETMIL.__init__","title":"<code>__init__(in_shape, att_dim=512, use_pmf=False, pmf_n_heads=4, pmf_use_mlp=True, pmf_dropout=0.0, pmf_kernel_list=[(3, 3), (5, 5), (7, 7)], pmf_stride_list=[(1, 1), (1, 1), (1, 1)], pmf_padding_list=[(1, 1), (2, 2), (3, 3)], pmf_dilation_list=[(1, 1), (1, 1), (1, 1)], set_n_layers=1, set_n_heads=4, set_use_mlp=True, set_dropout=0.0, rpe_ratio=1.9, rpe_method='product', rpe_mode='ctx', rpe_shared_head=True, rpe_skip=1, rpe_on='k', feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension used by the PMF and SET modules.</p> </li> <li> <code>use_pmf</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use Pyramid Multihead Feature (PMF) before the SET module.</p> </li> <li> <code>pmf_n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the PMF module.</p> </li> <li> <code>pmf_use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, use MLP in the PMF module.</p> </li> <li> <code>pmf_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in the PMF module.</p> </li> <li> <code>pmf_kernel_list</code>               (<code>list[tuple[int, int]]</code>, default:                   <code>[(3, 3), (5, 5), (7, 7)]</code> )           \u2013            <p>List of kernel sizes in the PMF module.</p> </li> <li> <code>pmf_stride_list</code>               (<code>list[tuple[int, int]]</code>, default:                   <code>[(1, 1), (1, 1), (1, 1)]</code> )           \u2013            <p>List of stride sizes in the PMF module.</p> </li> <li> <code>pmf_padding_list</code>               (<code>list[tuple[int, int]]</code>, default:                   <code>[(1, 1), (2, 2), (3, 3)]</code> )           \u2013            <p>List of padding sizes in the PMF module.</p> </li> <li> <code>pmf_dilation_list</code>               (<code>list[tuple[int, int]]</code>, default:                   <code>[(1, 1), (1, 1), (1, 1)]</code> )           \u2013            <p>List of dilation sizes in the PMF module.</p> </li> <li> <code>set_n_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in the SET module.</p> </li> <li> <code>set_n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the SET module.</p> </li> <li> <code>set_use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, use MLP in the SET module.</p> </li> <li> <code>set_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in the SET module.</p> </li> <li> <code>rpe_ratio</code>               (<code>float</code>, default:                   <code>1.9</code> )           \u2013            <p>Ratio for relative positional encoding.</p> </li> <li> <code>rpe_method</code>               (<code>str</code>, default:                   <code>'product'</code> )           \u2013            <p>Method for relative positional encoding. Possible values: 'product', 'concat'.</p> </li> <li> <code>rpe_mode</code>               (<code>str</code>, default:                   <code>'ctx'</code> )           \u2013            <p>Mode for relative positional encoding. Possible values: 'ctx', 'ctx+pos'.</p> </li> <li> <code>rpe_shared_head</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, share the attention head in the relative positional encoding.</p> </li> <li> <code>rpe_skip</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers to skip for relative positional encoding.</p> </li> <li> <code>rpe_on</code>               (<code>str</code>, default:                   <code>'k'</code> )           \u2013            <p>Apply relative positional encoding on 'q', 'k', or 'v'.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/setmil/#torchmil.models.SETMIL.forward","title":"<code>forward(X, coords, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>Coordinates of shape <code>(batch_size, bag_size, coord_dim)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/setmil/#torchmil.models.SETMIL.compute_loss","title":"<code>compute_loss(Y, X, coords)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>Coordinates of shape <code>(batch_size, bag_size, coord_dim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/setmil/#torchmil.models.SETMIL.predict","title":"<code>predict(X, coords, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>Coordinates of shape <code>(batch_size, bag_size, coord_dim)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/sm_abmil/","title":"SmABMIL","text":""},{"location":"api/models/sm_abmil/#torchmil.models.SmABMIL","title":"<code>torchmil.models.SmABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Attention-based Multiple Instance Learning (ABMIL) model with the \\(\\texttt{Sm}\\) operator. Proposed in Sm: enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, it aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) using an attention-based pooling mechanism that incorporates the \\(\\texttt{Sm}\\) operator,</p> \\[ \\mathbf{z}, \\mathbf{f} = \\operatorname{SmAttentionPool}(\\mathbf{X}), \\] <p>where \\(\\mathbf{f} \\in \\mathbb{R}^{N}\\) are the attention values. See SmAttentionPool for more details on the pooling operator The bag representation \\(\\mathbf{z}\\) is then fed into a classifier (one linear layer) to predict the bag label.</p>"},{"location":"api/models/sm_abmil/#torchmil.models.SmABMIL.__init__","title":"<code>__init__(in_shape=None, att_dim=128, att_act='tanh', sm_mode='approx', sm_alpha='trainable', sm_layers=1, sm_steps=10, sm_pre=False, sm_post=False, sm_spectral_norm=False, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>att_act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for attention. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>sm_mode</code>               (<code>str</code>, default:                   <code>'approx'</code> )           \u2013            <p>Mode for the Sm operator. Possible values: 'approx', 'exact'.</p> </li> <li> <code>sm_alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator. If 'trainable', alpha is trainable.</p> </li> <li> <code>sm_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers that use the Sm operator.</p> </li> <li> <code>sm_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps for the Sm operator.</p> </li> <li> <code>sm_pre</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator before the attention pooling.</p> </li> <li> <code>sm_post</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator after the attention pooling.</p> </li> <li> <code>sm_spectral_norm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply spectral normalization to all linear layers.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/sm_abmil/#torchmil.models.SmABMIL.forward","title":"<code>forward(X, adj, mask, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/sm_abmil/#torchmil.models.SmABMIL.compute_loss","title":"<code>compute_loss(Y, X, adj, mask)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/sm_abmil/#torchmil.models.SmABMIL.predict","title":"<code>predict(X, adj, mask, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/sm_transformer_abmil/","title":"SmTransformerABMIL","text":""},{"location":"api/models/sm_transformer_abmil/#torchmil.models.SmTransformerABMIL","title":"<code>torchmil.models.SmTransformerABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Transformer Attention-based Multiple Instance Learning model with the \\(\\texttt{Sm}\\) operator. Proposed in Sm: enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, it transforms the instance features using a transformer encoder with the \\(\\texttt{Sm}\\) operator,</p> \\[ \\mathbf{X} = \\text{SmTransformerEncoder}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}. \\] <p>Subsequently, it aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) using an attention-based pooling mechanism that incorporates the \\(\\texttt{Sm}\\) operator,</p> \\[ \\mathbf{z}, \\mathbf{f} = \\operatorname{SmAttentionPool}(\\mathbf{X}). \\] <p>where \\(\\mathbf{f} \\in \\mathbb{R}^{N}\\) are the attention values. Finally, the bag representation \\(\\mathbf{z}\\) is then fed into a classifier (one linear layer) to predict the bag label.</p> <p>See SmAttentionPool for more details on the attention-based pooling, and SmTransformerEncoder for more details on the transformer encoder.</p>"},{"location":"api/models/sm_transformer_abmil/#torchmil.models.SmTransformerABMIL.__init__","title":"<code>__init__(in_shape, pool_att_dim=128, pool_act='tanh', pool_sm_mode='approx', pool_sm_alpha='trainable', pool_sm_layers=1, pool_sm_steps=10, pool_sm_pre=False, pool_sm_post=False, pool_sm_spectral_norm=False, feat_ext=torch.nn.Identity(), transf_att_dim=512, transf_n_layers=1, transf_n_heads=4, transf_use_mlp=True, transf_add_self=True, transf_dropout=0.0, transf_sm_alpha=None, transf_sm_mode=None, transf_sm_steps=10, criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>pool_att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension for pooling.</p> </li> <li> <code>pool_act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for pooling. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>pool_sm_mode</code>               (<code>str</code>, default:                   <code>'approx'</code> )           \u2013            <p>Mode for the Sm operator in pooling. Possible values: 'approx', 'exact'.</p> </li> <li> <code>pool_sm_alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator in pooling. If 'trainable', alpha is trainable.</p> </li> <li> <code>pool_sm_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers that use the Sm operator in pooling.</p> </li> <li> <code>pool_sm_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps for the Sm operator in pooling.</p> </li> <li> <code>pool_sm_pre</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator before the attention pooling.</p> </li> <li> <code>pool_sm_post</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator after the attention pooling.</p> </li> <li> <code>pool_sm_spectral_norm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply spectral normalization to all linear layers in pooling.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>transf_att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension for transformer encoder.</p> </li> <li> <code>transf_n_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in transformer encoder.</p> </li> <li> <code>transf_n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in transformer encoder.</p> </li> <li> <code>transf_use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use MLP in transformer encoder.</p> </li> <li> <code>transf_add_self</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add input to output in transformer encoder.</p> </li> <li> <code>transf_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in transformer encoder.</p> </li> <li> <code>transf_sm_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Alpha value for the Sm operator in transformer encoder.</p> </li> <li> <code>transf_sm_mode</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Mode for the Sm operator in transformer encoder.</p> </li> <li> <code>transf_sm_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps for the Sm operator in transformer encoder.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/sm_transformer_abmil/#torchmil.models.SmTransformerABMIL.forward","title":"<code>forward(X, adj, mask, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/sm_transformer_abmil/#torchmil.models.SmTransformerABMIL.compute_loss","title":"<code>compute_loss(Y, X, adj, mask)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/sm_transformer_abmil/#torchmil.models.SmTransformerABMIL.predict","title":"<code>predict(X, adj, mask, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/transformer_abmil/","title":"TransformerABMIL","text":""},{"location":"api/models/transformer_abmil/#torchmil.models.TransformerABMIL","title":"<code>torchmil.models.TransformerABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Transformer Attention-based Multiple Instance Learning model.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\). Then, it transforms the instance features using a transformer encoder, </p> \\[ \\mathbf{X} = \\text{TransformerEncoder}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}, \\] <p>and finally it aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) using the attention-based pooling, </p> \\[ \\mathbf{z}, \\mathbf{f} = \\operatorname{AttentionPool}(\\mathbf{X}). \\] <p>where \\(\\mathbf{f} = \\operatorname{MLP}(\\mathbf{X}) \\in \\mathbb{R}^{N}\\) are the attention values. The bag representation \\(\\mathbf{z}\\) is then fed into a classifier (one linear layer) to predict the bag label.</p> <p>See AttentionPool for more details on the attention-based pooling, and TransformerEncoder for more details on the transformer encoder.</p>"},{"location":"api/models/transformer_abmil/#torchmil.models.TransformerABMIL.__init__","title":"<code>__init__(in_shape, pool_att_dim=128, pool_act='tanh', pool_gated=False, feat_ext=torch.nn.Identity(), transf_att_dim=512, transf_n_layers=1, transf_n_heads=8, transf_use_mlp=True, transf_add_self=True, transf_dropout=0.0, criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>pool_att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension for pooling.</p> </li> <li> <code>pool_act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for pooling. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>pool_gated</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use gated attention in the attention pooling.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>transf_att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension for transformer encoder.</p> </li> <li> <code>transf_n_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in transformer encoder.</p> </li> <li> <code>transf_n_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads in transformer encoder.</p> </li> <li> <code>transf_use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use MLP in transformer encoder.</p> </li> <li> <code>transf_add_self</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add input to output in transformer encoder.</p> </li> <li> <code>transf_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in transformer encoder.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/transformer_abmil/#torchmil.models.TransformerABMIL.forward","title":"<code>forward(X, mask, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_logits_pred</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/transformer_abmil/#torchmil.models.TransformerABMIL.compute_loss","title":"<code>compute_loss(Y, X, mask)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/transformer_abmil/#torchmil.models.TransformerABMIL.predict","title":"<code>predict(X, mask, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/models/transformer_prob_smooth_abmil/","title":"TransformerProbSmoothABMIL","text":""},{"location":"api/models/transformer_prob_smooth_abmil/#torchmil.models.TransformerProbSmoothABMIL","title":"<code>torchmil.models.TransformerProbSmoothABMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Transformer Attention-based Multiple Instance Learning model, with probabilistic attention-based pooling. Proposed in Probabilistic Smooth Attention for Deep Multiple Instance Learning in Medical Imaging.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\). Then, it transforms the instance features using a transformer encoder, </p> \\[ \\mathbf{X} = \\text{TransformerEncoder}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}. \\] <p>Subsequently, it aggregates the instance features into a bag representation using a probabilistic attention-based pooling mechanism, as detailed in ProbSmoothAttentionPool.</p> <p>Specifically, it computes a mean vector \\(\\mathbf{\\mu}_{\\mathbf{f}} \\in \\mathbb{R}^N\\) and a variance vector \\(\\mathbf{\\sigma}_{\\mathbf{f}^2} \\in \\mathbb{R}^N\\) that define the attention distribution \\(q(\\mathbf{f} \\mid \\mathbf{X}) = \\mathcal{N}\\left(\\mathbf{f} \\mid \\mathbf{\\mu}_{\\mathbf{f}}, \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2) \\right)\\),</p> \\[ \\mathbf{\\mu}_{\\mathbf{f}}, \\mathbf{\\sigma}_{\\mathbf{f}} = \\operatorname{ProbSmoothAttentionPool}(\\mathbf{X}). \\] <p>If <code>covar_mode='zero'</code>, the variance vector \\(\\mathbf{\\sigma}_{\\mathbf{f}}^2\\) is set to zero, resulting in a deterministic attention distribution.</p> <p>Then, \\(m\\) attention vectors \\(\\widehat{\\mathbf{F}} = \\left[ \\widehat{\\mathbf{f}}^{(1)}, \\ldots, \\widehat{\\mathbf{f}}^{(m)} \\right]^\\top \\in \\mathbb{R}^{m \\times N}\\) are sampled from the attention distribution. The bag representation \\(\\widehat{\\mathbf{z}} \\in \\mathbb{R}^{m \\times D}\\) is then computed as:</p> \\[ \\widehat{\\mathbf{z}} = \\operatorname{Softmax}(\\widehat{\\mathbf{F}}) \\mathbf{X}. \\] <p>The bag representation \\(\\widehat{\\mathbf{z}}\\) is fed into a classifier, implemented as a linear layer, to produce bag label predictions \\(Y_{\\text{pred}} \\in \\mathbb{R}^{m}\\).</p> <p>Notably, the attention distribution naturally induces a distribution over the bag label predictions. This model thus generates multiple predictions for each bag, corresponding to different samples from this distribution.</p> <p>Regularization. The probabilistic pooling mechanism introduces a regularization term to the loss function that encourages smoothness in the attention values. Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\) with adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), the regularization term corresponds to</p> \\[     \\ell_{\\text{KL}}(\\mathbf{X}, \\mathbf{A}) =          \\begin{cases}             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} \\quad &amp; \\text{if } \\texttt{covar_mode='zero'}, \\\\             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} + \\operatorname{Tr}(\\mathbf{L} \\mathbf{\\Sigma}_{\\mathbf{f}}) - \\frac{1}{2}\\log \\det( \\mathbf{\\Sigma}_{\\mathbf{f}} ) + \\operatorname{const} \\quad &amp; \\text{if } \\texttt{covar_mode='diag'}, \\\\         \\end{cases} \\] <p>where \\(\\operatorname{const}\\) is a constant term that does not depend on the parameters, \\(\\mathbf{\\Sigma}_{\\mathbf{f}} = \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2)\\), \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) is the graph Laplacian matrix, and \\(\\mathbf{D}\\) is the degree matrix of \\(\\mathbf{A}\\). This term is then averaged for all bags in the batch and added to the loss function.</p>"},{"location":"api/models/transformer_prob_smooth_abmil/#torchmil.models.TransformerProbSmoothABMIL.__init__","title":"<code>__init__(in_shape=None, pool_att_dim=128, covar_mode='diag', n_samples_train=1000, n_samples_test=5000, feat_ext=torch.nn.Identity(), transf_att_dim=512, transf_n_layers=1, transf_n_heads=8, transf_use_mlp=True, transf_add_self=True, transf_dropout=0.0, criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension). If not provided, it will be lazily initialized.</p> </li> <li> <code>pool_att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>covar_mode</code>               (<code>str</code>, default:                   <code>'diag'</code> )           \u2013            <p>Covariance mode for the Gaussian prior. Possible values: 'diag', 'full'.</p> </li> <li> <code>n_samples_train</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Number of samples for training.</p> </li> <li> <code>n_samples_test</code>               (<code>int</code>, default:                   <code>5000</code> )           \u2013            <p>Number of samples for testing.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor.</p> </li> <li> <code>transf_att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension for transformer encoder.</p> </li> <li> <code>transf_n_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers in transformer encoder.</p> </li> <li> <code>transf_n_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads in transformer encoder.</p> </li> <li> <code>transf_use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use MLP in transformer encoder.</p> </li> <li> <code>transf_add_self</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add input to output in transformer encoder.</p> </li> <li> <code>transf_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in transformer encoder.</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits for binary classification.</p> </li> </ul>"},{"location":"api/models/transformer_prob_smooth_abmil/#torchmil.models.TransformerProbSmoothABMIL.forward","title":"<code>forward(X, adj, mask=None, return_att=False, return_samples=False, return_kl_div=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>. Only required when <code>return_kl_div=True</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>Y_pred</code>.</p> </li> <li> <code>return_samples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and <code>return_att=True</code>, the attention values returned are samples from the attention distribution.</p> </li> <li> <code>return_kl_div</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the KL divergence between the attention distribution and the prior distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size, n_samples)</code> if <code>return_samples=True</code>, else <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape <code>(batch_size, bag_size, n_samples)</code> if <code>return_samples=True</code>, else <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>kl_div</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_kl_div=True</code>. KL divergence between the attention distribution and the prior distribution of shape <code>()</code>.</p> </li> </ul>"},{"location":"api/models/transformer_prob_smooth_abmil/#torchmil.models.TransformerProbSmoothABMIL.compute_loss","title":"<code>compute_loss(Y, X, adj, mask=None)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value and the KL divergence between the attention distribution and the prior distribution.</p> </li> </ul>"},{"location":"api/models/transformer_prob_smooth_abmil/#torchmil.models.TransformerProbSmoothABMIL.predict","title":"<code>predict(X, adj, mask=None, return_inst_pred=True, return_samples=False)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, returns the attention values as instance labels predictions, in addition to bag label predictions.</p> </li> <li> <code>return_samples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and <code>return_inst_pred=True</code>, the instance label predictions returned are samples from the instance label distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_inst_pred=True</code>. Attention values (before normalization) of shape <code>(batch_size, bag_size)</code> if <code>return_samples=False</code>, else <code>(batch_size, bag_size, n_samples)</code>.</p> </li> </ul>"},{"location":"api/models/transmil/","title":"TransMIL","text":""},{"location":"api/models/transmil/#torchmil.models.TransMIL","title":"<code>torchmil.models.TransMIL</code>","text":"<p>               Bases: <code>MILModel</code></p> <p>Method proposed in the paper TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times P}\\), the model optionally applies a feature extractor, \\(\\text{FeatExt}(\\cdot)\\), to transform the instance features: \\(\\mathbf{X} = \\text{FeatExt}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times D}\\).</p> <p>Then, following Algorithm 2 in the paper, it performs sequence squaring, adds a class token, and applies the novel TPT module. This module consists of two Nystr\u00f6mformer layers and the novel PPEG (Pyramid Positional Encoding Generator) layer.</p> <p>Finally, a linear classifier is used to predict the bag label from the class token.</p>"},{"location":"api/models/transmil/#torchmil.models.TransMIL.__init__","title":"<code>__init__(in_shape, att_dim=512, n_layers=2, n_heads=4, n_landmarks=None, pinv_iterations=6, dropout=0.0, use_mlp=False, feat_ext=torch.nn.Identity(), criterion=torch.nn.BCEWithLogitsLoss())</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of input data expected by the feature extractor (excluding batch dimension).</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Embedding dimension.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Number of Nystr\u00f6mformer layers.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the Nystr\u00f6mformer layer.</p> </li> <li> <code>n_landmarks</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of landmarks in the Nystr\u00f6mformer layer.</p> </li> <li> <code>pinv_iterations</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of iterations for the pseudo-inverse in the Nystr\u00f6mformer layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate in the Nystr\u00f6mformer layer.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a MLP after the Nystr\u00f6mformer layer.</p> </li> <li> <code>feat_ext</code>               (<code>Module</code>, default:                   <code>Identity()</code> )           \u2013            <p>Feature extractor. By default, the identity function (no feature extraction).</p> </li> <li> <code>criterion</code>               (<code>Module</code>, default:                   <code>BCEWithLogitsLoss()</code> )           \u2013            <p>Loss function. By default, Binary Cross-Entropy loss from logits.</p> </li> </ul>"},{"location":"api/models/transmil/#torchmil.models.TransMIL.forward","title":"<code>forward(X, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the attention values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/models/transmil/#torchmil.models.TransMIL.compute_loss","title":"<code>compute_loss(Y, X)</code>","text":"<p>Compute loss given true bag labels.</p> <p>Parameters:</p> <ul> <li> <code>Y</code>               (<code>Tensor</code>)           \u2013            <p>Bag labels of shape <code>(batch_size,)</code>.</p> </li> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>loss_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the loss value.</p> </li> </ul>"},{"location":"api/models/transmil/#torchmil.models.TransMIL.predict","title":"<code>predict(X, return_inst_pred=True)</code>","text":"<p>Predict bag and (optionally) instance labels.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>return_inst_pred</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, returns instance labels predictions, in addition to bag label predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y_pred</code> (              <code>Tensor</code> )          \u2013            <p>Bag label logits of shape <code>(batch_size,)</code>.</p> </li> <li> <code>y_inst_pred</code> (              <code>Tensor</code> )          \u2013            <p>If <code>return_inst_pred=True</code>, returns instance labels predictions of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/nn/","title":"torchmil.nn","text":""},{"location":"api/nn/#available-modules","title":"Available modules","text":"<ul> <li>Attention<ul> <li>Attention Pool</li> <li>Sm Attention Pool</li> <li>Probabilistic Smooth Attention Pool</li> <li>Multihead Self-Attention</li> <li>Multihead Self-Attention with Relative Positional Encoding (iRPE)</li> <li>Nystrom Attention</li> <li>Multihead Cross-Attention</li> </ul> </li> <li>Graph Neural Networks (GNNs)<ul> <li>Deep Graph Convolutional Network (DeepGCN) layer</li> <li>Graph Convolutional Network (GCN) convolution</li> <li>Dense MinCut pooling</li> </ul> </li> <li>Transformers<ul> <li>Transformer base class</li> <li>Conventional Transformer</li> <li>Sm Transformer</li> <li>Nystrom Transformer</li> <li>Transformer with image Relative Positional Encoding (iRPE)</li> <li>Tokens-2-Token</li> </ul> </li> <li>Sm operator</li> <li>Max Pool</li> <li>Mean Pool</li> </ul>"},{"location":"api/nn/max_pool/","title":"Max Pool","text":""},{"location":"api/nn/max_pool/#torchmil.nn.MaxPool","title":"<code>torchmil.nn.MaxPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Max pooling aggregation. </p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this model aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) as,</p> \\[ \\left[ \\mathbf{z} \\right]_d = \\max \\left\\{ \\left[ \\mathbf{x}_n \\right]_{d} \\ \\colon n \\in \\left\\{ 1, \\ldots, N \\right\\}  \\right\\}, \\] <p>where \\(\\left[ \\mathbf{a} \\right]_i\\) denotes the \\(i\\)-th element of the vector \\(\\mathbf{a}\\).</p>"},{"location":"api/nn/max_pool/#torchmil.nn.MaxPool.__init__","title":"<code>__init__()</code>","text":""},{"location":"api/nn/max_pool/#torchmil.nn.MaxPool.forward","title":"<code>forward(X, mask=None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/mean_pool/","title":"Mean Pool","text":""},{"location":"api/nn/mean_pool/#torchmil.nn.MeanPool","title":"<code>torchmil.nn.MeanPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Mean pooling aggregation. </p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this model aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{D}\\) as,</p> \\[     \\mathbf{z} = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_n. \\]"},{"location":"api/nn/mean_pool/#torchmil.nn.MeanPool.__init__","title":"<code>__init__()</code>","text":""},{"location":"api/nn/mean_pool/#torchmil.nn.MeanPool.forward","title":"<code>forward(X, mask=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:     z: Output tensor of shape <code>(batch_size, in_dim)</code>.</p>"},{"location":"api/nn/sm/","title":"Sm operator","text":""},{"location":"api/nn/sm/#torchmil.nn.Sm","title":"<code>torchmil.nn.Sm</code>","text":"<p>               Bases: <code>Module</code></p> <p>The \\(\\texttt{Sm}\\) operator, proposed in the paper \\(\\texttt{Sm}\\): enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input graph with node features \\(\\mathbf{U} \\in \\mathbb{R}^{N \\times D}\\) and adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), in the exact mode the \\(\\texttt{Sm}\\) operator is defined as:</p> \\[\\begin{align}     \\texttt{Sm}(\\mathbf{U}) = ( \\mathbf{I} + \\gamma \\mathbf{L} )^{-1} \\mathbf{U}, \\end{align}\\] <p>where \\(\\gamma \\in (0, \\infty)\\) is a hyperparameter, \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) is the graph Laplacian, and \\(\\mathbf{D}\\) is the degree matrix. If <code>mode='approx'</code>, the \\(\\texttt{Sm}\\) operator is approximated as \\(\\texttt{Sm}(\\mathbf{U}) = G(T)\\), where </p> \\[\\begin{align}     G(0) = \\mathbf{U}, \\quad G(t) = \\alpha ( \\mathbf{I} - \\mathbf{L} ) G(t-1) + (1-\\alpha) \\mathbf{U}, \\end{align}\\] <p>for \\(t \\in \\{1, \\ldots, T\\}\\), and \\(\\alpha \\in (0, 1)\\) is a hyperparameter.</p>"},{"location":"api/nn/sm/#torchmil.nn.Sm.__init__","title":"<code>__init__(alpha='trainable', num_steps=10, mode='approx')</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator. If 'trainable', alpha is a trainable parameter.</p> </li> <li> <code>num_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps to approximate the exact Sm operator.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'approx'</code> )           \u2013            <p>Mode of the Sm operator. Possible values: 'approx', 'exact'.</p> </li> </ul>"},{"location":"api/nn/sm/#torchmil.nn.Sm.forward","title":"<code>forward(f, adj_mat)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj_mat</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix tensor of shape <code>(batch_size, bag_size, bag_size)</code>. Sparse tensor is supported.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>g</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul>"},{"location":"api/nn/sm/#torchmil.nn.ApproxSm","title":"<code>torchmil.nn.ApproxSm</code>","text":"<p>               Bases: <code>Module</code></p> <p>\\(\\texttt{Sm}\\) operator in the approximate mode, proposed in the paper \\(\\texttt{Sm}\\): enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input graph with node features \\(\\mathbf{U} \\in \\mathbb{R}^{N \\times D}\\) and adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), it computes \\(\\texttt{Sm}(\\mathbf{U}) = G(T)\\), where </p> \\[\\begin{align}     G(0) = \\mathbf{U}, \\quad G(t) = \\alpha ( \\mathbf{I} - \\mathbf{L} ) G(t-1) + (1-\\alpha) \\mathbf{U}, \\end{align}\\] <p>for \\(t \\in \\{1, \\ldots, T\\}\\), and \\(\\alpha \\in (0, 1)\\) is a hyperparameter.</p>"},{"location":"api/nn/sm/#torchmil.nn.ApproxSm.__init__","title":"<code>__init__(alpha='trainable', num_steps=10)</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator. If 'trainable', alpha is a trainable parameter.</p> </li> <li> <code>num_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps to approximate the exact Sm operator.</p> </li> </ul>"},{"location":"api/nn/sm/#torchmil.nn.ApproxSm.forward","title":"<code>forward(f, adj_mat)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj_mat</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix tensor of shape <code>(batch_size, bag_size, bag_size)</code>. Sparse tensor is supported.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>g</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul>"},{"location":"api/nn/sm/#torchmil.nn.ExactSm","title":"<code>torchmil.nn.ExactSm</code>","text":"<p>               Bases: <code>Module</code></p> <p>\\(\\texttt{Sm}\\) operator in the exact mode, proposed in the paper \\(\\texttt{Sm}\\): enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input graph with node features \\(\\mathbf{U} \\in \\mathbb{R}^{N \\times D}\\) and adjacency matrix \\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\), it computes </p> \\[\\begin{align}     \\texttt{Sm}(\\mathbf{U}) = ( \\mathbf{I} + \\gamma \\mathbf{L} )^{-1} \\mathbf{U}, \\end{align}\\] <p>where \\(\\gamma \\in (0, \\infty)\\) is a hyperparameter, \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) is the graph Laplacian, and \\(\\mathbf{D}\\) is the degree matrix.</p>"},{"location":"api/nn/sm/#torchmil.nn.ExactSm.__init__","title":"<code>__init__(alpha='trainable')</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator. If 'trainable', alpha is a trainable parameter.</p> </li> </ul>"},{"location":"api/nn/sm/#torchmil.nn.ExactSm.forward","title":"<code>forward(f, adj_mat)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> <li> <code>adj_mat</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix tensor of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>g</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, ...)</code>.</p> </li> </ul>"},{"location":"api/nn/attention/","title":"torchmil.nn.attention","text":"<ul> <li>Attention Pool</li> <li>Sm Attention Pool</li> <li>Probabilistic Smooth Attention Pool</li> <li>Multihead Self-Attention</li> <li>Multihead Self-Attention with Relative Positional Encoding (iRPE)</li> <li>Nystrom Attention</li> <li>Multihead Cross-Attention</li> </ul>"},{"location":"api/nn/attention/attention_pool/","title":"Attention Pool","text":""},{"location":"api/nn/attention/attention_pool/#torchmil.nn.attention.AttentionPool","title":"<code>torchmil.nn.attention.AttentionPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Attention-based pooling, as proposed in the paper Attention-based Multiple Instance Learning.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times \\texttt{in_dim}}\\),  this model aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{\\texttt{in_dim}}\\) as, </p> \\[ \\mathbf{z} = \\mathbf{X}^\\top \\operatorname{Softmax}(\\mathbf{f}) = \\sum_{n=1}^N s_n \\mathbf{x}_n, \\] <p>where \\(\\mathbf{f} = \\operatorname{MLP}(\\mathbf{X}) \\in \\mathbb{R}^{N}\\) are the attention values and \\(s_n\\) is the normalized attention score for the \\(n\\)-th instance.</p> <p>To compute the attention values, the \\(\\operatorname{MLP}\\) is defined as </p> \\[\\begin{equation} \\operatorname{MLP}(\\mathbf{X}) = \\begin{cases} \\operatorname{act}(\\mathbf{X}\\mathbf{W}_1)\\mathbf{w}, &amp; \\text{if }\\texttt{gated=False}, \\\\ \\left(\\operatorname{act}(\\mathbf{X}\\mathbf{W}_1)\\odot\\operatorname{sigm}(\\mathbf{X}\\mathbf{W}_2)\\right)\\mathbf{w}, &amp; \\text{if }\\texttt{gated=True},     \\end{cases} \\end{equation}\\] <p>where \\(\\mathbf{W}_1 \\in \\mathbb{R}^{\\texttt{in_dim} \\times \\texttt{att_dim}}\\), \\(\\mathbf{W}_2 \\in \\mathbb{R}^{\\texttt{in_dim} \\times \\texttt{att_dim}}\\), \\(\\mathbf{w} \\in \\mathbb{R}^{\\texttt{att_dim}}\\), \\(\\operatorname{act} \\ \\colon \\mathbb{R} \\to \\mathbb{R}\\) is the activation function, \\(\\operatorname{sigm} \\ \\colon \\mathbb{R} \\to \\left] 0, 1 \\right[\\) is the sigmoid function, and \\(\\odot\\) denotes element-wise multiplication.</p>"},{"location":"api/nn/attention/attention_pool/#torchmil.nn.attention.AttentionPool.__init__","title":"<code>__init__(in_dim=None, att_dim=128, act='tanh', gated=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Input dimension. If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>act</code>               (<code>str</code>, default:                   <code>'tanh'</code> )           \u2013            <p>Activation function for attention. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>gated</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use gated attention.</p> </li> </ul>"},{"location":"api/nn/attention/attention_pool/#torchmil.nn.attention.AttentionPool.forward","title":"<code>forward(X, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>z</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>Tensor</code> )          \u2013            <p>Bag representation of shape <code>(batch_size, in_dim)</code>.</p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/nn/attention/irpe_multihead_self_attention/","title":"iRPE Multihead Self-Attention","text":""},{"location":"api/nn/attention/irpe_multihead_self_attention/#torchmil.nn.attention.iRPEMultiheadSelfAttention","title":"<code>torchmil.nn.attention.iRPEMultiheadSelfAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multihead Self-Attention with image Relative Position Encoding (iRPE), as described in Rethinking and Improving Relative Position Encoding for Vision Transformer.</p> <p>The iRPE implementation is based on the official codebase.</p>"},{"location":"api/nn/attention/irpe_multihead_self_attention/#torchmil.nn.attention.iRPEMultiheadSelfAttention.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, dropout=0.0, learn_weights=True, rpe_ratio=1.9, rpe_method='product', rpe_mode='ctx', rpe_shared_head=True, rpe_skip=1, rpe_on='k')</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>learn_weights</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to learn the query, key, and value weights.</p> </li> <li> <code>rpe_ratio</code>               (<code>float</code>, default:                   <code>1.9</code> )           \u2013            <p>Relative position encoding ratio.</p> </li> <li> <code>rpe_method</code>               (<code>str</code>, default:                   <code>'product'</code> )           \u2013            <p>Relative position encoding method.</p> </li> <li> <code>rpe_mode</code>               (<code>str</code>, default:                   <code>'ctx'</code> )           \u2013            <p>Relative position encoding mode.</p> </li> <li> <code>rpe_shared_head</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to share relative position encoding weights across heads.</p> </li> <li> <code>rpe_skip</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Relative position encoding skip.</p> </li> <li> <code>rpe_on</code>               (<code>str</code>, default:                   <code>'k'</code> )           \u2013            <p>Relative position encoding on query, key, or value.</p> </li> </ul>"},{"location":"api/nn/attention/irpe_multihead_self_attention/#torchmil.nn.attention.iRPEMultiheadSelfAttention.forward","title":"<code>forward(x, mask=None, return_attention=False, height=None, width=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, seq_len)</code>.</p> </li> <li> <code>height</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Height of the input sequence. If None, <code>height = floor(sqrt(seq_len))</code>.</p> </li> <li> <code>width</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Width of the input sequence. If None, <code>width = floor(sqrt(seq_len))</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, seq_len, att_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/attention/multihead_cross_attention/","title":"Multihead Cross-Attention","text":""},{"location":"api/nn/attention/multihead_cross_attention/#torchmil.nn.attention.MultiheadCrossAttention","title":"<code>torchmil.nn.attention.MultiheadCrossAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>The Multihead Cross Attention module, as described in Attention is All You Need. </p> <p>Given input bags \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times \\texttt{in_dim}}\\), and \\(\\mathbf{Y} = \\left[ \\mathbf{y}_1, \\ldots, \\mathbf{y}_M \\right]^\\top \\in \\mathbb{R}^{M \\times \\texttt{in_dim}}\\), this module computes:</p> \\[\\begin{gather*} \\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{Y}\\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{Y}\\mathbf{W}_V,\\\\ \\mathbf{Z} = \\operatorname{Softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d}} \\right) \\mathbf{V}, \\end{gather*}\\] <p>where \\(d = \\texttt{att_dim}\\) and \\(\\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V \\in \\mathbb{R}^{\\texttt{in_dim} \\times \\texttt{att_dim}}\\) are learnable weight matrices.</p> <p>If \\(\\texttt{out_dim} \\neq \\texttt{att_dim}\\), \\(\\mathbf{Y}\\) is passed through a linear layer with output dimension \\(\\texttt{out_dim}\\).</p>"},{"location":"api/nn/attention/multihead_cross_attention/#torchmil.nn.attention.MultiheadCrossAttention.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, dropout=0.0, learn_weights=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>learn_weights</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to learn the weights.</p> </li> </ul>"},{"location":"api/nn/attention/multihead_cross_attention/#torchmil.nn.attention.MultiheadCrossAttention.forward","title":"<code>forward(x, y, mask=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len_x, in_dim)</code>.</p> </li> <li> <code>y</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len_y, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, seq_len)</code>.</p> </li> </ul> <p>Returns:     y: Output tensor of shape <code>(batch_size, seq_len_y, att_dim)</code>.</p>"},{"location":"api/nn/attention/multihead_self_attention/","title":"Multihead Self-Attention","text":""},{"location":"api/nn/attention/multihead_self_attention/#torchmil.nn.attention.MultiheadSelfAttention","title":"<code>torchmil.nn.attention.MultiheadSelfAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>The Multihead Self Attention module, as described in Attention is All You Need. </p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times \\texttt{in_dim}}\\), this module computes:</p> \\[\\begin{gather*} \\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{X}\\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{X}\\mathbf{W}_V,\\\\ \\mathbf{Y} = \\operatorname{Softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d}} \\right) \\mathbf{V}, \\end{gather*}\\] <p>where \\(d = \\texttt{att_dim}\\) and \\(\\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V \\in \\mathbb{R}^{\\texttt{in_dim} \\times \\texttt{att_dim}}\\) are learnable weight matrices.</p> <p>If \\(\\texttt{out_dim} \\neq \\texttt{att_dim}\\), \\(\\mathbf{Y}\\) is passed through a linear layer with output dimension \\(\\texttt{out_dim}\\).</p>"},{"location":"api/nn/attention/multihead_self_attention/#torchmil.nn.attention.MultiheadSelfAttention.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, dropout=0.0, learn_weights=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/attention/multihead_self_attention/#torchmil.nn.attention.MultiheadSelfAttention.forward","title":"<code>forward(x, mask=None, return_attention=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, seq_len)</code>.</p> </li> </ul> <p>Returns:     y: Output tensor of shape <code>(batch_size, seq_len, att_dim)</code>.     att: Only returned when <code>return_attention=True</code>. Attention weights of shape <code>(batch_size, n_heads, seq_len, seq_len)</code>.</p>"},{"location":"api/nn/attention/nystrom_attention/","title":"Nystr\u00f6m Attention","text":""},{"location":"api/nn/attention/nystrom_attention/#torchmil.nn.attention.NystromAttention","title":"<code>torchmil.nn.attention.NystromAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Nystrom attention, as described in the paper Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention.</p> <p>Implementation based on the official code.</p>"},{"location":"api/nn/attention/nystrom_attention/#torchmil.nn.attention.NystromAttention.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, learn_weights=True, n_landmarks=256, pinv_iterations=6, eps=1e-08)</code>","text":""},{"location":"api/nn/attention/nystrom_attention/#torchmil.nn.attention.NystromAttention.forward","title":"<code>forward(x, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, seq_len)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return attention weights.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, seq_len, att_dim)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention weights of shape <code>(batch_size, n_heads, seq_len, seq_len)</code>.</p> </li> </ul>"},{"location":"api/nn/attention/prob_smooth_attention_pool/","title":"Prob Smooth Attention Pool","text":""},{"location":"api/nn/attention/prob_smooth_attention_pool/#torchmil.nn.attention.ProbSmoothAttentionPool","title":"<code>torchmil.nn.attention.ProbSmoothAttentionPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Probabilistic Smooth Attention Pooling, proposed in in Probabilistic Smooth Attention for Deep Multiple Instance Learning in Medical Imaging and Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times \\texttt{in_dim}}\\), this model computes an attention distribution \\(q(\\mathbf{f} \\mid \\mathbf{X}) = \\mathcal{N}\\left(\\mathbf{f} \\mid \\mathbf{\\mu}_{\\mathbf{f}}, \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2) \\right)\\), where:</p> \\[\\begin{gather}     \\mathbf{H} = \\operatorname{MLP}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times 2\\texttt{att_dim}}, \\\\     \\mathbf{\\mu}_{\\mathbf{f}} = \\mathbf{H}\\mathbf{w}_{\\mu} \\in \\mathbb{R}^{N}, \\\\     \\log \\mathbf{\\sigma}_{\\mathbf{f}}^2 = \\mathbf{H}\\mathbf{w}_{\\sigma} \\in \\mathbb{R}^{N}, \\end{gather}\\] <p>where \\(\\operatorname{MLP}\\) is a multi-layer perceptron, and \\(\\mathbf{w}_{\\mu},\\mathbf{w}_{\\sigma} \\in \\mathbb{R}^{2\\texttt{att_dim} \\times 1}\\). If <code>covar_mode='zero'</code>, the variance vector \\(\\mathbf{\\sigma}_{\\mathbf{f}}^2\\) is set to zero, resulting in a deterministic attention distribution.</p> <p>Then, \\(M\\) samples from the attention distribution are drawn as \\(\\widehat{\\mathbf{f}}^{(m)} \\sim q(\\mathbf{f} \\mid \\mathbf{X})\\).  With these samples, the bag representation is computed as:</p> \\[ \\widehat{\\mathbf{z}} = \\operatorname{Softmax}(\\widehat{\\mathbf{F}}) \\mathbf{X} \\in \\mathbb{R}^{\\texttt{in_dim} \\times M}, \\] <p>where \\(\\widehat{\\mathbf{F}} = \\left[ \\widehat{\\mathbf{f}}^{(1)}, \\ldots, \\widehat{\\mathbf{f}}^{(M)} \\right]^\\top \\in \\mathbb{R}^{N \\times M}\\).</p> <p>Kullback-Leibler Divergence. Given a bag with adjancency matrix \\(\\mathbf{A}\\), the KL divergence between the attention distribution and the prior distribution is computed as:</p> \\[     \\ell_{\\text{KL}} =          \\begin{cases}             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} \\quad &amp; \\text{if } \\texttt{covar_mode='zero'}, \\\\             \\mathbf{\\mu}_{\\mathbf{f}}^\\top \\mathbf{L} \\mathbf{\\mu}_{\\mathbf{f}} + \\operatorname{Tr}(\\mathbf{L} \\mathbf{\\Sigma}_{\\mathbf{f}}) - \\frac{1}{2}\\log \\det( \\mathbf{\\Sigma}_{\\mathbf{f}} ) + \\operatorname{const} \\quad &amp; \\text{if } \\texttt{covar_mode='diag'}, \\\\         \\end{cases} \\] <p>where \\(\\operatorname{const}\\) is a constant term that does not depend on the parameters, \\(\\mathbf{\\Sigma}_{\\mathbf{f}} = \\operatorname{diag}(\\mathbf{\\sigma}_{\\mathbf{f}}^2)\\), \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) is the graph Laplacian matrix, and \\(\\mathbf{D}\\) is the degree matrix of \\(\\mathbf{A}\\).</p>"},{"location":"api/nn/attention/prob_smooth_attention_pool/#torchmil.nn.attention.ProbSmoothAttentionPool.__init__","title":"<code>__init__(in_dim=None, att_dim=128, covar_mode='diag', n_samples_train=1000, n_samples_test=5000)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Input dimension. If not provided, it will be lazily initialized.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>covar_mode</code>               (<code>str</code>, default:                   <code>'diag'</code> )           \u2013            <p>Covariance mode. Must be 'diag' or 'zero'.</p> </li> <li> <code>n_samples_train</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Number of samples during training.</p> </li> <li> <code>n_samples_test</code>               (<code>int</code>, default:                   <code>5000</code> )           \u2013            <p>Number of samples during testing.</p> </li> </ul>"},{"location":"api/nn/attention/prob_smooth_attention_pool/#torchmil.nn.attention.ProbSmoothAttentionPool.forward","title":"<code>forward(X, adj=None, mask=None, return_att=False, return_attdist=False, return_kl_div=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, D)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>. Only required when <code>return_kl_div=True</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns a sample from the attention distribution <code>f</code> in addition to <code>z</code>.</p> </li> <li> <code>return_attdist</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the attention distribution (<code>mu_f</code>, <code>diag_Sigma_f</code>) in addition to <code>z</code>.</p> </li> <li> <code>return_kl_div</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the KL divergence between the attention distribution and the prior distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>Tensor</code> )          \u2013            <p>Bag representation of shape <code>(batch_size, D)</code>.</p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>Sample from the attention distribution of shape <code>(batch_size, bag_size, n_samples)</code>. Only returned when <code>return_att=True</code>.</p> </li> <li> <code>mu_f</code> (              <code>Tensor</code> )          \u2013            <p>Mean of the attention distribution of shape <code>(batch_size, bag_size, 1)</code>. Only returned when <code>return_attdist=True</code>.</p> </li> <li> <code>diag_Sigma_f</code> (              <code>Tensor</code> )          \u2013            <p>Covariance of the attention distribution of shape <code>(batch_size, bag_size, 1)</code>. Only returned when <code>return_attdist=True</code>.</p> </li> <li> <code>kl_div</code> (              <code>Tensor</code> )          \u2013            <p>KL divergence between the attention distribution and the prior distribution, of shape <code>()</code>. Only returned when <code>return_kl_div=True</code>.</p> </li> </ul>"},{"location":"api/nn/attention/sm_attention_pool/","title":"Sm Attention Pool","text":""},{"location":"api/nn/attention/sm_attention_pool/#torchmil.nn.attention.SmAttentionPool","title":"<code>torchmil.nn.attention.SmAttentionPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Attention-based pooling with the Sm operator, as proposed in Sm: enhanced localization in Multiple Instance Learning for medical imaging classification.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times \\texttt{in_dim}}\\),  this model aggregates the instance features into a bag representation \\(\\mathbf{z} \\in \\mathbb{R}^{\\texttt{in_dim}}\\) as, </p> \\[\\begin{gather}     \\mathbf{f} = \\operatorname{SmMLP}(\\mathbf{X}) \\in \\mathbb{R}^{N}, \\\\     \\mathbf{z} = \\mathbf{X}^\\top \\operatorname{Softmax}(\\mathbf{f}) = \\sum_{n=1}^N s_n \\mathbf{x}_n,  \\end{gather}\\] <p>where \\(s_n\\) is the normalized attention score for the \\(n\\)-th instance. </p> <p>To compute the attention values, \\(\\operatorname{SmMLP}\\) is defined as \\(\\operatorname{SmMLP}(\\mathbf{X}) = \\mathbf{Y}^L\\) where</p> \\[\\begin{gather}     \\mathbf{Y}^0 = \\mathbf{X}\\mathbf{W^0}, \\\\     \\mathbf{Y}^l = \\operatorname{act}( \\texttt{Sm}(\\mathbf{Y}^{l-1}\\mathbf{W}^l)), \\quad \\text{for } l = 1, \\ldots, L-1, \\\\     \\mathbf{Y}^L = \\mathbf{Y}^{L-1}\\mathbf{w}, \\end{gather}\\] <p>where \\(\\mathbf{W^0} \\in \\mathbb{R}^{\\texttt{in_dim} \\times \\texttt{att_dim}}\\), \\(\\mathbf{W}^l \\in \\mathbb{R}^{\\texttt{att_dim} \\times \\texttt{att_dim}}\\), \\(\\mathbf{w} \\in \\mathbb{R}^{\\texttt{att_dim} \\times 1}\\), \\(\\operatorname{act} \\ \\colon \\mathbb{R} \\to \\mathbb{R}\\) is the activation function, and \\(\\texttt{Sm}\\) is the Sm operator, see Sm for more details.</p> <p>Note: If <code>sm_pre=True</code>, the Sm operator is applied before \\(\\operatorname{SmMLP}\\). If <code>sm_post=True</code>, the Sm operator is applied after \\(\\operatorname{SmMLP}\\).</p>"},{"location":"api/nn/attention/sm_attention_pool/#torchmil.nn.attention.SmAttentionPool.__init__","title":"<code>__init__(in_dim, att_dim=128, act='gelu', sm_mode='approx', sm_alpha='trainable', sm_layers=1, sm_steps=10, sm_pre=False, sm_post=False, sm_spectral_norm=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>act</code>               (<code>str</code>, default:                   <code>'gelu'</code> )           \u2013            <p>Activation function for attention. Possible values: 'tanh', 'relu', 'gelu'.</p> </li> <li> <code>sm_mode</code>               (<code>str</code>, default:                   <code>'approx'</code> )           \u2013            <p>Mode for the Sm operator. Possible values: 'approx', 'exact'.</p> </li> <li> <code>sm_alpha</code>               (<code>Union[float, str]</code>, default:                   <code>'trainable'</code> )           \u2013            <p>Alpha value for the Sm operator. If 'trainable', alpha is trainable.</p> </li> <li> <code>sm_layers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of layers that use the Sm operator.</p> </li> <li> <code>sm_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps for the Sm operator.</p> </li> <li> <code>sm_pre</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator before the attention pooling.</p> </li> <li> <code>sm_post</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply Sm operator after the attention pooling.</p> </li> <li> <code>sm_spectral_norm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, apply spectral normalization to all linear layers.</p> </li> </ul>"},{"location":"api/nn/attention/sm_attention_pool/#torchmil.nn.attention.SmAttentionPool.forward","title":"<code>forward(X, adj, mask=None, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Bag features of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask of shape <code>(batch_size, bag_size)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns attention values (before normalization) in addition to <code>z</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>Tensor</code> )          \u2013            <p>Bag representation of shape <code>(batch_size, in_dim)</code>.</p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention values (before normalization) of shape (batch_size, bag_size).</p> </li> </ul>"},{"location":"api/nn/gnns/","title":"torchmil.nn.gnns","text":"<ul> <li>Deep Graph Convolutional Network (DeepGCN) layer</li> <li>Graph Convolutional Network (GCN) convolution</li> <li>Dense MinCut pooling</li> </ul>"},{"location":"api/nn/gnns/deepgcn/","title":"DeepGCNLayer","text":""},{"location":"api/nn/gnns/deepgcn/#torchmil.nn.gnns.DeepGCNLayer","title":"<code>torchmil.nn.gnns.DeepGCNLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implementation of a DeepGCN layer.</p> <p>Adapts the implementation from torch_geometric.</p>"},{"location":"api/nn/gnns/deepgcn/#torchmil.nn.gnns.DeepGCNLayer.__init__","title":"<code>__init__(conv=None, norm=None, act=None, block='res', dropout=0.0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>conv</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Convolutional layer.</p> </li> <li> <code>norm</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Normalization layer.</p> </li> <li> <code>act</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Activation layer.</p> </li> <li> <code>block</code>               (<code>str</code>, default:                   <code>'res'</code> )           \u2013            <p>Skip connection type. Possible values: 'res', 'res+', 'dense', 'plain'.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/gnns/deepgcn/#torchmil.nn.gnns.DeepGCNLayer.forward","title":"<code>forward(x, adj)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Node features of shape <code>(batch_size, n_nodes, in_dim)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, n_nodes, n_nodes)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, n_nodes, out_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/gnns/dense_mincut_pool/","title":"Dense MinCut pooling","text":""},{"location":"api/nn/gnns/dense_mincut_pool/#torchmil.nn.gnns.dense_mincut_pool","title":"<code>torchmil.nn.gnns.dense_mincut_pool</code>","text":""},{"location":"api/nn/gnns/dense_mincut_pool/#torchmil.nn.gnns.dense_mincut_pool.dense_mincut_pool","title":"<code>dense_mincut_pool(x, adj, s, mask=None, temp=1.0)</code>","text":"<p>Dense MinCut Pooling.</p> <p>Adapts the implementation from torch_geometric.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, n_nodes, in_dim)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency tensor of shape <code>(batch_size, n_nodes, n_nodes)</code>.</p> </li> <li> <code>s</code>               (<code>Tensor</code>)           \u2013            <p>Dense learned assignments tensor of shape <code>(batch_size, n_nodes, n_cluster)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, n_nodes)</code>.</p> </li> <li> <code>temp</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_</code> (              <code>Tensor</code> )          \u2013            <p>Pooled node feature tensor of shape <code>(batch_size, n_cluster, in_dim)</code>.</p> </li> <li> <code>adj_</code> (              <code>Tensor</code> )          \u2013            <p>Coarsened adjacency tensor of shape <code>(batch_size, n_cluster, n_cluster)</code>.</p> </li> <li> <code>mincut_loss</code> (              <code>Tensor</code> )          \u2013            <p>MinCut loss.</p> </li> <li> <code>ortho_loss</code> (              <code>Tensor</code> )          \u2013            <p>Orthogonality loss.</p> </li> </ul>"},{"location":"api/nn/gnns/gcn_conv/","title":"GCNConv","text":""},{"location":"api/nn/gnns/gcn_conv/#torchmil.nn.gnns.GCNConv","title":"<code>torchmil.nn.gnns.GCNConv</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implementation of a Graph Convolutional Network (GCN) layer.</p> <p>Adapts the implementation from torch_geometric.</p>"},{"location":"api/nn/gnns/gcn_conv/#torchmil.nn.gnns.GCNConv.__init__","title":"<code>__init__(in_dim, out_dim, add_self_loops=False, bias=True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>)           \u2013            <p>Output dimension.</p> </li> <li> <code>add_self_loops</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add self-loops.</p> </li> <li> <code>bias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use bias.</p> </li> </ul>"},{"location":"api/nn/gnns/gcn_conv/#torchmil.nn.gnns.GCNConv.forward","title":"<code>forward(x, adj)</code>","text":"<p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <p>Node features of shape (batch_size, n_nodes, in_dim).</p> </li> <li> <code>adj</code>           \u2013            <p>Adjacency matrix of shape (batch_size, n_nodes, n_nodes).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape (batch_size, n_nodes, out_dim).</p> </li> </ul>"},{"location":"api/nn/transformers/","title":"torchmil.nn.transformers","text":"<ul> <li>Transformer base class</li> <li>Conventional Transformer</li> <li>Sm Transformer</li> <li>Nystrom Transformer</li> <li>Transformer with image Relative Positional Encoding (iRPE)</li> <li>Tokens-2-Token</li> </ul>"},{"location":"api/nn/transformers/base_transformer/","title":"Transformer base class","text":""},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Encoder","title":"<code>torchmil.nn.transformers.Encoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generic Transformer encoder class.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\) and (optional) additional arguments, this module computes:</p> \\[\\begin{align*} \\mathbf{X}^{0} &amp; = \\mathbf{X} \\\\ \\mathbf{X}^{l} &amp; = \\operatorname{Layer}^{l}\\left( \\mathbf{X}^{l-1}, \\ldots \\right), \\quad l = 1, \\ldots, L \\\\ \\end{align*}\\] <p>where \\(\\ldots\\) denotes additional arguments. The list of layers, \\(\\operatorname{Layer}^{l}\\) for \\(l = 1, \\ldots, L\\), is given by the <code>layers</code> argument, and should be a subclass of Layer.</p> <p>This module outputs \\(\\operatorname{Encoder}(\\mathbf{X}) = \\mathbf{X}^{L}\\) if <code>add_self=False</code>,  and \\(\\operatorname{Encoder}(\\mathbf{X}) = \\mathbf{X}^{L} + \\mathbf{X}\\) if <code>add_self=True</code>.</p>"},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Encoder.__init__","title":"<code>__init__(layers, add_self=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>layers</code>               (<code>ModuleList</code>)           \u2013            <p>List of encoder layers.</p> </li> <li> <code>add_self</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add input to output.</p> </li> </ul>"},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Encoder.forward","title":"<code>forward(X, **kwargs)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Layer","title":"<code>torchmil.nn.transformers.Layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generic Transformer layer class.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), and (optional) additional arguments, this module computes:</p> \\[\\begin{align*} \\mathbf{Z} &amp; = \\mathbf{X} + \\operatorname{Att}( \\operatorname{LayerNorm}(\\mathbf{X}) ) \\\\ \\mathbf{Y} &amp; = \\mathbf{Z} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z})), \\\\ \\end{align*}\\] <p>and outputs \\(\\mathbf{Y}\\). \\(\\operatorname{Att}\\) is given by the <code>att_module</code> argument, and \\(\\operatorname{MLP}\\) is given by the <code>mlp_module</code> argument.</p>"},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Layer.__init__","title":"<code>__init__(in_dim, att_dim, att_module, out_dim=None, use_mlp=True, mlp_module=None, dropout=0.0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>)           \u2013            <p>Attention dimension.</p> </li> <li> <code>att_module</code>               (<code>Module</code>)           \u2013            <p>Attention module.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use a MLP after the attention layer.</p> </li> <li> <code>mlp_module</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>MLP module.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/transformers/base_transformer/#torchmil.nn.transformers.Layer.forward","title":"<code>forward(X, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/conventional_transformer/","title":"Conventional Transformer","text":""},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerEncoder","title":"<code>torchmil.nn.transformers.TransformerEncoder</code>","text":"<p>               Bases: <code>Encoder</code></p> <p>A Transformer encoder with skip connections and layer normalization.</p> <p>Given an input bag input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), it computes:</p> \\[\\begin{align*} \\mathbf{X}^{0} &amp; = \\mathbf{X} \\\\ \\mathbf{Z}^{l} &amp; = \\mathbf{X}^{l-1} + \\operatorname{SelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}^{l-1}) ), \\quad l = 1, \\ldots, L \\\\ \\mathbf{X}^{l} &amp; = \\mathbf{Z}^{l} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z}^{l})), \\quad l = 1, \\ldots, L. \\\\ \\end{align*}\\] <p>This module outputs \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L}\\) if <code>add_self=False</code>,  and \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L} + \\mathbf{X}\\) if <code>add_self=True</code>.</p>"},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerEncoder.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, n_layers=4, use_mlp=True, add_self=False, dropout=0.0)</code>","text":"<p>Class constructor</p> <p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of layers.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>add_self</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add input to output.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerEncoder.forward","title":"<code>forward(X, mask=None)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerLayer","title":"<code>torchmil.nn.transformers.TransformerLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>One layer of the Transformer encoder.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this module computes:</p> \\[\\begin{align*} \\mathbf{Z} &amp; = \\mathbf{X} + \\operatorname{SelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}) ) \\\\ \\mathbf{Y} &amp; = \\mathbf{Z} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z})), \\\\ \\end{align*}\\] <p>and outputs \\(\\mathbf{Y}\\).</p>"},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerLayer.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, use_mlp=True, dropout=0.0)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate</p> </li> </ul>"},{"location":"api/nn/transformers/conventional_transformer/#torchmil.nn.transformers.TransformerLayer.forward","title":"<code>forward(X, mask=None)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, out_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/irpe_transformer/","title":"iRPE Transformer","text":""},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerEncoder","title":"<code>torchmil.nn.transformers.iRPETransformerEncoder</code>","text":"<p>               Bases: <code>Encoder</code></p> <p>Given an input bag input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), it computes:</p> \\[\\begin{align*} \\mathbf{X}^{0} &amp; = \\mathbf{X} \\\\ \\mathbf{Z}^{l} &amp; = \\mathbf{X}^{l-1} + \\operatorname{iRPESelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}^{l-1}) ), \\quad l = 1, \\ldots, L \\\\ \\mathbf{X}^{l} &amp; = \\mathbf{Z}^{l} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z}^{l})), \\quad l = 1, \\ldots, L. \\\\ \\end{align*}\\] <p>See iRPEMultiheadSelfAttention for more details about \\(\\operatorname{iRPESelfAttention}\\).</p> <p>This module outputs \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L}\\) if <code>add_self=False</code>,  and \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L} + \\mathbf{X}\\) if <code>add_self=True</code>.</p>"},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerEncoder.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, n_layers=4, use_mlp=True, add_self=False, dropout=0.0, rpe_ratio=1.9, rpe_method='product', rpe_mode='ctx', rpe_shared_head=True, rpe_skip=1, rpe_on='k')</code>","text":"<p>Class constructor</p> <p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of layers.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>add_self</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add input to output.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>rpe_ratio</code>               (<code>float</code>, default:                   <code>1.9</code> )           \u2013            <p>Relative position encoding ratio.</p> </li> <li> <code>rpe_method</code>               (<code>str</code>, default:                   <code>'product'</code> )           \u2013            <p>Relative position encoding method.</p> </li> <li> <code>rpe_mode</code>               (<code>str</code>, default:                   <code>'ctx'</code> )           \u2013            <p>Relative position encoding mode.</p> </li> <li> <code>rpe_shared_head</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to share relative position encoding weights across heads.</p> </li> <li> <code>rpe_skip</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Relative position encoding skip.</p> </li> <li> <code>rpe_on</code>               (<code>str</code>, default:                   <code>'k'</code> )           \u2013            <p>Relative position encoding on query, key, or value.</p> </li> </ul>"},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerEncoder.forward","title":"<code>forward(X)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerLayer","title":"<code>torchmil.nn.transformers.iRPETransformerLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Transformer layer with image Relative Position Encoding (iRPE), as described in Rethinking and Improving Relative Position Encoding for Vision Transformer.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this module computes:</p> \\[\\begin{align*} \\mathbf{Z} &amp; = \\mathbf{X} + \\operatorname{iRPESelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}) ) \\\\ \\mathbf{Y} &amp; = \\mathbf{Z} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z})), \\\\ \\end{align*}\\] <p>and outputs \\(\\mathbf{Y}\\). See iRPEMultiheadSelfAttention for more details about \\(\\operatorname{iRPESelfAttention}\\).</p>"},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerLayer.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, use_mlp=True, dropout=0.0, rpe_ratio=1.9, rpe_method='product', rpe_mode='ctx', rpe_shared_head=True, rpe_skip=1, rpe_on='k')</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension. If None, in_dim = att_dim.</p> </li> <li> <code>out_dim</code>           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>rpe_ratio</code>               (<code>float</code>, default:                   <code>1.9</code> )           \u2013            <p>Relative position encoding ratio.</p> </li> <li> <code>rpe_method</code>               (<code>str</code>, default:                   <code>'product'</code> )           \u2013            <p>Relative position encoding method.</p> </li> <li> <code>rpe_mode</code>               (<code>str</code>, default:                   <code>'ctx'</code> )           \u2013            <p>Relative position encoding mode.</p> </li> <li> <code>rpe_shared_head</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to share relative position encoding weights across heads.</p> </li> <li> <code>rpe_skip</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Relative position encoding skip.</p> </li> <li> <code>rpe_on</code>               (<code>str</code>, default:                   <code>'k'</code> )           \u2013            <p>Relative position encoding on query, key, or value.</p> </li> </ul>"},{"location":"api/nn/transformers/irpe_transformer/#torchmil.nn.transformers.iRPETransformerLayer.forward","title":"<code>forward(X)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, out_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/nystrom_transformer/","title":"Nystr\u00f6m Transformer","text":""},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerEncoder","title":"<code>torchmil.nn.transformers.NystromTransformerEncoder</code>","text":"<p>               Bases: <code>Encoder</code></p> <p>Nystrom Transformer encoder with skip connections and layer normalization.</p> <p>Given an input bag input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), it computes:</p> \\[\\begin{align*} \\mathbf{X}^{0} &amp; = \\mathbf{X} \\\\ \\mathbf{Z}^{l} &amp; = \\mathbf{X}^{l-1} + \\operatorname{NystromSelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}^{l-1}) ), \\quad l = 1, \\ldots, L \\\\ \\mathbf{X}^{l} &amp; = \\mathbf{Z}^{l} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z}^{l})), \\quad l = 1, \\ldots, L \\\\ \\end{align*}\\] <p>This module outputs \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L}\\) if <code>add_self=False</code>,  and \\(\\operatorname{TransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L} + \\mathbf{X}\\) if <code>add_self=True</code>.</p> <p>\\(\\operatorname{NystromSelfAttention}\\) is implemented using the NystromAttention module, see NystromAttention.</p>"},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerEncoder.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=8, n_layers=4, n_landmarks=256, pinv_iterations=6, residual=True, dropout=0.0, use_mlp=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of layers.</p> </li> <li> <code>n_landmarks</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Number of landmarks.</p> </li> <li> <code>pinv_iterations</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of iterations for the pseudo-inverse.</p> </li> <li> <code>residual</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use residual in the attention layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a MLP after the attention layer.</p> </li> </ul>"},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerEncoder.forward","title":"<code>forward(X, return_att=False)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, att_dim)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return attention weights.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, att_dim)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention weights of shape <code>(batch_size, n_heads, bag_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerLayer","title":"<code>torchmil.nn.transformers.NystromTransformerLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>One layer of the NystromTransformer encoder.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this module computes:</p> \\[\\begin{align*} \\mathbf{Z} &amp; = \\mathbf{X} + \\operatorname{NystromSelfAttention}( \\operatorname{LayerNorm}(\\mathbf{X}) ) \\\\ \\mathbf{Y} &amp; = \\mathbf{Z} + \\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathbf{Z})), \\\\ \\end{align*}\\] <p>and outputs \\(\\mathbf{Y}\\). \\(\\operatorname{NystromSelfAttention}\\) is implemented using the NystromAttention module, see NystromAttention.</p>"},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerLayer.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, learn_weights=True, n_landmarks=256, pinv_iterations=6, dropout=0.0, use_mlp=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>n_landmarks</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Number of landmarks.</p> </li> <li> <code>pinv_iterations</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of iterations for the pseudo-inverse.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a MLP after the attention layer.</p> </li> </ul>"},{"location":"api/nn/transformers/nystrom_transformer/#torchmil.nn.transformers.NystromTransformerLayer.forward","title":"<code>forward(X, return_att=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, att_dim)</code>.</p> </li> <li> <code>return_att</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return attention weights.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>X</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, att_dim)</code>.</p> </li> <li> <code>att</code> (              <code>Tensor</code> )          \u2013            <p>Only returned when <code>return_att=True</code>. Attention weights of shape <code>(batch_size, n_heads, bag_size, bag_size)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/sm_transformer/","title":"Sm Transformer","text":""},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerEncoder","title":"<code>torchmil.nn.transformers.SmTransformerEncoder</code>","text":"<p>               Bases: <code>Encoder</code></p> <p>A Transformer encoder with the \\(\\texttt{Sm}\\) operator, skip connections and layer normalization.</p> <p>Given an input bag input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), it computes:</p> \\[\\begin{align*} \\mathbf{X}^{0} &amp; = \\mathbf{X} \\\\ \\mathbf{Z}^{l} &amp; = \\mathbf{X}^{l-1} + \\texttt{Sm}( \\text{SelfAttention}( \\text{LayerNorm}(\\mathbf{X}^{l-1}) ) ), \\quad l = 1, \\ldots, L \\\\ \\mathbf{X}^{l} &amp; = \\mathbf{Z}^{l} + \\text{MLP}(\\text{LayerNorm}(\\mathbf{Z}^{l})), \\quad l = 1, \\ldots, L \\\\ \\end{align*}\\] <p>This module outputs \\(\\text{SmTransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L}\\) if <code>add_self=False</code>, and \\(\\text{SmTransformerEncoder}(\\mathbf{X}) = \\mathbf{X}^{L} + \\mathbf{X}\\) if <code>add_self=True</code>.</p> <p>See Sm for more details on the Sm operator.</p>"},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerEncoder.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, n_layers=4, use_mlp=True, add_self=False, dropout=0.0, sm_alpha=None, sm_mode=None, sm_steps=10)</code>","text":"<p>Class constructor</p> <p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>n_layers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of layers.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>add_self</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add input to output.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerEncoder.forward","title":"<code>forward(X, adj, mask=None)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerLayer","title":"<code>torchmil.nn.transformers.SmTransformerLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>One layer of the Transformer encoder with the \\(\\texttt{Sm}\\) operator.</p> <p>Given an input bag \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), this module computes:</p> \\[\\begin{align*} \\mathbf{Z} &amp; = \\mathbf{X} + \\texttt{Sm}( \\text{SelfAttention}( \\text{LayerNorm}(\\mathbf{X}) ) )\\\\ \\mathbf{Y} &amp; = \\mathbf{Z} + \\text{MLP}(\\text{LayerNorm}(\\mathbf{Z})), \\\\ \\end{align*}\\] <p>and outputs \\(\\mathbf{Y}\\).</p> <p>See Sm for more details on the Sm operator.</p>"},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerLayer.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, n_heads=4, use_mlp=True, dropout=0.0, sm_alpha=None, sm_mode=None, sm_steps=10)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>           \u2013            <p>Output dimension. If None, out_dim = in_dim.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate</p> </li> <li> <code>sm_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Alpha value for the Sm operator.</p> </li> <li> <code>sm_mode</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Sm mode.</p> </li> <li> <code>sm_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of steps to approximate the exact Sm operator.</p> </li> </ul>"},{"location":"api/nn/transformers/sm_transformer/#torchmil.nn.transformers.SmTransformerLayer.forward","title":"<code>forward(X, adj, mask=None)</code>","text":"<p>Forward method.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> <li> <code>adj</code>               (<code>Tensor</code>)           \u2013            <p>Adjacency matrix of shape <code>(batch_size, bag_size, bag_size)</code>.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Mask tensor of shape <code>(batch_size, bag_size)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Y</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor of shape <code>(batch_size, bag_size, in_dim)</code>.</p> </li> </ul>"},{"location":"api/nn/transformers/t2t/","title":"Tokens-to-Token (T2T)","text":""},{"location":"api/nn/transformers/t2t/#torchmil.nn.transformers.T2TLayer","title":"<code>torchmil.nn.transformers.T2TLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Tokens-to-Token (T2T) Transformer layer from Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</p>"},{"location":"api/nn/transformers/t2t/#torchmil.nn.transformers.T2TLayer.__init__","title":"<code>__init__(in_dim, out_dim=None, att_dim=512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(1, 1), n_heads=4, use_mlp=True, dropout=0.0)</code>","text":"<p>Parameters:</p> <ul> <li> <code>in_dim</code>               (<code>int</code>)           \u2013            <p>Input dimension.</p> </li> <li> <code>out_dim</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Output dimension. If None, output dimension will be <code>kernel_size[0] * kernel_size[1] * att_dim</code>.</p> </li> <li> <code>att_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Attention dimension.</p> </li> <li> <code>kernel_size</code>               (<code>tuple[int, int]</code>, default:                   <code>(3, 3)</code> )           \u2013            <p>Kernel size.</p> </li> <li> <code>stride</code>               (<code>tuple[int, int]</code>, default:                   <code>(1, 1)</code> )           \u2013            <p>Stride.</p> </li> <li> <code>padding</code>               (<code>tuple[int, int]</code>, default:                   <code>(2, 2)</code> )           \u2013            <p>Padding.</p> </li> <li> <code>dilation</code>               (<code>tuple[int, int]</code>, default:                   <code>(1, 1)</code> )           \u2013            <p>Dilation.</p> </li> <li> <code>n_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads.</p> </li> <li> <code>use_mlp</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use feedforward layer.</p> </li> <li> <code>dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Dropout rate.</p> </li> </ul>"},{"location":"api/nn/transformers/t2t/#torchmil.nn.transformers.T2TLayer.forward","title":"<code>forward(X)</code>","text":"<p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(batch_size, seq_len, in_dim)</code>.</p> </li> </ul> <p>Returns:     Y: Output tensor of shape <code>(batch_size, new_seq_len, out_dim)</code>. If <code>out_dim</code> is None, <code>out_dim</code> will be <code>att_dim * kernel_size[0] * kernel_size[1]</code>.</p>"},{"location":"api/utils/","title":"torchmil.utils","text":"<ul> <li>Annealing Scheduler</li> <li>Graph utils</li> <li>Trainer</li> </ul>"},{"location":"api/utils/annealing_scheduler/","title":"Annealing Scheduler","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.AnnealingScheduler","title":"<code>torchmil.utils.AnnealingScheduler</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.AnnealingScheduler.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.AnnealingScheduler.step","title":"<code>step()</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.AnnealingScheduler.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.ConstantAnnealingScheduler","title":"<code>torchmil.utils.ConstantAnnealingScheduler</code>","text":"<p>               Bases: <code>AnnealingScheduler</code></p>"},{"location":"api/utils/annealing_scheduler/#torchmil.utils.ConstantAnnealingScheduler.__init__","title":"<code>__init__(coef=1.0, *args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.ConstantAnnealingScheduler.step","title":"<code>step()</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.ConstantAnnealingScheduler.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.LinearAnnealingScheduler","title":"<code>torchmil.utils.LinearAnnealingScheduler</code>","text":"<p>               Bases: <code>AnnealingScheduler</code></p>"},{"location":"api/utils/annealing_scheduler/#torchmil.utils.LinearAnnealingScheduler.__init__","title":"<code>__init__(coef_init=0.0, coef_end=1.0, n_steps=100, *args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.LinearAnnealingScheduler.step","title":"<code>step()</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.LinearAnnealingScheduler.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.CyclicalAnnealingScheduler","title":"<code>torchmil.utils.CyclicalAnnealingScheduler</code>","text":"<p>               Bases: <code>AnnealingScheduler</code></p>"},{"location":"api/utils/annealing_scheduler/#torchmil.utils.CyclicalAnnealingScheduler.__init__","title":"<code>__init__(cycle_len, min_coef=0.0, max_coef=1.0, cycle_prop=0.5, warmup_steps=0, verbose=False, *args, **kwargs)</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.CyclicalAnnealingScheduler.step","title":"<code>step()</code>","text":""},{"location":"api/utils/annealing_scheduler/#torchmil.utils.CyclicalAnnealingScheduler.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":""},{"location":"api/utils/graph_utils/","title":"Graph utils","text":""},{"location":"api/utils/graph_utils/#torchmil.utils.degree","title":"<code>torchmil.utils.degree(index, edge_weight=None, n_nodes=None)</code>","text":"<p>Compute the degree of the adjacency matrix. Assumes that the graph is undirected.</p> <p>Parameters:</p> <ul> <li> <code>index</code>               (<code>ndarray</code>)           \u2013            <p>Edge index of the adjacency matrix, shape (2, n_edges).</p> </li> <li> <code>edge_weight</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>Edge weight of the adjacency matrix, shape (n_edges,).</p> </li> <li> <code>n_nodes</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of nodes in the graph.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>degree</code> (              <code>ndarray</code> )          \u2013            <p>Degree of the adjacency matrix.</p> </li> </ul>"},{"location":"api/utils/graph_utils/#torchmil.utils.add_self_loops","title":"<code>torchmil.utils.add_self_loops(edge_index, edge_weight=None, n_nodes=None)</code>","text":"<p>Add self-loops to the adjacency matrix.</p> <p>Parameters:</p> <ul> <li> <code>edge_index</code>               (<code>ndarray</code>)           \u2013            <p>Edge index of the adjacency matrix, shape (2, n_edges).</p> </li> <li> <code>edge_weight</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>Edge weight of the adjacency matrix, shape (n_edges,).</p> </li> <li> <code>n_nodes</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of nodes in the graph.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_edge_index</code> (              <code>ndarray</code> )          \u2013            <p>Edge index of the adjacency matrix with self-loops.</p> </li> <li> <code>new_edge_weight</code> (              <code>ndarray</code> )          \u2013            <p>Edge weight of the adjacency matrix with self-loops.</p> </li> </ul>"},{"location":"api/utils/graph_utils/#torchmil.utils.normalize_adj","title":"<code>torchmil.utils.normalize_adj(edge_index, edge_weight=None, n_nodes=None)</code>","text":"<p>Normalize the adjacency matrix.</p> <p>Parameters:</p> <ul> <li> <code>edge_index</code>               (<code>ndarray</code>)           \u2013            <p>Edge index of the adjacency matrix.</p> </li> <li> <code>edge_weight</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>Edge weight of the adjacency matrix.</p> </li> <li> <code>n_nodes</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of nodes in the graph.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>edge_weight</code> (              <code>ndarray</code> )          \u2013            <p>Edge weight of the normalized adjacency matrix.</p> </li> </ul>"},{"location":"api/utils/graph_utils/#torchmil.utils.build_adj","title":"<code>torchmil.utils.build_adj(coords, feat=None, dist_thr=1.0, add_self_loops=False)</code>","text":"<p>Build the adjacency matrix for a general graph given the coordinates and features of the nodes.</p> <p>Parameters:</p> <ul> <li> <code>coords</code>               (<code>ndarray</code>)           \u2013            <p>Coordinates of the nodes.</p> </li> <li> <code>feat</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>Features of the nodes, used to compute the edge weights. If None, the adjacency matrix is binary.</p> </li> <li> <code>dist_thr</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Distance threshold to consider two nodes as neighbors. Default is 1.0.</p> </li> <li> <code>add_self_loops</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add self-loops.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>edge_index</code> (              <code>ndarray</code> )          \u2013            <p>Edge index of the adjacency matrix.</p> </li> <li> <code>edge_weight</code> (              <code>ndarray</code> )          \u2013            <p>Edge weight of the adjacency matrix</p> </li> </ul>"},{"location":"api/utils/trainer/","title":"Trainer","text":""},{"location":"api/utils/trainer/#torchmil.utils.Trainer","title":"<code>torchmil.utils.Trainer</code>","text":"<p>Generic trainer class for training MIL models.</p>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer.__init__","title":"<code>__init__(model, optimizer, metrics_dict={'auroc': torchmetrics.AUROC(task='binary')}, obj_metric='auroc', obj_metric_mode='max', lr_scheduler=None, annealing_scheduler_dict=None, device='cuda', logger=None, early_stop_patience=None, disable_pbar=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>model</code>               (<code>MILModel</code>)           \u2013            <p>MIL model to be trained. Must be an instance of MILModel.</p> </li> <li> <code>optimizer</code>               (<code>Optimizer</code>)           \u2013            <p>Optimizer for training the model.</p> </li> <li> <code>metrics_dict</code>               (<code>dict[str:Metric]</code>, default:                   <code>{'auroc': AUROC(task='binary')}</code> )           \u2013            <p>Dictionary of metrics to be computed during training. Metrics should be instances of torchmetrics.Metric.</p> </li> <li> <code>obj_metric</code>               (<code>str</code>, default:                   <code>'auroc'</code> )           \u2013            <p>Objective metric to be used for early stopping and to track the best model. Must be one of the keys in <code>metrics_dict</code> or the loss used by the model.</p> </li> <li> <code>obj_metric_mode</code>               (<code>str</code>, default:                   <code>'max'</code> )           \u2013            <p>Mode for the objective metric. Must be one of 'max' or 'min'. If 'max', the best model is the one with the highest value of the objective metric. If 'min', the best model is the one with the lowest value of the objective metric.</p> </li> <li> <code>lr_scheduler</code>               (<code>_LRScheduler</code>, default:                   <code>None</code> )           \u2013            <p>Learning rate scheduler.</p> </li> <li> <code>annealing_scheduler_dict</code>               (<code>dict[str:AnnealingScheduler]</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary of annealing schedulers for loss coefficients. Keys should be the loss names and values should be instances of AnnealingScheduler.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>Device to be used for training.</p> </li> <li> <code>logger</code>           \u2013            <p>Logger to log metrics. Must have a <code>log</code> method. It can be, for example, a Wandb Run.</p> </li> <li> <code>early_stop_patience</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Patience for early stopping. If None, early stopping is disabled.</p> </li> <li> <code>disable_pbar</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Disable progress bar.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer.train","title":"<code>train(max_epochs, train_dataloader, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Train the model.</p> <p>Parameters:</p> <ul> <li> <code>max_epochs</code>               (<code>int</code>)           \u2013            <p>Maximum number of epochs to train.</p> </li> <li> <code>train_dataloader</code>               (<code>DataLoader</code>)           \u2013            <p>Train dataloader.</p> </li> <li> <code>val_dataloader</code>               (<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>Validation dataloader. If None, the train dataloader is used.</p> </li> <li> <code>test_dataloader</code>               (<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>Test dataloader. If None, test metrics are not computed.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer.get_model_state_dict","title":"<code>get_model_state_dict()</code>","text":"<p>Get (a deepcopy of) the state dictionary of the model.</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>State dictionary of the model.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer.get_best_model_state_dict","title":"<code>get_best_model_state_dict()</code>","text":"<p>Get the state dictionary of the best model (the model with the best objective metric).</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>State dictionary of the best model.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer.get_best_model","title":"<code>get_best_model()</code>","text":"<p>Get the best model (the model with the best objective metric).</p> <p>Returns:</p> <ul> <li> <code>MILModel</code>           \u2013            <p>Best model.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer._log","title":"<code>_log(metrics)</code>","text":"<p>Log metrics using the logger.</p> <p>Parameters:</p> <ul> <li> <code>metrics</code>               (<code>dict[str:float]</code>)           \u2013            <p>Dictionary of metrics to be logged.</p> </li> </ul>"},{"location":"api/utils/trainer/#torchmil.utils.Trainer._shared_loop","title":"<code>_shared_loop(dataloader, disable_pbar=False, epoch=0, mode='train')</code>","text":"<p>Shared training/validation/test loop.</p> <p>Parameters:</p> <ul> <li> <code>dataloader</code>               (<code>DataLoader</code>)           \u2013            <p>Dataloader.</p> </li> <li> <code>disable_pbar</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Disable progress bar.</p> </li> <li> <code>epoch</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Epoch number.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Mode of the loop. Must be one of 'train', 'val', 'test'.</p> </li> </ul>"},{"location":"api/visualize/","title":"torchmil.visualize","text":"<ul> <li>Visualizing CT scans</li> <li>Visualizing WSIs</li> </ul>"},{"location":"api/visualize/vis_ctscan/","title":"Visualizing CT scans","text":""},{"location":"api/visualize/vis_ctscan/#torchmil.visualize.slices_to_canvas","title":"<code>torchmil.visualize.slices_to_canvas(slices_list, slice_size)</code>","text":"<p>Given a list of images of CT scan slices, return a canvas with all the slices.</p> <p>Parameters:</p> <ul> <li> <code>slices_list</code>               (<code>list[ndarray]</code>)           \u2013            <p>List of images of CT scan slices. Each image is a numpy array with shape <code>(slice_size, slice_size, 3)</code>.</p> </li> <li> <code>slice_size</code>               (<code>int</code>)           \u2013            <p>Size of the slices.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with all the slices. It has shape <code>(slice_size, bag_len*slice_size, 3)</code>.</p> </li> </ul>"},{"location":"api/visualize/vis_ctscan/#torchmil.visualize.draw_slices_contour","title":"<code>torchmil.visualize.draw_slices_contour(canvas, slice_size, contour_prop=0.05)</code>","text":"<p>Given a canvas with CT scan slices already drawn, draw a contour around each slice.    </p> <p>Parameters:</p> <ul> <li> <code>canvas</code>               (<code>ndarray</code>)           \u2013            <p>Canvas with all the slices. It has shape <code>(slice_size, bag_len*slice_size, 3)</code>.</p> </li> <li> <code>slice_size</code>               (<code>int</code>)           \u2013            <p>Size of the slices.</p> </li> <li> <code>contour_prop</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>Proportion of the slice size that the contour will cover.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with the contours drawn. It has shape <code>(slice_size, bag_len*slice_size, 3)</code>.</p> </li> </ul>"},{"location":"api/visualize/vis_ctscan/#torchmil.visualize.draw_heatmap_ctscan","title":"<code>torchmil.visualize.draw_heatmap_ctscan(canvas, values, slice_size, alpha=0.5, max_color=np.array([0.8392156862745098, 0.15294117647058825, 0.1568627450980392]), min_color=np.array([0.17254901960784313, 0.6274509803921569, 0.17254901960784313]))</code>","text":"<p>Given a canvas with CT scan slices already drawn, draw a heatmap on top of the slices. This heatmap is defined by <code>values</code>, which should be normalized between 0 and 1.</p> <p>Parameters:</p> <ul> <li> <code>canvas</code>               (<code>ndarray</code>)           \u2013            <p>Canvas with all the slices. It has shape <code>(slice_size, bag_len*slice_size, 3)</code>.</p> </li> <li> <code>values</code>               (<code>ndarray</code>)           \u2013            <p>List of values to draw the heatmap. Each value should be normalized between 0 and 1.</p> </li> <li> <code>slice_size</code>               (<code>int</code>)           \u2013            <p>Size of the slices.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Alpha value for blending the heatmap with the canvas.</p> </li> <li> <code>max_color</code>               (<code>ndarray</code>, default:                   <code>array([0.8392156862745098, 0.15294117647058825, 0.1568627450980392])</code> )           \u2013            <p>Color for the maximum value in the heatmap.</p> </li> <li> <code>min_color</code>               (<code>ndarray</code>, default:                   <code>array([0.17254901960784313, 0.6274509803921569, 0.17254901960784313])</code> )           \u2013            <p>Color for the minimum value in the heatmap.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with the heatmap drawn. It has shape <code>(slice_size, bag_len*slice_size, 3)</code>.</p> </li> </ul>"},{"location":"api/visualize/vis_wsi/","title":"Visualizing WSIs","text":""},{"location":"api/visualize/vis_wsi/#torchmil.visualize.patches_to_canvas","title":"<code>torchmil.visualize.patches_to_canvas(patches_list, row_array, column_array, patch_size)</code>","text":"<p>Given a list of WSI patches and their corresponding row and column indices, return a canvas with all the patches.</p> <p>Parameters:</p> <ul> <li> <code>patches_list</code>               (<code>list</code>)           \u2013            <p>List of WSI patches. Each patch is a numpy array with shape <code>(patch_size, patch_size, 3)</code>.</p> </li> <li> <code>row_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the row indices of the patches.</p> </li> <li> <code>column_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the column indices of the patches.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Size of the patches.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with all the patches. It has shape <code>(max_row*patch_size, max_column*patch_size, 3)</code>.</p> </li> </ul>"},{"location":"api/visualize/vis_wsi/#torchmil.visualize.draw_patches_contour","title":"<code>torchmil.visualize.draw_patches_contour(canvas, row_array, column_array, patch_size, contour_prop=0.05)</code>","text":"<p>Given a canvas with WSI patches already drawn, draw a contour around each patch.</p> <p>Parameters:</p> <ul> <li> <code>canvas</code>               (<code>ndarray</code>)           \u2013            <p>Canvas with all the patches. It has shape <code>(max_row*patch_size, max_column*patch_size, 3)</code>.</p> </li> <li> <code>row_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the row indices of the patches.</p> </li> <li> <code>column_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the column indices of the patches.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Size of the patches.</p> </li> <li> <code>contour_prop</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>Proportion of the patch size that the contour will cover.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with the contours drawn. It has shape <code>(max_row*patch_size, max_column*patch_size, 3)</code>.</p> </li> </ul>"},{"location":"api/visualize/vis_wsi/#torchmil.visualize.draw_heatmap_wsi","title":"<code>torchmil.visualize.draw_heatmap_wsi(canvas, values, patch_size, row_array, col_array, alpha=0.5, max_color=np.array([0.8392156862745098, 0.15294117647058825, 0.1568627450980392]), min_color=np.array([0.17254901960784313, 0.6274509803921569, 0.17254901960784313]))</code>","text":"<p>Given a canvas with WSI patches already drawn, draw a heatmap on top of the patches. This heatmap is defined by <code>values</code>, which should be normalized between 0 and 1.</p> <p>Parameters:</p> <ul> <li> <code>canvas</code>               (<code>ndarray</code>)           \u2013            <p>Canvas with all the patches. It has shape <code>(max_row*patch_size, max_column*patch_size, 3)</code>.</p> </li> <li> <code>values</code>               (<code>ndarray</code>)           \u2013            <p>Array with the values of the heatmap.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Size of the patches.</p> </li> <li> <code>row_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the row indices of the patches.</p> </li> <li> <code>col_array</code>               (<code>ndarray</code>)           \u2013            <p>Array with the column indices of the patches.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Alpha value of the heatmap.</p> </li> <li> <code>max_color</code>               (<code>ndarray</code>, default:                   <code>array([0.8392156862745098, 0.15294117647058825, 0.1568627450980392])</code> )           \u2013            <p>Color of the highest value of the heatmap.</p> </li> <li> <code>min_color</code>               (<code>ndarray</code>, default:                   <code>array([0.17254901960784313, 0.6274509803921569, 0.17254901960784313])</code> )           \u2013            <p>Color of the lowest value of the heatmap.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>canvas</code> (              <code>ndarray</code> )          \u2013            <p>Canvas with the heatmap drawn. It has shape <code>(max_row*patch_size, max_column*patch_size, 3)</code>.</p> </li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Here you can find a collection of examples that demonstrate how to use torchmil in practice. </p> <ul> <li>Representing data in torchmil: we explain how data is represented in torchmil. We cover bags, instances, graphs, mini-batching, and more.</li> <li>Datasets in torchmil: we explain how datasets are implemented in torchmil. We cover processed datasets vs non-processed datasets, and how to create your own dataset.</li> <li>Training your first MIL model: we show how to train a simple attention-based MIL model with a toy dataset.</li> <li>WSI classification in torchmil: we show how to train an attention-based MIL model with WSI data.</li> <li>CT scan classification in torchmil: we show how to train an attention-based MIL model with CT scan data.</li> </ul>"},{"location":"examples/data_representation/","title":"Representing bags in torchmil","text":"<p>In the following, we explain how torchmil represents bags and how to use them in your code. </p> <p>This notebook contains:</p> <ul> <li>A brief introduction to bags in Multiple Instance Learning (MIL). </li> <li>How to represent bags in torchmil.</li> <li>A first look at the <code>ToyDataset</code> from the torchmil.datasets module.</li> <li>How mini-batching is handled in torchmil.</li> <li>Differences between the sequential and spatial representations of bags.</li> </ul> <pre><code>import torch\nfrom torchvision import datasets, transforms\n\n# Load MNIST dataset\nmnist = datasets.MNIST('/tmp/', train=True, download=True, transform=transforms.ToTensor())\n\n# Extract features and labels\ndata = mnist.data.view(-1, 28*28) / 255\nlabels = mnist.targets\n</code></pre> <p>Let's create a bag of 10 instances. The label of each instance will be the digit it represents, and the label of the bag will be the maximum digit in the bag.</p> <pre><code>from tensordict import TensorDict\n\n# Select 10 random indices\nindices = torch.randperm(data.size(0))[:10]\n\nbag = TensorDict({\n    'X': data[indices],\n    'y_inst': labels[indices],\n    'Y': labels[indices].max()\n})\nbag\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([10, 784]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)</code>\n</pre> <p>Now, let's create a MIL dataset using the <code>ToyDataset</code> class from torchmil.datasets.</p> <p>We will create a binary dataset, where the digits \\(4\\) and \\(5\\) are the positive instances (their label is \\(1\\)), and the rest are the negative instances (their label is \\(0\\)). Thus, the label of the bag is \\(1\\) if it contains at least one \\(4\\) or \\(5\\), and \\(0\\) otherwise.</p> <pre><code>from torchmil.datasets import ToyDataset\n\n# Define positive labels\nobj_labels = [4, 5] # Digits 4 and 5 are considered positive\n\n# Create MIL dataset\ntoy_dataset = ToyDataset(data, labels, num_bags=100, obj_labels=obj_labels, bag_size=10)\n\n# Retrieve a bag\nbag = toy_dataset[0]\nbag\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([10, 784]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)</code>\n</pre> <p>Let's visualize the bags</p> <pre><code>import matplotlib.pyplot as plt\n\ndef plot_bag(bag):\n    bag_size = len(bag['X'])\n    fig, axes = plt.subplots(1, bag_size, figsize=(bag_size, 1.8))\n    for i in range(bag_size):\n        ax = axes[i]\n        ax.imshow(bag['X'][i].view(28, 28), cmap='gray')\n        ax.set_title(f\"label: {bag['y_inst'][i].item()}\")        \n        ax.axis('off')\n    fig.suptitle(f'Bag label: {bag[\"Y\"].item()}')\n    plt.show()\n\nfor i in range(3):\n    bag = toy_dataset[i]\n    plot_bag(bag)\n</code></pre> <pre><code>from torchmil.datasets import ToyDataset\n\n# Define positive labels\nobj_labels = [4, 5] # Digits 4 and 5 are considered positive\n\n# Create MIL dataset\ntoy_dataset = ToyDataset(data, labels, num_bags=100, obj_labels=obj_labels, bag_size=(4, 10))\n</code></pre> <p>We retrieve four bags from the dataset and collate them into a batch. A batch is just a <code>TensorDict</code> object containing the padded tensors and the mask tensor.</p> <pre><code>from torchmil.data import collate_fn\n\nbag_list = [toy_dataset[i] for i in range(4)]\nbatch = collate_fn(bag_list)\nbatch\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([4, 9, 784]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False),\n        mask: Tensor(shape=torch.Size([4, 9]), device=cpu, dtype=torch.uint8, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([4, 9]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)</code>\n</pre> <p>Let's plot the bags in the batch and the mask tensor.</p> <pre><code>def plot_batch(batch):\n    batch_size = len(batch['X'])\n    bag_size = len(batch['X'][0])\n    fig, axes = plt.subplots(batch_size, bag_size, figsize=(bag_size, 1.5*batch_size))\n    for i in range(batch_size):\n        for j in range(bag_size):\n            ax = axes[i, j]\n            ax.imshow(batch['X'][i][j].view(28, 28), cmap='gray')\n            ax.set_title(f\"label: {batch['y_inst'][i][j].item()}\\nmask: {batch['mask'][i][j].item()}\")        \n            ax.axis('off')\n        fig.suptitle(f'Bag labels: {batch[\"Y\"].tolist()}')\n\nplot_batch(batch)\n</code></pre> <p>As we can see, the bags are padded to the maximum size of the bags in the batch with zeros. The mask tensor indicates which elements are real instances and which are padding. Additionally, the function <code>collate_fn</code> also pads other tensors, such as the adjacency matrix or the instance coordinates. </p> <pre><code># Select 10 random indices\nindices = torch.randperm(data.size(0))[:5]\n\nbag = TensorDict({\n    'X': data[indices],\n    'y_inst': labels[indices],\n    'Y': labels[indices].max()\n})\n\n# Create the canvas\nheight = 3*28\nwidth = 5*28\ncanvas = torch.zeros(height, width)\n\n# Randomly place the digits on the canvas\ntorch.manual_seed(5) # set seed\ncoords_list = []\nfor n in range(5):\n    i = torch.randint(0, height-28, (1,)).item()\n    j = torch.randint(0, width-28, (1,)).item()\n    canvas[i:i+28, j:j+28] = bag['X'][n].view(28, 28)\n    coords_list.append((i, j))\n\n# Convert to tensor\ncoords = torch.tensor(coords_list)\n\n# Display the canvas\nplt.imshow(canvas, cmap='gray')\nplt.title('Original canvas')\nplt.axis('off')\nplt.show()\n</code></pre> <p>Now, the digits in our bag have a spatial structure given by their coordinates. Let's compute the spatial representation of the bag using the coordinates.</p> <pre><code>from torchmil.data import seq_to_spatial, spatial_to_seq\nX = bag['X'].unsqueeze(0) # add batch dimension for seq_to_spatial and spatial_to_seq\ncoords = coords.unsqueeze(0) # add batch dimension for seq_to_spatial and spatial_to_seq\nX_esp = seq_to_spatial(X, coords) # remove batch dimension\nX_seq = spatial_to_seq(X_esp, coords) # remove batch dimension\n\n# Remove batch dimension\ncoords = coords.squeeze(0)\nX = X.squeeze(0)\nX_seq = X_seq.squeeze(0)\nX_esp = X_esp.squeeze(0)\n\nprint('X shape:', X.shape)\nprint('X_seq shape:', X_seq.shape)\nprint('X_esp shape:', X_esp.shape)\nprint('X and X_seq are equal:', torch.allclose(X, X_seq))\n</code></pre> <pre>\n<code>X shape: torch.Size([5, 784])\nX_seq shape: torch.Size([5, 784])\nX_esp shape: torch.Size([44, 111, 784])\nX and X_seq are equal: True\n</code>\n</pre> <p><code>X_esp</code> is the spatial representation of the bag. It is equivalent to the canvas with the digits placed in the corresponding coordinates. Let's reconstruct the canvas using it. </p> <pre><code>canvas_rec = torch.zeros(height, width)\nfor n in range(5):\n    i, j = coords[n]\n    canvas_rec[i:i+28, j:j+28] = X_esp[i,j,:].view(28, 28)\n\nplt.imshow(canvas_rec, cmap='gray')\nplt.title('Reconstructed canvas')\nplt.axis('off')\nplt.show()\n</code></pre> <p>Finally, the sequential representation, which is already stored in <code>bag</code>, can be coupled with the coordinates. Using these coordinates, we can compute an adjacency matrix. </p> <pre><code>coords = coords.squeeze(0).type(torch.float32) # convert to float32\nbag['coords'] = coords # add to bag\n\n# Create the adjacency matrix. Each entry (i, j) is given by an RBF kernel evaluated at coordinates i and j with a length scale of 28\nadj = torch.zeros(5, 5)\nfor i in range(5):\n    for j in range(5):\n        if i != j:\n            adj[i, j] = torch.exp(- torch.norm(coords[i] - coords[j]) / 28 )\nbag['adj'] = adj # add to bag\n\n# Plot the canvas with the adjacency matrix overlayed\nplt.imshow(canvas, cmap='gray')\nfor i in range(5):\n    for j in range(5):\n        plt.plot([coords[i, 1]+14, coords[j, 1]+14], [coords[i, 0]+14, coords[j, 0]+14], 'r', alpha=adj[i, j].item())\nplt.axis('off')\nplt.show()\n</code></pre> <p>The adjacency matrix indicates the distance between the digits in the bag. </p>"},{"location":"examples/data_representation/#representing-bags-in-torchmil","title":"Representing bags in torchmil","text":""},{"location":"examples/data_representation/#what-is-a-bag","title":"What is a bag?","text":"<p>In Multiple Instance Learning (MIL), a bag is a collection of instances. Usually, both instances and bags have labels. However, it is assumed that the labels of the instances in a bag are not available at training time. Instead, we only have access to:</p> <ul> <li>The label of the bag, </li> <li>Some kind of relation between the instance labels and the bag label.</li> </ul> <p>Additionally, a bag can have some structure, such as a graph representing the relationships between the instances in the bag, or the coordinates of the instances in some space. All these cases can be handled with torchmil.</p> <p>Example: MIL binary classification</p> <p>In this case, the bags have the form \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), where each \\(\\mathbf{x}_n \\in \\mathbb{R}^D\\) is an instance.  The labels of the instances are \\(\\mathbf{y} = \\left[ y_1, \\ldots, y_N \\right]^\\top \\in \\{0, 1\\}^N\\), but we do not have access to them at training time (they may be accessible at test time). The label of the bag is \\(Y \\in \\{0, 1\\}\\), and the relation between the instance labels and the bag label is as follows:</p> \\[ Y = \\max \\left\\{ y_1, \\ldots, y_N \\right\\} \\] <p>This example is the most common in MIL, but there are many other possibilities. </p>"},{"location":"examples/data_representation/#how-bags-are-represented-in-torchmil","title":"How bags are represented in torchmil?","text":"<p>In torchmil, bags are represented as a <code>TensorDict</code> object, which stores any kind of information about the bag. In most cases, a bag will contain at least the following keys:</p> <ul> <li><code>bag['X']</code>: a tensor of shape <code>(bag_size, ...)</code> containing the instances in the bag. Usually, this tensor is called bag feature matrix, since these instances are feature vectors extracted from the raw representation of the instances. Therefore, in most cases it has shape <code>(bag_size, feature_dim)</code>. </li> <li><code>bag['Y']</code>: a tensor containing the label of the bag. In the simplest case, this tensor is a scalar, but it can be a tensor of any shape (e.g., in multi-class MIL).</li> </ul> <p>Additionally, a bag may contain other keys. The most common ones in torchmil are:</p> <ul> <li><code>bag['y_inst']</code>: a tensor of shape <code>(bag_size, ...)</code> containing the labels of the instances in the bag. In the pure MIL setting, this tensor is only used for evaluation purposes since the label of the instances are not known. However, some methods may require some sort of supervision at the instance level.</li> <li><code>bag['adj']</code>: a tensor of shape <code>(bag_size, bag_size)</code> containing the adjacency matrix of the bag. This matrix is used to represent the relationships between the instances in the bag. The methods implemented in torchmil.models allow this matrix to be a sparse tensor.</li> <li><code>bag['coords']</code>: a tensor of shape <code>(bag_size, coords_dim)</code> containing the coordinates of the instances in the bag. This tensor is used to represent the absolute position of the instances in the bag.</li> </ul>"},{"location":"examples/data_representation/#example-mnist","title":"Example: MNIST","text":"<p>Creating a bag is as simple as creating a <code>TensorDict</code> object. Let's use the MNIST dataset to illustrate how bags are represented in torchmil.</p>"},{"location":"examples/data_representation/#mini-batches-in-torchmil-masks","title":"Mini-batches in torchmil: masks","text":"<p>Mini-batches enable the training of deep learning models with huge amounts of data. In torchmil, mini-batches are handled by the <code>collate_fn</code> function of torchmil.data, which is used to collate a list of bags into a batch.</p> <p>In MIL, each bag can be of different size. To solve this, in torchmil, the tensors in the bags are padded to the maximum size of the bags in the batch. A mask tensor is used to indicate which elements of the padded tensors are real instances and which are padding. This mask tensor is used to adjust the behavior of the models to ignore the padding elements (e.g., in the attention mechanism).</p> <p>Why not use <code>torch.nested</code>?</p> <p><code>torch.nested</code> offer a more flexible method for handling bags of varying sizes. However, since the PyTorch API for nested tensors is still in the prototype stage, torchmil currently relies on the padding approach.</p> <p>We illustrate this behaviour in the following example. We use again the MNIST dataset, but this time we create a dataset with bags of different sizes.</p>"},{"location":"examples/data_representation/#sequential-representation-vs-spatial-representation","title":"Sequential representation vs spatial representation","text":"<p>In torchmil, bags can be represented in two ways: sequential and spatial. </p> <p>In the sequential representation <code>bag['X']</code> is a tensor of shape <code>(bag_size, dim)</code>. This representation is the most common in MIL.  When the bag has some spatial structure, the sequential representation can be coupled with a graph using an adjacency matrix or with the coordinates of the instances. These are stored as <code>bag['adj']</code> (of shape <code>(bag_size, bag_size)</code>) and <code>bag['coords']</code> (of shape <code>(bag_size, coords_dim)</code>), respectively.</p> <p>Alternatively, the spatial representation can be used. In this case, <code>bag['X']</code> is a tensor of shape <code>(coord1, ..., coordN, dim)</code>, where <code>N=coords_dim</code> is the number of dimensions of the space.</p> <p>In torchmil, you can convert from one representation to the other using the functions <code>torchmil.utils.seq_to_spatial</code> and <code>torchmil.utils.spatial_to_seq</code> from the torchmil.data module. These functions need the coordinates of the instances in the bag, stored as <code>bag['coords']</code>.</p> <p>Example: Whole Slide Images</p> <p>Due to their large resolution, Whole Slide Images (WSIs) are usually represented as bags of patches. Each patch is an image, from which a feature vector of is typically extracted. The spatial representation of a WSI has shape <code>(height, width, feat_dim)</code>, while the sequential representation has shape <code>(bag_size, feat_dim)</code>. The coordinates corresponds to the coordinates of the patches in the WSI. </p> <p>SETMIL is an example of a model that uses the spatial representation of a WSI. </p> <p>Let's illustrate this with an example. Again, using MNIST, we will create a bag of 10 instances, and randomly place them in a canvas.</p>"},{"location":"examples/training_your_first_mil_model/","title":"Training your first MIL model","text":"<pre><code>import torch\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n</code></pre> <pre><code>from torchvision import datasets, transforms\nfrom torchmil.datasets import ToyDataset\n\n# Load MNIST dataset\nmnist = datasets.MNIST('/tmp/', train=True, download=True, transform=transforms.ToTensor())\n\n# Extract features and labels\ndata = mnist.data.view(-1, 28*28) / 255.0\nlabels = mnist.targets\n\n# Define positive labels\nobj_labels = [0, 1] \n\n# Define bag size\nbag_size = (7, 12) # Randomly sample between 8 and 12 instances per bag\n\n# Create MIL dataset\ntrain_dataset = ToyDataset(data, labels, num_bags=10000, obj_labels=obj_labels, bag_size=bag_size, seed=0)\ntest_dataset = ToyDataset(data, labels, num_bags=2000, obj_labels=obj_labels, bag_size=bag_size, seed=2)\n\n# Print one bag\nprint(train_dataset[0])\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([11, 784]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)\n</code>\n</pre> <p>In torchmil, each bag is a <code>TensorDict</code>. The different keys correspond to different elements of the bag. In this case, each bag has a feature matrix <code>X</code>, the bag label <code>Y</code>, and the instance labels <code>y_inst</code>. Recall that the instance labels cannot be used during training, they are available only for evaluation purposes.</p> <pre><code>from torchmil.data import collate_fn\n\n# Create dataloaders\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nfrom tensordict import TensorDict\n\ndef plot_batch(batch, max_bags=5):\n    batch_size = min(len(batch['X']), max_bags)\n    bag_size = len(batch['X'][0])\n    fig, axes = plt.subplots(batch_size, bag_size, figsize=(bag_size, 1.5*batch_size))\n    for i in range(batch_size):\n        for j in range(bag_size):\n            ax = axes[i, j]\n            ax.imshow(batch['X'][i][j].view(28, 28), cmap='gray')\n            title_str = f\"label: {batch['y_inst'][i][j].item()}\\nmask: {batch['mask'][i][j].item()}\"\n            ax.set_title(title_str)        \n            ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nbatch = next(iter(train_dataloader))\nprint(batch)\nplot_batch(batch, max_bags=4)\n</code></pre> <pre>\n<code>TensorDict(\n    fields={\n        X: Tensor(shape=torch.Size([32, 11, 784]), device=cpu, dtype=torch.float32, is_shared=False),\n        Y: Tensor(shape=torch.Size([32]), device=cpu, dtype=torch.int64, is_shared=False),\n        mask: Tensor(shape=torch.Size([32, 11]), device=cpu, dtype=torch.uint8, is_shared=False),\n        y_inst: Tensor(shape=torch.Size([32, 11]), device=cpu, dtype=torch.int64, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)\n</code>\n</pre> <p>Each batch is again a <code>TensorDict</code> with an additional key <code>mask</code> that indicates which instances are real and which are padding. As we can see, the bags are padded to the maximum size of the bags in the batch with zeros. The mask tensor indicates which elements are real instances and which are padding. The function <code>collate_fn</code> also pads other tensors, such as the adjacency matrix or the instance coordinates. </p> <pre><code>from torchmil.nn import masked_softmax\n\nclass ABMIL(torch.nn.Module):\n    def __init__(self, emb_dim, att_dim):\n        super().__init__()\n\n        # Feature extractor\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(28*28, 512),\n            torch.nn.ReLU(),\n            torch.nn.Linear(512, emb_dim),\n        )\n\n        self.fc1 = torch.nn.Linear(emb_dim, att_dim)\n        self.fc2 = torch.nn.Linear(att_dim, 1)\n\n        self.classifier = torch.nn.Linear(emb_dim, 1)       \n\n\n    def forward(self, X, mask, return_att=False):\n        X = self.mlp(X) # (batch_size, bag_size, emb_dim)\n        H = torch.tanh(self.fc1(X)) # (batch_size, bag_size, att_dim)\n        att = torch.sigmoid(self.fc2(H)) # (batch_size, bag_size, 1)\n        att_s = masked_softmax(att, mask) # (batch_size, bag_size, 1)\n        # att_s = torch.nn.functional.softmax(att, dim=1)\n        X = torch.bmm(att_s.transpose(1, 2), X).squeeze(1) # (batch_size, emb_dim)\n        y = self.classifier(X).squeeze(1) # (batch_size,)\n        if return_att:\n            return y, att_s\n        else:\n            return y\n\nmodel = ABMIL(emb_dim=256, att_dim=128)\nprint(model)\n</code></pre> <pre>\n<code>ABMIL(\n  (mlp): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=256, bias=True)\n  )\n  (fc1): Linear(in_features=256, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=1, bias=True)\n  (classifier): Linear(in_features=256, out_features=1, bias=True)\n)\n</code>\n</pre> <p>Great! Now, let's train the model. We are going to use the <code>torch.optim.Adam</code> optimizer and the <code>torch.nn.BCEWithLogitsLoss</code> loss function. We will train the model for 20 epochs.</p> <p>Thanks to the torchmil library, training a MIL model is as simple as training a standard PyTorch model. The training loop is straightforward, similar to the standard PyTorch training loop. We iterate over the training dataloader, compute the loss, and update the model parameters. We also track the loss and accuracy.</p> <pre><code>model = ABMIL(emb_dim=256, att_dim=128)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n\ndef train(dataloader, epoch):\n    model.train()\n\n    sum_loss = 0.0\n    sum_correct = 0.0\n    for batch in dataloader:\n        batch = batch.to(device)\n        out = model(batch['X'], batch['mask'])\n        loss = criterion(out, batch['Y'].float())\n        loss.backward()        \n        optimizer.step()\n        optimizer.zero_grad()\n\n        sum_loss += loss.item()\n        pred = (out &amp;gt; 0).float()\n        sum_correct += (pred == batch['Y']).sum().item()\n        sum_loss += loss.item()\n\n    print(f\"[Epoch {epoch}] Train, train/loss: {sum_loss / len(dataloader)}, 'train/bag/acc': {sum_correct / len(dataloader.dataset)}\")\n\ndef val(dataloader, epoch):\n    model.eval()\n\n    sum_loss = 0.0\n    sum_correct = 0.0\n    for batch in dataloader:\n        batch = batch.to(device)\n        out = model(batch['X'], batch['mask'])\n        loss = criterion(out, batch['Y'].float())\n\n        sum_loss += loss.item()\n        pred = (out &amp;gt; 0).float()\n        sum_correct += (pred == batch['Y']).sum().item()\n        sum_loss += loss.item()\n\n    print(f\"[Epoch {epoch}] Validation, val/loss: {sum_loss / len(dataloader)}, 'val/bag/acc': {sum_correct / len(dataloader.dataset)}\")\n\nmodel = model.to(device)\nfor epoch in range(20):\n    train(train_dataloader, epoch+1)\n    val(test_dataloader, epoch+1)\n</code></pre> <pre>\n<code>[Epoch 1] Train, train/loss: 0.3859690427304076, 'train/bag/acc': 0.9286\n[Epoch 1] Validation, val/loss: 0.1880752561939141, 'val/bag/acc': 0.9695\n[Epoch 2] Train, train/loss: 0.16442154182311589, 'train/bag/acc': 0.9739\n[Epoch 2] Validation, val/loss: 0.12515571917451562, 'val/bag/acc': 0.9845\n[Epoch 3] Train, train/loss: 0.14584087032063034, 'train/bag/acc': 0.9771\n[Epoch 3] Validation, val/loss: 0.1147860995538178, 'val/bag/acc': 0.9825\n[Epoch 4] Train, train/loss: 0.13143371639493556, 'train/bag/acc': 0.9804\n[Epoch 4] Validation, val/loss: 0.11624959915403336, 'val/bag/acc': 0.985\n[Epoch 5] Train, train/loss: 0.12332545817731477, 'train/bag/acc': 0.9827\n[Epoch 5] Validation, val/loss: 0.10535499797246996, 'val/bag/acc': 0.985\n[Epoch 6] Train, train/loss: 0.12535623266263463, 'train/bag/acc': 0.9815\n[Epoch 6] Validation, val/loss: 0.10129267238967475, 'val/bag/acc': 0.987\n[Epoch 7] Train, train/loss: 0.11201968839534888, 'train/bag/acc': 0.984\n[Epoch 7] Validation, val/loss: 0.11095998269165792, 'val/bag/acc': 0.9835\n[Epoch 8] Train, train/loss: 0.10963183950213269, 'train/bag/acc': 0.9831\n[Epoch 8] Validation, val/loss: 0.11002697482971209, 'val/bag/acc': 0.983\n[Epoch 9] Train, train/loss: 0.11524660762000721, 'train/bag/acc': 0.9836\n[Epoch 9] Validation, val/loss: 0.13182692171355326, 'val/bag/acc': 0.9785\n[Epoch 10] Train, train/loss: 0.10611518676773594, 'train/bag/acc': 0.9836\n[Epoch 10] Validation, val/loss: 0.12102683454692836, 'val/bag/acc': 0.981\n[Epoch 11] Train, train/loss: 0.09992601268793852, 'train/bag/acc': 0.9857\n[Epoch 11] Validation, val/loss: 0.09442376957408019, 'val/bag/acc': 0.986\n[Epoch 12] Train, train/loss: 0.09881719468331066, 'train/bag/acc': 0.9853\n[Epoch 12] Validation, val/loss: 0.11326536555064931, 'val/bag/acc': 0.983\n[Epoch 13] Train, train/loss: 0.09219979361360209, 'train/bag/acc': 0.9859\n[Epoch 13] Validation, val/loss: 0.10536213541432979, 'val/bag/acc': 0.984\n[Epoch 14] Train, train/loss: 0.0933219926556745, 'train/bag/acc': 0.9872\n[Epoch 14] Validation, val/loss: 0.12082792839242353, 'val/bag/acc': 0.9805\n[Epoch 15] Train, train/loss: 0.08617063817350915, 'train/bag/acc': 0.9869\n[Epoch 15] Validation, val/loss: 0.10966313540166805, 'val/bag/acc': 0.984\n[Epoch 16] Train, train/loss: 0.08572717891714443, 'train/bag/acc': 0.9888\n[Epoch 16] Validation, val/loss: 0.0979384019497841, 'val/bag/acc': 0.9855\n[Epoch 17] Train, train/loss: 0.08640271301170031, 'train/bag/acc': 0.9869\n[Epoch 17] Validation, val/loss: 0.09659811220915308, 'val/bag/acc': 0.987\n[Epoch 18] Train, train/loss: 0.08080236079876486, 'train/bag/acc': 0.9875\n[Epoch 18] Validation, val/loss: 0.09953816693335299, 'val/bag/acc': 0.986\n[Epoch 19] Train, train/loss: 0.07695601370182638, 'train/bag/acc': 0.9889\n[Epoch 19] Validation, val/loss: 0.11378709812988601, 'val/bag/acc': 0.983\n[Epoch 20] Train, train/loss: 0.07340383955973763, 'train/bag/acc': 0.9883\n[Epoch 20] Validation, val/loss: 0.10783347497058529, 'val/bag/acc': 0.9865\n</code>\n</pre> <p>The loss decreases as the model learns to predict the bag labels. The accuracy increases as the model learns to predict the correct bag labels. This is a good sign that the model is learning!</p> <p>Let's evaluate the model. We are going to compute the accuracy and f1-score on the test set. The accuracy is the proportion of correctly classified bags, while the f1-score is the harmonic mean of precision and recall. The f1-score is a good metric for imbalanced datasets. Typically, in MIL datasets, there are many more negative instances than positive instances. In this case, the f1-score will be very useful.</p> <p>To compute predictions at the instance level, we are going to use the attention values. They indicate the importance given by the model to each instance in the bag. As explained in the ABMIL paper, in positive bags, the model should give more importance to the positive instances, so the attention values should be higher for positive instances. </p> <p>Instance-level predictions have certain caveats due to padding. When performing operations such as normalizing across the bag, it\u2019s crucial to handle padded instances carefully, as they can affect predictions. Additionally, padded instances should be excluded when computing metrics to ensure accuracy.</p> <p>First, we define some auxiliary functions. Take a look at how we handle padded instances in <code>normalize</code>.</p> <pre><code>def accuracy(pred, y):\n    return (pred == y).sum().item() / len(y)\n\ndef f1_score(pred, y):\n    tp = ((pred == 1) &amp;amp; (y == 1)).sum().item()\n    fp = ((pred == 1) &amp;amp; (y == 0)).sum().item()\n    fn = ((pred == 0) &amp;amp; (y == 1)).sum().item()\n    precision = tp / (tp + fp + 1e-6)\n    recall = tp / (tp + fn + 1e-6)\n    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n    return f1\n\ndef normalize(att, mask = None):\n    if mask is None:\n        mask = torch.ones_like(att)\n    else:\n        mask = mask.unsqueeze(-1)\n\n    # exclude masked values from computing min and max\n    att_tmp = att.clone()\n    att_tmp[mask == 0] = float('inf')\n    att_min = att_tmp.min(dim=1, keepdim=True).values\n    att_tmp[mask == 0] = float('-inf')\n    att_max = att_tmp.max(dim=1, keepdim=True).values\n    att_min = att_min.expand_as(att)\n    att_max = att_max.expand_as(att)\n\n    # padded instances are set to min value, so that after normalization are set to 0 \n    att = torch.where(mask == 0, att_min, att)\n\n    # normalize\n    att_norm = (att - att_min) / (att_max - att_min + 1e-10)\n    return att_norm\n</code></pre> <p>Now, we compute the predictions at both the bag and instance level. Take a look at how we handle the padded instances!</p> <pre><code>inst_pred_list = []\ny_inst_list = []\nY_pred_list = []\nY_list = []\n\nfor batch in test_dataloader:\n    batch = batch.to(device)\n\n    # predict bag label and attention\n    out, att = model(batch['X'], batch['mask'], return_att=True)\n    Y_pred = (out &amp;gt; 0).float()\n\n    # normalize attention\n    att_norm = normalize(att, batch['mask'])\n\n    # remove attention corresponding to padded instances\n    att_norm = att_norm.view(-1)[batch['mask'].view(-1) == 1]\n    inst_pred = (att_norm &amp;gt; 0.5).float()\n\n    # remove labels corresponding to padded instances\n    y_inst = batch['y_inst'].view(-1)[batch['mask'].view(-1) == 1]\n\n    inst_pred_list.append(inst_pred)\n    y_inst_list.append(y_inst)\n    Y_pred_list.append(Y_pred)\n    Y_list.append(batch['Y'])\n\n\ninst_pred = torch.cat(inst_pred_list)\ny_inst = torch.cat(y_inst_list)\nY_pred = torch.cat(Y_pred_list)\nY = torch.cat(Y_list)\n\nprint(f\"test/bag/acc: {accuracy(Y_pred, Y)}\")\nprint(f\"test/bag/f1: {f1_score(Y_pred, Y)}\")\nprint(f\"test/inst/acc: {accuracy(inst_pred, y_inst)}\")\nprint(f\"test/inst/f1: {f1_score(inst_pred, y_inst)}\")\n</code></pre> <pre>\n<code>test/bag/acc: 0.9865\ntest/bag/f1: 0.9865062456410512\ntest/inst/acc: 0.9869223096531087\ntest/inst/f1: 0.9446524078781653\n</code>\n</pre> <p>Good! Our model is working well. The accuracy and f1-score are high. This means that the attention values are correctly indicating the importance of the instances in the bag. Let's visualize the attention values for some bags.</p> <p>Finally, we are going to take a look at the attention maps, which show the importance of each instance in the bag. The attention maps are computed by normalizing the attention values across the bag. Also, we highlight the instances predicted as positive in the positive bags. </p> <pre><code>import numpy as np\n\ndef plot_batch_att(batch, att, max_bags=5):\n    batch_size = min(len(batch['X']), max_bags)\n    bag_size = len(batch['X'][0])\n    fig, axes = plt.subplots(batch_size, bag_size, figsize=(bag_size, 1.5*batch_size))\n    for i in range(batch_size):\n        for j in range(bag_size):\n            ax = axes[i, j]\n            ax.imshow(batch['X'][i][j].view(28, 28), cmap='gray')\n            # if the bag is positive, overlay red mask on the instance predicted as positive\n            if batch['Y'][i] == 1:\n                att_bag = normalize(att[i].unsqueeze(0), batch['mask'][i].unsqueeze(0)).squeeze(0)\n                pred = (att_bag &amp;gt; 0.5).float()\n                overlay_red = np.full((28, 28, 4), [1., 0., 0., 0.4*pred[j].item()])\n                ax.imshow(overlay_red)\n            title_str = f\"label: {batch['y_inst'][i][j].item()}\\natt: {att[i][j].item():.2}\"\n            ax.set_title(title_str)\n            ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nbatch = next(iter(test_dataloader))\nbatch = batch.to(device)\n_, att = model(batch['X'], batch['mask'], return_att=True)\nbatch = batch.detach().cpu()\natt = att.detach().cpu()\nplot_batch_att(batch, att, max_bags=10)\n</code></pre> <p>As we can see, the model assigns more importance to the positive instances. This is a good sign! This means that the model itself learned to predict the positive instances correctly, even though it was not trained with instance-level labels. This is the power of Multiple Instance Learning!</p>"},{"location":"examples/training_your_first_mil_model/#training-your-first-mil-model","title":"Training your first MIL model","text":"<p>In the following, we explain how to train a simple Multiple Instance Learning (MIL) model using the torchmil library.</p>"},{"location":"examples/training_your_first_mil_model/#the-dataset","title":"The dataset","text":"<p>MIL binary classification</p> <p>In this case, the bags have the form \\(\\mathbf{X} = \\left[ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right]^\\top \\in \\mathbb{R}^{N \\times D}\\), where each \\(\\mathbf{x}_n \\in \\mathbb{R}^D\\) is an instance.  The labels of the instances are \\(\\mathbf{y} = \\left[ y_1, \\ldots, y_N \\right]^\\top \\in \\{0, 1\\}^N\\), but we do not have access to them at training time (they may be accessible at test time). The label of the bag is \\(Y \\in \\{0, 1\\}\\), and the relation between the instance labels and the bag label is as follows:</p> \\[ Y = \\max \\left\\{ y_1, \\ldots, y_N \\right\\} \\] <p>This example is the most common in MIL, but there are many other possibilities. </p> <p>We will create a synthetic MIL dataset from the MNIST dataset using the <code>ToyDataset</code> from the torchmil.datasets module. The <code>ToyDataset</code> generates an MIL dataset from a labeled dataset by specifying the desired bag size and the objective labels, which determine positive instances. Subsequently, <code>ToyDataset</code> constructs bags of the specified size and labels them based on these objective labels. For further details, refer to here for more details.</p> <p>In this example, we are going to set <code>obj_labels=[2, 5, 8]</code>. This means that a bag is positive if it contains at least one instance with label 5 or 8.</p>"},{"location":"examples/training_your_first_mil_model/#mini-batching-of-bags","title":"Mini-batching of bags","text":"<p>Tipically, the bags in a MIL dataset have different size. This can be a problem when creating mini-batches. To solve this, we use the function <code>collate_fn</code> from the torchmil.data module. This function creates a mini-batch of bags by padding the bags with zeros to the size of the largest bag in the batch. The function also returns a mask tensor that indicates which instances are real and which are padding.</p> <p>Why not use <code>torch.nested</code>?</p> <p><code>torch.nested</code> offer a more flexible method for handling bags of varying sizes. However, since the PyTorch API for nested tensors is still in the prototype stage, torchmil currently relies on the padding approach.</p> <p>Let's create the dataloaders and visualize some bags from a mini-batch.</p>"},{"location":"examples/training_your_first_mil_model/#the-model","title":"The model","text":"<p>In this example, we are going to use the ABMIL model.  Although it is readily available in the torchmil.models module, we are going to implement a simple variation from scratch to show how to create a custom MIL model.</p>"},{"location":"examples/training_your_first_mil_model/#training-the-model","title":"Training the model","text":""},{"location":"examples/training_your_first_mil_model/#evaluating-the-model","title":"Evaluating the model","text":""},{"location":"examples/training_your_first_mil_model/#inspecting-the-predictions","title":"Inspecting the predictions","text":""}]}