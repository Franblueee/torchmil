{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "def get_emb(sin_inp):\n",
    "    \"\"\"\n",
    "    Gets a base embedding for one dimension with sin and cos intertwined\n",
    "    \"\"\"\n",
    "    emb = torch.stack((sin_inp.sin(), sin_inp.cos()), dim=-1)\n",
    "    return torch.flatten(emb, -2, -1)\n",
    "\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, channels, dtype_override=None):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        :param dtype_override: If set, overrides the dtype of the output embedding.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "        self.org_channels = channels\n",
    "        channels = int(np.ceil(channels / 4) * 2)\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.register_buffer(\"cached_penc\", None, persistent=False)\n",
    "        self.dtype_override = dtype_override\n",
    "        self.channels = channels\n",
    "\n",
    "        print('Channels:', channels)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: A 4d tensor of size (batch_size, x, y, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 4:\n",
    "            raise RuntimeError(\"The input tensor has to be 4d!\")\n",
    "\n",
    "        if self.cached_penc is not None and self.cached_penc.shape == tensor.shape:\n",
    "            return self.cached_penc\n",
    "\n",
    "        self.cached_penc = None\n",
    "        batch_size, x, y, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device, dtype=self.inv_freq.dtype)\n",
    "        pos_y = torch.arange(y, device=tensor.device, dtype=self.inv_freq.dtype)\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
    "        emb_x = get_emb(sin_inp_x).unsqueeze(1)\n",
    "        emb_y = get_emb(sin_inp_y)\n",
    "        emb = torch.zeros(\n",
    "            (x, y, self.channels * 2),\n",
    "            device=tensor.device,\n",
    "            dtype=(\n",
    "                self.dtype_override if self.dtype_override is not None else tensor.dtype\n",
    "            ),\n",
    "        )\n",
    "        emb[:, :, : self.channels] = emb_x\n",
    "        emb[:, :, self.channels : 2 * self.channels] = emb_y\n",
    "\n",
    "        print(emb.shape)\n",
    "\n",
    "        self.cached_penc = emb[None, :, :, :orig_ch].repeat(tensor.shape[0], 1, 1, 1)\n",
    "        return self.cached_penc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingND(nn.Module):\n",
    "    def __init__(self, n_dim, channels, dtype_override=None):\n",
    "        \"\"\"\n",
    "        Positional encoding for N-dimensional tensors.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        super(PositionalEncodingND, self).__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.org_channels = channels\n",
    "        channels = int(np.ceil(channels / (2*n_dim)) * 2)\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.dtype_override = dtype_override\n",
    "        self.channels = channels\n",
    "\n",
    "        print(\"Channels: \", channels)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: A 2+nd tensor of size (batch_size, x1, x2, ..., xn, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x1, x2, ..., xn, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != self.n_dim + 2:\n",
    "            raise RuntimeError(\"The input tensor has to be {}d!\".format(self.n_dim + 2))\n",
    "\n",
    "\n",
    "        shape = tensor.shape\n",
    "\n",
    "        orig_ch = shape[-1]\n",
    "        emb_shape = list(shape)[1:]\n",
    "        emb_shape[-1] = self.channels * self.n_dim\n",
    "\n",
    "        emb = torch.zeros(\n",
    "            emb_shape,\n",
    "            device=tensor.device,\n",
    "            dtype=(\n",
    "                self.dtype_override if self.dtype_override is not None else tensor.dtype\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        for i in range(self.n_dim):\n",
    "            pos = torch.arange(shape[i+1], device=tensor.device, dtype=self.inv_freq.dtype)\n",
    "            sin_inp = torch.einsum(\"i,j->ij\", pos, self.inv_freq)\n",
    "            emb_i = get_emb(sin_inp)\n",
    "            for _ in range(self.n_dim-i-1):\n",
    "                emb_i = emb_i.unsqueeze(1)\n",
    "            \n",
    "            emb[..., i*self.channels : (i+1)*self.channels] = emb_i\n",
    "\n",
    "        return emb[None, ..., :orig_ch].repeat(shape[0], *(1 for _ in range(self.n_dim)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_enc_nd = PositionalEncodingND(4, 10)\n",
    "\n",
    "x = torch.randn(1, 5, 5, 5, 5, 10)\n",
    "\n",
    "\n",
    "# Apply the positional encoding\n",
    "pos_enc_x_nd = pos_enc_nd(x)\n",
    "\n",
    "print(pos_enc_x_nd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_spatial_representation(\n",
    "        X : torch.Tensor,\n",
    "        coords : torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the spatial representation of a bag given the sequential representation and the coordinates.\n",
    "\n",
    "    Given the input tensor `X` of shape `(batch_size, bag_size, dim)` and the coordinates `coords` of shape `(batch_size, bag_size, n)`, \n",
    "    this function returns the spatial representation `X_enc` of shape `(batch_size, coord1, coord2, ..., coordn, dim)`.\n",
    "\n",
    "    This representation is characterized by the fact that the coordinates are used to index the elements of spatial representation:\n",
    "    `X_enc[batch, i1, i2, ..., in, :] = X[batch, idx, :]` where `(i1, i2, ..., in) = coords[batch, idx]`.\n",
    "\n",
    "    Arguments:\n",
    "        X (Tensor): Sequential representation of shape `(batch_size, bag_size, dim)`.\n",
    "        coords (Tensor): Coordinates of shape `(batch_size, bag_size, n)`.\n",
    "    \n",
    "    Returns:\n",
    "        X_esp: Spatial representation of shape `(batch_size, coord1, coord2, ..., coordn, dim)`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the shape of the spatial representation\n",
    "    batch_size = X.shape[0]\n",
    "    bag_size = X.shape[1]\n",
    "    n = coords.shape[-1]\n",
    "    shape = torch.Size([X.shape[0]] + [int(coords[:, :, i].max().item()) + 1 for i in range(n)] + [X.shape[-1]])\n",
    "\n",
    "    # Initialize the spatial representation\n",
    "    X_enc = torch.zeros(shape, device=X.device, dtype=X.dtype)\n",
    "\n",
    "    # Create batch indices of shape (batch_size, bag_size)\n",
    "    batch_indices = torch.arange(batch_size, device=X.device).unsqueeze(1).expand(-1, bag_size)\n",
    "\n",
    "    # Create a list of spatial indices (one per coordinate dimension), each of shape (batch_size, bag_size)\n",
    "    spatial_indices = [coords[:, :, i] for i in range(n)]\n",
    "\n",
    "    # Build the index tuple without using the unpack operator in the subscript.\n",
    "    index_tuple = (batch_indices,) + tuple(spatial_indices)\n",
    "\n",
    "    # Use advanced indexing to assign values from X into X_enc.\n",
    "    X_enc[index_tuple] = X\n",
    "\n",
    "\n",
    "    return X_enc\n",
    "\n",
    "def get_seq_representation(\n",
    "        X_esp : torch.Tensor,\n",
    "        coords : torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the sequential representation of a bag given the spatial representation and the coordinates.\n",
    "\n",
    "    Given the spatial tensor `X_esp` of shape `(batch_size, coord1, coord2, ..., coordn, dim)` and the coordinates `coords` of shape `(batch_size, bag_size, n)`, \n",
    "    this function returns the sequential representation `X` of shape `(batch_size, bag_size, dim)`.\n",
    "\n",
    "    This representation is characterized by the fact that the coordinates are used to index the elements of spatial representation:\n",
    "    `X_seq[batch, idx, :] = X_esp[batch, i1, i2, ..., in, :]` where `(i1, i2, ..., in) = coords[batch, idx]`.\n",
    "\n",
    "    Arguments:\n",
    "        X_esp (Tensor): Spatial representation of shape `(batch_size, coord1, coord2, ..., coordn, dim)`.\n",
    "        coords (Tensor): Coordinates of shape `(batch_size, bag_size, n)`.\n",
    "    \n",
    "    Returns:\n",
    "        X_seq: Sequential representation of shape `(batch_size, bag_size, dim)`.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = X_esp.shape[0]\n",
    "    bag_size = coords.shape[1]\n",
    "    n = coords.shape[-1]\n",
    "\n",
    "    # Create batch indices with shape (batch_size, bag_size)\n",
    "    batch_indices = torch.arange(batch_size, device=X_esp.device).unsqueeze(1).expand(-1, bag_size)\n",
    "\n",
    "    # Build the index tuple without using the unpack operator in the subscript.\n",
    "    # Each element in the tuple has shape (batch_size, bag_size)\n",
    "    index_tuple = (batch_indices,) + tuple(coords[:, :, i] for i in range(n))\n",
    "\n",
    "    # Use advanced indexing to extract the sequential representation from X_esp.\n",
    "    # The result will have shape (batch_size, bag_size, dim)\n",
    "    X_seq = X_esp[index_tuple]\n",
    "\n",
    "    return X_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "bag_size = 5\n",
    "dim = 1\n",
    "n = 2\n",
    "\n",
    "X_seq = torch.randn(batch_size, bag_size, dim)\n",
    "\n",
    "# create random coordinates. ensure there are no duplicates\n",
    "repeat = True\n",
    "while repeat:\n",
    "    coords = torch.randint(0, 5, (batch_size, bag_size, n))\n",
    "    repeat = False\n",
    "    for b in range(batch_size):\n",
    "        coords_list = []\n",
    "        for i in range(bag_size):\n",
    "            coords_list.append(tuple(coords[b, i, :].tolist()))\n",
    "        coords_set = set(coords_list)\n",
    "        if len(coords_set) != len(coords_list):\n",
    "            repeat = True\n",
    "            break\n",
    "\n",
    "\n",
    "X_esp = get_spatial_representation(X_seq, coords)\n",
    "\n",
    "X_seq_reconstructed = get_seq_representation(X_esp, coords)\n",
    "X_esp_reconstructed = get_spatial_representation(X_seq_reconstructed, coords)\n",
    "\n",
    "\n",
    "print('X_seq:', X_seq.shape)\n",
    "print('X_esp:', X_esp.shape)\n",
    "print('X_seq_reconstructed:', X_seq_reconstructed.shape)\n",
    "print('X_esp_reconstructed:', X_esp_reconstructed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_seq:', X_seq)\n",
    "print('X_seq_reconstructed:', X_seq_reconstructed)\n",
    "print('X_esp:', X_esp)\n",
    "print('X_esp_reconstructed:', X_esp_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - min(x)) / (max(x) - min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_unique(x):\n",
    "    \"\"\"\n",
    "    Quantize a list of real numbers into unique integers based on their rank.\n",
    "    \n",
    "    Parameters:\n",
    "        x (list or np.ndarray): List of n real numbers.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of unique integers (0 to n-1) corresponding to the rank order of x.\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    order = np.argsort(x)\n",
    "    ranks = np.empty_like(order)\n",
    "    ranks[order] = np.arange(len(x))\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 5\n",
    "x = np.random.rand(n)\n",
    "\n",
    "print('x:', x)\n",
    "print('quantize_unique(x):', quantize_unique(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "def quantize_coords_1d(x):\n",
    "    \"\"\"\n",
    "    Quantize a list of real numbers into unique integers.\n",
    "    \n",
    "    Parameters:\n",
    "        x (list or np.ndarray): List of n real numbers.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of unique integers (0 to n-1) corresponding to the rank order of x.\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    x = normalize(x)\n",
    "\n",
    "    # find the minimum distance between two points\n",
    "    min_dist = np.min(np.diff(np.sort(x)))\n",
    "\n",
    "    m = np.ceil(1 / min_dist)\n",
    "    x_quant = np.round(x * m).astype(int)\n",
    "    return x_quant\n",
    "\n",
    "n = 5\n",
    "x = np.random.rand(n)\n",
    "\n",
    "print('x:', x)\n",
    "print('quantize_coords_1d(x):', quantize_coords_1d(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "def quantize_coords_nd(X):\n",
    "    \"\"\"\n",
    "    Quantize N-dimensional coordinates into unique discrete integers while preserving relative distances.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.ndarray): Array of shape (n, d) representing n points in d-dimensional space.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Quantized coordinates of shape (n, d).\n",
    "    \"\"\"\n",
    "    X = np.array(X, dtype=float)\n",
    "    X = normalize(X)\n",
    "\n",
    "    # Compute all pairwise distances in the normalized space\n",
    "    from scipy.spatial import distance_matrix\n",
    "    dist_matrix = distance_matrix(X, X)\n",
    "    np.fill_diagonal(dist_matrix, np.inf)  # Ignore self-distances\n",
    "\n",
    "    # Find the smallest nonzero pairwise distance\n",
    "    min_dist = np.min(dist_matrix)\n",
    "\n",
    "    if min_dist == 0:\n",
    "        min_dist = np.min(dist_matrix[dist_matrix > 0])  # Smallest nonzero distance\n",
    "\n",
    "    # Compute scaling factor and quantize\n",
    "    m = np.ceil(1 / min_dist)\n",
    "    X_quant = np.round(X * m).astype(int)\n",
    "\n",
    "    return X_quant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 10000\n",
    "x = np.random.rand(n, 20)\n",
    "\n",
    "# print('x:', x)\n",
    "print('quantize_coords(x):', quantize_coords_nd(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A():\n",
    "    def __init__(self):\n",
    "        print('A init')\n",
    "        self.d = 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('A forward')\n",
    "        return x\n",
    "\n",
    "class B():\n",
    "    def __init__(self, mode='r'):\n",
    "        print('B init, mode:', mode)\n",
    "        self.r = 5\n",
    "\n",
    "class B_p(A, B):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        B.__init__(self, 'w')\n",
    "\n",
    "b = B_p()\n",
    "b.forward(1)\n",
    "b.d, b.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "soft_split1 = torch.nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "\n",
    "bs = 1\n",
    "seq_len = 10\n",
    "h = 5\n",
    "w = 5\n",
    "\n",
    "x = torch.randn(bs, seq_len, h, w) # (bs, seq_len, h, w)\n",
    "soft_split1(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 10, 5, 8)\n",
    "x.transpose(1, -1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchmil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
