{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "<tt>torchmil</tt> provides a framework to instantiate datasets for Multiple Instance Learning (MIL) problems. It allows users to create custom datasets that suit their specific needs. In addition, <tt>torchmil</tt> includes some pre-defined datasets that can be used directly. These correspond to popular benchmark datasets in the field of MIL, such as [Camelyon16](https://camelyon16.grand-challenge.org/). See [<tt><b>torchmil.datasets</b></tt>](../api/datasets/index.md) for a complete list of the datasets available in <tt>torchmil</tt>.\n",
    "\n",
    "In the following, we explain the logic behind the design of datasets in <tt>torchmil</tt>, the required data and folder structure, and how to create your own dataset. We will also provide a simple example of how to use the `ProcessedMILDataset` class to create a custom dataset.\n",
    "\n",
    "!!! question \"Data representation in <tt>torchmil</tt>\"\n",
    "    Take a look at the [data representation example](../examples/data_representation/) example to see how the data is represented in <tt>torchmil</tt>.\n",
    "\n",
    "    In <tt><b>torchmil</b></tt>, bags are represented as a `TensorDict` object with at least the following properties:\n",
    "\n",
    "    - `bag['X']`: it is usually called _bag feature matrix_, since it represents feature vectors extracted from the raw representation of the instances. \n",
    "    - `bag['Y']`: it represents the label of the bag. \n",
    "\n",
    "    Additionally, a bag may contain other properties. The most common ones are:\n",
    "\n",
    "    - `bag['y_inst']`: it contains the labels of the instances in the bag. \n",
    "    - `bag['adj']`: it contains the adjacency matrix of the bag, which represents the relationships between the instances in the bag.\n",
    "    - `bag['coords']`: it contains the coordinates of the instances in the bag, which represent the absolute position of the instances in the bag.\n",
    "\n",
    "## The `ProcessedMILDataset` class\n",
    "\n",
    "The [ProcessedMILDataset class](../api/datasets/preprocessed_mil_dataset.md) allows for efficient loading and processing of large datasets. To enable this, it expects each bag to have been pre-processed, saving its properties in separate files:\n",
    "- A feature file should yield an array of shape `(bag_size, ...)`, where `...` represents the shape of the features.\n",
    "- A label file should yield an array of shape arbitrary shape, e.g., `(1,)` for binary classification.\n",
    "- An instance label file should yield an array of shape `(bag_size, ...)`, where `...` represents the shape of the instance labels.\n",
    "- A coordinates file should yield an array of shape `(bag_size, coords_dim)`, where `coords_dim` is the dimension of the coordinates.\n",
    "\n",
    "The path to these properties should be specified in the `__init__` method of the `ProcessedMILDataset` class. To illustrate this behaviour, let's load the CAMELYON16 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bags: 399\n"
     ]
    }
   ],
   "source": [
    "from torchmil.datasets import ProcessedMILDataset\n",
    "\n",
    "dataset_dir = \"/data/datasets/CAMELYON16\"\n",
    "features_path = \"/data/datasets/CAMELYON16/patches_512_preset/features_UNI/\"\n",
    "labels_path = \"/data/datasets/CAMELYON16/patches_512_preset/labels/\"\n",
    "inst_labels_path = \"/data/datasets/CAMELYON16/patches_512_preset/patch_labels/\"\n",
    "coords_path = \"/data/datasets/CAMELYON16/patches_512_preset/coords/\"\n",
    "\n",
    "dataset = ProcessedMILDataset(\n",
    "    features_path=features_path,\n",
    "    labels_path=labels_path,\n",
    "    inst_labels_path=inst_labels_path,\n",
    "    coords_path=coords_path\n",
    ")\n",
    "\n",
    "print(f\"Number of bags: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have specified the path to the properties of each bag. The `ProcessedMILDataset` class will load the properties of each bag from the specified files assuming the following structure:\n",
    "```\n",
    "features_path/\n",
    "├── bag1.npy\n",
    "├── bag2.npy\n",
    "└── ...\n",
    "labels_path/\n",
    "├── bag1.npy\n",
    "├── bag2.npy\n",
    "└── ...\n",
    "inst_labels_path/\n",
    "├── bag1.npy\n",
    "├── bag2.npy\n",
    "└── ...\n",
    "coords_path/\n",
    "├── bag1.npy\n",
    "├── bag2.npy\n",
    "└── ...\n",
    "```\n",
    "Let's take a look at one of the bags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([1983, 1024])\n",
      "Y: torch.Size([1])\n",
      "y_inst: torch.Size([1983])\n",
      "adj: torch.Size([1983, 1983])\n",
      "coords: torch.Size([1983, 2])\n"
     ]
    }
   ],
   "source": [
    "bag = dataset[0]\n",
    "for key in bag.keys():\n",
    "    print(f\"{key}: {bag[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `__getitem__` method is called, the `ProcessedMILDataset` class builds the bag. First, it loads the properties of the bag from the specified files. Then, if the coordinates have been provided, it builds the adjacency matrix of the bag (see the documentation for more details). Finally, it creates a `TensorDict` object with the properties of the bag. The `__getitem__` method then returns the `TensorDict` object with the properties of the bag.\n",
    "\n",
    "We can choose which bags we want to load using the `bag_names` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bags: 2\n",
      "X: torch.Size([12255, 1024])\n",
      "Y: torch.Size([1])\n",
      "y_inst: torch.Size([12255])\n",
      "adj: torch.Size([12255, 12255])\n",
      "coords: torch.Size([12255, 2])\n"
     ]
    }
   ],
   "source": [
    "bag_names = [\"test_001\", \"test_002\"]\n",
    "dataset = ProcessedMILDataset(\n",
    "    features_path=features_path,\n",
    "    labels_path=labels_path,\n",
    "    inst_labels_path=inst_labels_path,\n",
    "    coords_path=coords_path,\n",
    "    bag_names=bag_names\n",
    ")\n",
    "print(f\"Number of bags: {len(dataset)}\")\n",
    "bag = dataset[0]\n",
    "for key in bag.keys():\n",
    "    print(f\"{key}: {bag[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose which properties we want to load using the `bag_keys` argument. For example, if we want to load only the features and the labels of the bags, we can do it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bags: 399\n",
      "X: torch.Size([1983, 1024])\n",
      "Y: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "dataset = ProcessedMILDataset(\n",
    "    features_path=features_path,\n",
    "    labels_path=labels_path,\n",
    "    inst_labels_path=inst_labels_path,\n",
    "    coords_path=coords_path,\n",
    "    bag_keys=['X', 'Y'],\n",
    ")\n",
    "print(f\"Number of bags: {len(dataset)}\")\n",
    "bag = dataset[0]\n",
    "for key in bag.keys():\n",
    "    print(f\"{key}: {bag[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to see all the options in the [documentation](../api/datasets/processed_mil_dataset.md).\n",
    "\n",
    "## Extending the `ProcessedMILDataset` class.\n",
    "\n",
    "The `ProcessedMILDataset` can be extended to add custom functionalities. One example is the `BinaryClassificationDataset` class, which is a subclass of `ProcessedMILDataset` that is tailored for binary classification tasks. It assumes that the bag label $Y$ and the instance labels $\\left\\{ y_1, \\ldots, y_N \\right\\}$ are binary values, i.e., they can take values in $\\left\\{ 0, 1 \\right\\}$. The class also assumes that the bag label is the maximum of the instance labels, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "Y = \\max \\left\\{ y_1, \\ldots, y_N \\right\\}.\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Let's take a look at the implementation to illustrate how to extend the `ProcessedMILDataset` class. The `BinaryClassificationDataset` class is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "class BinaryClassificationDataset(ProcessedMILDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features_path: str,\n",
    "        labels_path: str,\n",
    "        inst_labels_path: str = None,\n",
    "        coords_path: str = None,\n",
    "        bag_names: list = None,\n",
    "        bag_keys: list = [\"X\", \"Y\", \"y_inst\", \"adj\", \"coords\"],\n",
    "        dist_thr: float = 1.5,\n",
    "        adj_with_dist: bool = False,\n",
    "        norm_adj: bool = True,\n",
    "        load_at_init: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            features_path=features_path,\n",
    "            labels_path=labels_path,\n",
    "            inst_labels_path=inst_labels_path,\n",
    "            coords_path=coords_path,\n",
    "            bag_names=bag_names,\n",
    "            bag_keys=bag_keys,\n",
    "            dist_thr=dist_thr,\n",
    "            adj_with_dist=adj_with_dist,\n",
    "            norm_adj=norm_adj,\n",
    "            load_at_init=load_at_init,\n",
    "        )\n",
    "\n",
    "    def _fix_inst_labels(self, inst_labels):\n",
    "        \"\"\"\n",
    "        Make sure that instance labels have shape (bag_size,).\n",
    "        \"\"\"\n",
    "        if inst_labels is not None:\n",
    "            while inst_labels.ndim > 1:\n",
    "                inst_labels = np.squeeze(inst_labels, axis=-1)\n",
    "        return inst_labels\n",
    "\n",
    "    def _fix_labels(self, labels):\n",
    "        \"\"\"\n",
    "        Make sure that labels have shape ().\n",
    "        \"\"\"\n",
    "        labels = np.squeeze(labels)\n",
    "        return labels\n",
    "\n",
    "    def _load_inst_labels(self, name):\n",
    "        inst_labels = super()._load_inst_labels(name)\n",
    "        inst_labels = self._fix_inst_labels(inst_labels)\n",
    "        return inst_labels\n",
    "\n",
    "    def _load_labels(self, name):\n",
    "        labels = super()._load_labels(name)\n",
    "        labels = self._fix_labels(labels)\n",
    "        return labels\n",
    "\n",
    "    def _consistency_check(self, bag_dict, name):\n",
    "        \"\"\"\n",
    "        Check if the instance labels are consistent with the bag label.\n",
    "        \"\"\"\n",
    "        if 'Y' in bag_dict:\n",
    "            if 'y_inst' in bag_dict:\n",
    "                if bag_dict['Y'] != (bag_dict['y_inst']).max():\n",
    "                    msg = f\"Instance labels (max(y_inst)={(bag_dict['y_inst']).max()}) are not consistent with bag label (Y={bag_dict['Y']}) for bag {name}. Setting all instance labels to -1 (unknown).\"\n",
    "                    warnings.warn(msg)\n",
    "                    bag_dict['y_inst'] = np.full((bag_dict['X'].shape[0],), -1)\n",
    "            else:\n",
    "                if bag_dict['Y'] == 0:\n",
    "                    bag_dict['y_inst'] = np.zeros(bag_dict['X'].shape[0])\n",
    "                else:\n",
    "                    msg = f'Instance labels not found for bag {name}. Setting all to -1.'\n",
    "                    warnings.warn(msg)\n",
    "                    bag_dict['y_inst'] = np.full((bag_dict['X'].shape[0],), -1)\n",
    "        return bag_dict\n",
    "\n",
    "    def _load_bag(self, name: str) -> dict[str, torch.Tensor]:\n",
    "        bag_dict = super()._load_bag(name)\n",
    "        bag_dict = self._consistency_check(bag_dict, name)\n",
    "        return bag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have added explicit comprobations to ensure that the above conditions are fullfilled. If they are not, a warning is shown on the output stream. All we need to do is to override the corresponding methods to add the desired functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating your own dataset\n",
    "\n",
    "Finally, let's implement a custom dataset. For this, we will use the [`WSIDataset` class](../api/datasets/wsi_dataset.md), which assumes that the bags are Whole Slide Images (WSIs). It also gives the coordinates of the patches (`coords`) a special treatment, normalizing their values.\n",
    "\n",
    "We are going to use the slides from the [Genotype-Tissue Expression (GTEx) Project](https://www.gtexportal.org/home/), which can be downloaded for free. Particularly, we will use slides of <tt>UrinaryBladder</tt> tissue. \n",
    "\n",
    "To create the dataset, we must first extract the `coords` of the patches from the original <tt>.tiff</tt> files and then extract `features` from those patches. To achieve that, a tool like [CLAM](https://github.com/mahmoodlab/CLAM) can be used. We will assume that no fine-grained annotations, so we will not have access to `labels` or `inst_labels`. We have extracted the features using the foundation model [UNI](https://huggingface.co/MahmoodLab/UNI).\n",
    "\n",
    "Then, creating the dataset is as simple as defining a new class that extends `WSIDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmil.datasets import WSIDataset\n",
    "from torchmil.utils.common import read_csv, keep_only_existing_files\n",
    "\n",
    "\n",
    "class GTExUrinaryBladderDataset(WSIDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root : str,\n",
    "        features : str = 'UNI',\n",
    "        bag_keys: list = [\"X\", \"adj\", \"coords\"],\n",
    "        patch_size: int = 512,\n",
    "        adj_with_dist: bool = False,\n",
    "        norm_adj: bool = True,\n",
    "        load_at_init: bool = True\n",
    "    ) -> None:\n",
    "\n",
    "        features_path = f'{root}/patches_{patch_size}/features/features_{features}/'\n",
    "        labels_path = f'{root}/patches_{patch_size}/labels/'\n",
    "        patch_labels_path = f'{root}/patches_{patch_size}/inst_labels/'\n",
    "        coords_path = f'{root}/patches_{patch_size}/coords/'\n",
    "\n",
    "        # This csv is generated by CLAM, with slide_id containing \"bag_name.format\"\n",
    "        bag_names_file = f'{root}/patches_{patch_size}/process_list_autogen.csv'\n",
    "        dict_list = read_csv(bag_names_file)\n",
    "        wsi_names = list(set([ row['slide_id'].split('.')[0] for row in dict_list]))\n",
    "        wsi_names = keep_only_existing_files(features_path, wsi_names)\n",
    "\n",
    "        WSIDataset.__init__(\n",
    "            self,\n",
    "            features_path=features_path,\n",
    "            labels_path=labels_path,\n",
    "            patch_labels_path=patch_labels_path,\n",
    "            coords_path=coords_path,\n",
    "            wsi_names=wsi_names,\n",
    "            bag_keys=bag_keys,\n",
    "            patch_size=patch_size,\n",
    "            adj_with_dist=adj_with_dist,\n",
    "            norm_adj=norm_adj,\n",
    "            load_at_init=load_at_init\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined our new `GTExUrinaryBladderDataset` class. We can now instantiate it, using as `bag_keys` only the features `X` and the adjacency matrix `adj`.  We only have to specify the root path! We will use `load_at_init = False` so that the features of the slides are only loaded when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GTEX-N7MS-2125', 'GTEX-N7MT-1825', 'GTEX-NFK9-2125']\n"
     ]
    }
   ],
   "source": [
    "# This is my root, change it to your own!\n",
    "root = '/data/data_fjaviersaezm/GTExTorchmil/UrinaryBladder/'\n",
    "dataset = GTExUrinaryBladderDataset(\n",
    "    root = root, \n",
    "    features='UNI', \n",
    "    bag_keys=['X', 'adj'], \n",
    "    patch_size=512\n",
    ")\n",
    "print(dataset.bag_names[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The dataset object initialized without problems. Now we can display a bag, which is returned as a `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([825, 1024])\n",
      "adj: torch.Size([825, 825])\n"
     ]
    }
   ],
   "source": [
    "el = dataset[0]\n",
    "for key in el.keys():\n",
    "    print(f\"{key}: {el[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The dataset has correctly loaded the `X` tensor and has built the adjacency matrix `adj`. We can already use this bag as input for a MIL model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchmil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
