{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training your first MIL model\n",
    "\n",
    "In the following, we explain how to train a simple MIL model using the <tt>torchmil</tt> library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "We will create a synthetic Multiple Instance Learning (MIL) dataset from the MNIST dataset using the `ToyDataset` from the [<tt>torchmil.datasets</tt>](../api/datasets/index.md) module. The `ToyDataset` generates an MIL dataset from a labeled dataset by specifying the desired bag size and the objective labels, which determine positive instances. Subsequently, `ToyDataset` constructs bags of the specified size and labels them based on these objective labels. For further details, refer to [here](../api/datasets/toy_dataset.md) for more details.\n",
    "\n",
    "In this example, we are going to set `obj_labels=[2, 5, 8]`. This means that a bag is positive if it contains at least one instance with label 5 or 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchmil.datasets import ToyDataset\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = datasets.MNIST('/tmp/', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Extract features and labels\n",
    "data = mnist.data.view(-1, 28*28) / 255.0\n",
    "labels = mnist.targets\n",
    "\n",
    "# Define positive labels\n",
    "obj_labels = [0, 1] \n",
    "\n",
    "# Define bag size\n",
    "bag_size = (7, 12) # Randomly sample between 8 and 12 instances per bag\n",
    "\n",
    "# Create MIL dataset\n",
    "train_dataset = ToyDataset(data, labels, num_bags=10000, obj_labels=obj_labels, bag_size=bag_size)\n",
    "val_dataset = ToyDataset(data, labels, num_bags=2000, obj_labels=obj_labels, bag_size=bag_size)\n",
    "test_dataset = ToyDataset(data, labels, num_bags=2000, obj_labels=obj_labels, bag_size=bag_size)\n",
    "\n",
    "# Print one bag\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <tt>torchmil</tt>, each bag is a `TensorDict`. The different keys correspond to different elements of the bag. In this case, each bag has a feature matrix `X`, the bag label `Y`, and the instance labels `y_inst`. Recall that the instance labels cannot be used during training, they are available only for evaluation purposes.\n",
    "\n",
    "## Mini-batching of bags\n",
    "\n",
    "Tipically, the bags in a MIL dataset have different size. This can be a problem when creating mini-batches. To solve this, we use the function `collate_fn` from the [<tt><b>torchmil.data</b></tt>](../api/data/index.md) module. This function creates a mini-batch of bags by padding the bags with zeros to the size of the largest bag in the batch. The function also returns a mask tensor that indicates which instances are real and which are padding.\n",
    "\n",
    "!!! info \"Why not use `NestedTensor`?\"\n",
    "    The `NestedTensor` approach is a more flexible way to handle bags with different sizes. However, the PyTorch API of nested tensors is still in prototype stage.\n",
    "\n",
    "Let's create the dataloaders and visualize some bags from a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmil.data import collate_fn\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensordict import TensorDict\n",
    "\n",
    "def plot_batch(batch, max_bags=5):\n",
    "    batch_size = min(len(batch['X']), max_bags)\n",
    "    bag_size = len(batch['X'][0])\n",
    "    fig, axes = plt.subplots(batch_size, bag_size, figsize=(bag_size, 1.5*batch_size))\n",
    "    for i in range(batch_size):\n",
    "        for j in range(bag_size):\n",
    "            ax = axes[i, j]\n",
    "            ax.imshow(batch['X'][i][j].view(28, 28), cmap='gray')\n",
    "            title_str = f\"label: {batch['y_inst'][i][j].item()}\\nmask: {batch['mask'][i][j].item()}\"\n",
    "            ax.set_title(title_str)        \n",
    "            ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)\n",
    "plot_batch(batch, max_bags=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each batch is again a `TensorDict` with an additional key `mask` that indicates which instances are real and which are padding. As we can see, the bags are padded to the maximum size of the bags in the batch with zeros. The mask tensor indicates which elements are real instances and which are padding. The function `collate_fn` also pads other tensors, such as the adjacency matrix or the instance coordinates. \n",
    "\n",
    "## The model\n",
    "\n",
    "In this example, we are going to use the [ABMIL](../api/models/abmil.md) model. \n",
    "Although it is readily available in the [<tt><b>torchmil.models</b></tt>](../api/models/index.md) module, we are going to implement a simple variation from scratch to show how to create a custom MIL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmil.nn import masked_softmax\n",
    "\n",
    "class ABMIL(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, att_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extractor\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28*28, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, emb_dim),\n",
    "        )\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(emb_dim, att_dim)\n",
    "        self.fc2 = torch.nn.Linear(att_dim, 1)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(emb_dim, 1)       \n",
    "\n",
    "        \n",
    "    def forward(self, X, mask, return_att=False):\n",
    "        X = self.mlp(X) # (batch_size, bag_size, emb_dim)\n",
    "        H = torch.tanh(self.fc1(X)) # (batch_size, bag_size, att_dim)\n",
    "        att = torch.sigmoid(self.fc2(H)) # (batch_size, bag_size, 1)\n",
    "        att_s = masked_softmax(att, mask) # (batch_size, bag_size, 1)\n",
    "        # att_s = torch.nn.functional.softmax(att, dim=1)\n",
    "        X = torch.bmm(att_s.transpose(1, 2), X).squeeze(1) # (batch_size, emb_dim)\n",
    "        y = self.classifier(X).squeeze(1) # (batch_size,)\n",
    "        if return_att:\n",
    "            return y, att_s\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "model = ABMIL(emb_dim=256, att_dim=128)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's train the model. We are going to use the `torch.optim.Adam` optimizer and the `torch.nn.BCEWithLogitsLoss` loss function. We will train the model for 20 epochs.\n",
    "\n",
    "Thanks to the <tt>torchmil</tt> library, training a MIL model is as simple as training a standard PyTorch model. The training loop is straightforward, similar to the standard PyTorch training loop. We iterate over the training dataloader, compute the loss, and update the model parameters. We also track the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = ABMIL(emb_dim=256, att_dim=128)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "def train(dataloader, epoch):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
    "    pbar.set_description(f\"[Epoch {epoch}] Train \")\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    sum_correct = 0.0\n",
    "    for batch_idx, batch in pbar:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch['X'], batch['mask'])\n",
    "        loss = criterion(out, batch['Y'].float())\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        pred = (out > 0).float()\n",
    "        sum_correct += (pred == batch['Y']).sum().item()\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        if batch_idx < len(dataloader) - 1:\n",
    "            pbar.set_postfix({'train/loss' : sum_loss / (batch_idx+1)})\n",
    "        else:\n",
    "            pbar.set_postfix({'train/loss' : sum_loss / (batch_idx+1), 'train/bag/acc' : sum_correct / len(dataloader.dataset)})\n",
    "\n",
    "def val(dataloader, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
    "    pbar.set_description(f\"[Epoch {epoch}] Validation \")\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    sum_correct = 0.0\n",
    "    for batch_idx, batch in pbar:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch['X'], batch['mask'])\n",
    "        loss = criterion(out, batch['Y'].float())\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        pred = (out > 0).float()\n",
    "        sum_correct += (pred == batch['Y']).sum().item()\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        if batch_idx < len(dataloader) - 1:\n",
    "            pbar.set_postfix({'val/loss' : sum_loss / (batch_idx+1)})\n",
    "        else:\n",
    "            pbar.set_postfix({'val/loss' : sum_loss / (batch_idx+1), 'val/bag/acc' : sum_correct / len(dataloader.dataset)})\n",
    "\n",
    "model = model.to(device)\n",
    "for epoch in range(20):\n",
    "    train(train_dataloader, epoch+1)\n",
    "    val(test_dataloader, epoch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreases as the model learns to predict the bag labels. The accuracy increases as the model learns to predict the correct bag labels. This is a good sign that the model is learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model. We are going to compute the accuracy and f1-score on the test set. The accuracy is the proportion of correctly classified bags, while the f1-score is the harmonic mean of precision and recall. The f1-score is a good metric for imbalanced datasets.\n",
    "Typically, in MIL datasets, there are many more negative instances than positive instances. In this case, the f1-score will be very useful.\n",
    "\n",
    "To compute predictions at the instance level, we are going to use the _attention_ values. They indicate the importance given by the model to each instance in the bag. As explained in the [ABMIL](../api/models/abmil.md) paper, in positive bags, the model should give more importance to the positive instances, so the attention values should be higher for positive instances. \n",
    "\n",
    "Instance-level predictions have certain caveats due to padding. When performing operations such as normalizing across the bag, it’s crucial to handle padded instances carefully, as they can affect predictions. Additionally, padded instances should be excluded when computing metrics to ensure accuracy.\n",
    "\n",
    "First, we define some auxiliary functions. Take a look at how we handle padded instances in `normalize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, y):\n",
    "    return (pred == y).sum().item() / len(y)\n",
    "\n",
    "def f1_score(pred, y):\n",
    "    tp = ((pred == 1) & (y == 1)).sum().item()\n",
    "    fp = ((pred == 1) & (y == 0)).sum().item()\n",
    "    fn = ((pred == 0) & (y == 1)).sum().item()\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    return f1\n",
    "\n",
    "def normalize(att, mask = None):\n",
    "    if mask is None:\n",
    "        mask = torch.ones_like(att)\n",
    "    else:\n",
    "        mask = mask.unsqueeze(-1)\n",
    "    \n",
    "    # exclude masked values from computing min and max\n",
    "    att_tmp = att.clone()\n",
    "    att_tmp[mask == 0] = float('inf')\n",
    "    att_min = att_tmp.min(dim=1, keepdim=True).values\n",
    "    att_tmp[mask == 0] = float('-inf')\n",
    "    att_max = att_tmp.max(dim=1, keepdim=True).values\n",
    "    att_min = att_min.expand_as(att)\n",
    "    att_max = att_max.expand_as(att)\n",
    "\n",
    "    # padded instances are set to min value, so that after normalization are set to 0 \n",
    "    att = torch.where(mask == 0, att_min, att)\n",
    "\n",
    "    # normalize\n",
    "    att_norm = (att - att_min) / (att_max - att_min + 1e-10)\n",
    "    return att_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the predictions at both the bag and instance level. Take a look at how we handle the padded instances!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_pred_list = []\n",
    "y_inst_list = []\n",
    "Y_pred_list = []\n",
    "Y_list = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    # predict bag label and attention\n",
    "    out, att = model(batch['X'], batch['mask'], return_att=True)\n",
    "    Y_pred = (out > 0).float()\n",
    "\n",
    "    # normalize attention\n",
    "    att_norm = normalize(att, batch['mask'])\n",
    "\n",
    "    # remove attention corresponding to padded instances\n",
    "    att_norm = att_norm.view(-1)[batch['mask'].view(-1) == 1]\n",
    "    inst_pred = (att_norm > 0.5).float()\n",
    "\n",
    "    # remove labels corresponding to padded instances\n",
    "    y_inst = batch['y_inst'].view(-1)[batch['mask'].view(-1) == 1]\n",
    "\n",
    "    inst_pred_list.append(inst_pred)\n",
    "    y_inst_list.append(y_inst)\n",
    "    Y_pred_list.append(Y_pred)\n",
    "    Y_list.append(batch['Y'])\n",
    "\n",
    "\n",
    "inst_pred = torch.cat(inst_pred_list)\n",
    "y_inst = torch.cat(y_inst_list)\n",
    "Y_pred = torch.cat(Y_pred_list)\n",
    "Y = torch.cat(Y_list)\n",
    "\n",
    "print(f\"test/bag/acc: {accuracy(Y_pred, Y)}\")\n",
    "print(f\"test/bag/f1: {f1_score(Y_pred, Y)}\")\n",
    "print(f\"test/inst/acc: {accuracy(inst_pred, y_inst)}\")\n",
    "print(f\"test/inst/f1: {f1_score(inst_pred, y_inst)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Our model is working well. The accuracy and f1-score are high. This means that the attention values are correctly indicating the importance of the instances in the bag. Let's visualize the attention values for some bags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are going to take a look at the _attention maps_, which show the importance of each instance in the bag. The attention maps are computed by normalizing the attention values across the bag. Also, we highlight the instances predicted as positive in the positive bags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_batch_att(batch, att, max_bags=5):\n",
    "    batch_size = min(len(batch['X']), max_bags)\n",
    "    bag_size = len(batch['X'][0])\n",
    "    fig, axes = plt.subplots(batch_size, bag_size, figsize=(bag_size, 1.5*batch_size))\n",
    "    for i in range(batch_size):\n",
    "        for j in range(bag_size):\n",
    "            ax = axes[i, j]\n",
    "            ax.imshow(batch['X'][i][j].view(28, 28), cmap='gray')\n",
    "            if batch['Y'][i] == 1:\n",
    "                att_bag = normalize(att[i].unsqueeze(0), batch['mask'][i].unsqueeze(0)).squeeze(0)\n",
    "                pred = (att_bag > 0.5).float()\n",
    "                overlay_red = np.full((28, 28, 4), [1., 0., 0., 0.4*pred[j].item()])\n",
    "                ax.imshow(overlay_red)\n",
    "            title_str = f\"label: {batch['y_inst'][i][j].item()}\\natt: {att[i][j].item():.2}\"\n",
    "            ax.set_title(title_str)\n",
    "            ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "batch = next(iter(test_dataloader))\n",
    "batch = batch.to(device)\n",
    "_, att = model(batch['X'], batch['mask'], return_att=True)\n",
    "batch = batch.detach().cpu()\n",
    "att = att.detach().cpu()\n",
    "plot_batch_att(batch, att, max_bags=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is correctly giving more importance to the positive instances in the positive bags. This is a good sign! Also, the model itself learned to predict the positive instances correctly, even though it was not trained with instance-level labels! This is the power of Multiple Instance Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchmil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
